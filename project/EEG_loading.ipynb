{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf                                \n",
    "from tensorflow import keras             \n",
    "import numpy as np                       \n",
    "from sklearn.model_selection import train_test_split   \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten,Dropout\n",
    "from keras.layers import Conv2D,BatchNormalization,MaxPooling2D,Reshape, LSTM, ConvLSTM2D, Permute, TimeDistributed\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Reshape, Conv2DTranspose, LeakyReLU, BatchNormalization, Input, concatenate\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples:\n",
    "* CNN\n",
    "* CNN-LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = np.load(\"X_test.npy\")\n",
    "y_test = np.load(\"y_test.npy\")\n",
    "person_train_valid = np.load(\"person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"y_train_valid.npy\")\n",
    "person_test = np.load(\"person_test.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Training/Valid data shape: {}'.format(X_train_valid.shape))\n",
    "print ('Test data shape: {}'.format(X_test.shape))\n",
    "print ('Training/Valid target shape: {}'.format(y_train_valid.shape))\n",
    "print ('Test target shape: {}'.format(y_test.shape))\n",
    "print ('Person train/valid shape: {}'.format(person_train_valid.shape))\n",
    "print ('Person test shape: {}'.format(person_test.shape))\n",
    "print(\"testing data:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(X,y,sub_sample,average,noise):\n",
    "    \n",
    "    total_X = None\n",
    "    total_y = None\n",
    "    \n",
    "    # Trimming the data (sample,22,1000) -> (sample,22,500)\n",
    "    X = X[:,:,0:500]\n",
    "    # print('Shape of X after trimming:',X.shape)\n",
    "    \n",
    "    # Maxpooling the data (sample,22,1000) -> (sample,22,500/sub_sample)\n",
    "    X_max = np.max(X.reshape(X.shape[0], X.shape[1], -1, sub_sample), axis=3)\n",
    "    \n",
    "    \n",
    "    total_X = X_max\n",
    "    total_y = y\n",
    "    # print('Shape of X after maxpooling:',total_X.shape)\n",
    "    \n",
    "    # Averaging + noise \n",
    "    X_average = np.mean(X.reshape(X.shape[0], X.shape[1], -1, average),axis=3)\n",
    "    X_average = X_average + np.random.normal(0.0, 0.5, X_average.shape)\n",
    "    \n",
    "    total_X = np.vstack((total_X, X_average))\n",
    "    total_y = np.hstack((total_y, y))\n",
    "    # print('Shape of X after averaging+noise and concatenating:',total_X.shape)\n",
    "    \n",
    "    # Subsampling\n",
    "    \n",
    "    for i in range(sub_sample):\n",
    "        \n",
    "        X_subsample = X[:, :, i::sub_sample] + \\\n",
    "                            (np.random.normal(0.0, 0.5, X[:, :,i::sub_sample].shape) if noise else 0.0)\n",
    "            \n",
    "        total_X = np.vstack((total_X, X_subsample))\n",
    "        total_y = np.hstack((total_y, y))\n",
    "        \n",
    "    \n",
    "    # print('Shape of X after subsampling and concatenating:',total_X.shape)\n",
    "    return total_X,total_y\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading and visualizing the data\n",
    "\n",
    "## Loading the dataset\n",
    "\n",
    "\n",
    "X_test = np.load(\"X_test.npy\")\n",
    "y_test = np.load(\"y_test.npy\")\n",
    "person_train_valid = np.load(\"person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"y_train_valid.npy\")\n",
    "person_test = np.load(\"person_test.npy\")\n",
    "\n",
    "## Adjusting the labels so that \n",
    "\n",
    "# Cue onset left - 0\n",
    "# Cue onset right - 1\n",
    "# Cue onset foot - 2\n",
    "# Cue onset tongue - 3\n",
    "\n",
    "y_train_valid -= 769\n",
    "y_test -= 769\n",
    "\n",
    "## Visualizing the data\n",
    "\n",
    "ch_data = X_train_valid[:,8,:] # extracts the 9th channel from the data\n",
    "\n",
    "\n",
    "class_0_ind = np.where(y_train_valid == 0) # finds the indices where the label is 0\n",
    "ch_data_class_0 = ch_data[class_0_ind] # finds the data where label is 0\n",
    "avg_ch_data_class_0 = np.mean(ch_data_class_0,axis=0) # finds the average representation of the 9th channel when label is 0\n",
    "\n",
    "\n",
    "class_1_ind = np.where(y_train_valid == 1)\n",
    "ch_data_class_1 = ch_data[class_1_ind]\n",
    "avg_ch_data_class_1 = np.mean(ch_data_class_1,axis=0)\n",
    "\n",
    "class_2_ind = np.where(y_train_valid == 2)\n",
    "ch_data_class_2 = ch_data[class_2_ind]\n",
    "avg_ch_data_class_2 = np.mean(ch_data_class_2,axis=0)\n",
    "\n",
    "class_3_ind = np.where(y_train_valid == 3)\n",
    "ch_data_class_3 = ch_data[class_3_ind]\n",
    "avg_ch_data_class_3 = np.mean(ch_data_class_3,axis=0)\n",
    "\n",
    "\n",
    "plt.plot(np.arange(1000),avg_ch_data_class_0)\n",
    "plt.plot(np.arange(1000),avg_ch_data_class_1)\n",
    "plt.plot(np.arange(1000),avg_ch_data_class_2)\n",
    "plt.plot(np.arange(1000),avg_ch_data_class_3)\n",
    "plt.axvline(x=500, label='line at t=500',c='cyan')\n",
    "\n",
    "plt.legend([\"Cue Onset left\", \"Cue Onset right\", \"Cue onset foot\", \"Cue onset tongue\"])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random splitting and reshaping the data\n",
    "# First generating the training and validation indices using random splitting\n",
    "\n",
    "ind_valid = np.random.choice(2115, 375, replace=False)\n",
    "ind_train = np.array(list(set(range(2115)).difference(set(ind_valid))))\n",
    "\n",
    "# Creating the training and validation sets using the generated indices\n",
    "(X_train, X_valid) = X_train_valid[ind_train], X_train_valid[ind_valid] \n",
    "(y_train, y_valid) = y_train_valid[ind_train], y_train_valid[ind_valid]\n",
    "\n",
    "\n",
    "## Preprocessing the dataset\n",
    "x_train,y_train = data_prep(X_train,y_train,2,2,True)\n",
    "x_valid,y_valid = data_prep(X_valid,y_valid,2,2,True)\n",
    "X_test_prep,y_test_prep = data_prep(X_test,y_test,2,2,True)\n",
    "\n",
    "\n",
    "print('Shape of testing set:',X_test_prep.shape)\n",
    "print('Shape of testing labels:',y_test_prep.shape)\n",
    "\n",
    "print('Shape of training set:',x_train.shape)\n",
    "print('Shape of validation set:',x_valid.shape)\n",
    "print('Shape of training labels:',y_train.shape)\n",
    "print('Shape of validation labels:',y_valid.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Converting the labels to categorical variables for multiclass classification\n",
    "y_train = to_categorical(y_train, 4)\n",
    "y_valid = to_categorical(y_valid, 4)\n",
    "y_test = to_categorical(y_test_prep, 4)\n",
    "print('Shape of training labels after categorical conversion:',y_train.shape)\n",
    "print('Shape of validation labels after categorical conversion:',y_valid.shape)\n",
    "print('Shape of test labels after categorical conversion:',y_test.shape)\n",
    "\n",
    "# Adding width of the segment to be 1\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)\n",
    "x_test = X_test_prep.reshape(X_test_prep.shape[0], X_test_prep.shape[1], X_test_prep.shape[2], 1)\n",
    "print('Shape of training set after adding width info:',x_train.shape)\n",
    "print('Shape of validation set after adding width info:',x_valid.shape)\n",
    "print('Shape of test set after adding width info:',x_test.shape)\n",
    "\n",
    "\n",
    "# Reshaping the training and validation dataset\n",
    "x_train = np.swapaxes(x_train, 1,3)\n",
    "x_train = np.swapaxes(x_train, 1,2)\n",
    "x_valid = np.swapaxes(x_valid, 1,3)\n",
    "x_valid = np.swapaxes(x_valid, 1,2)\n",
    "x_test = np.swapaxes(x_test, 1,3)\n",
    "x_test = np.swapaxes(x_test, 1,2)\n",
    "print('Shape of training set after dimension reshaping:',x_train.shape)\n",
    "print('Shape of validation set after dimension reshaping:',x_valid.shape)\n",
    "print('Shape of test set after dimension reshaping:',x_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Building the CNN model using sequential class\n",
    "basic_cnn_model = Sequential()\n",
    "\n",
    "# Conv. block 1\n",
    "basic_cnn_model.add(Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu', input_shape=(250,1,22)))\n",
    "basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same')) # Read the keras documentation\n",
    "basic_cnn_model.add(BatchNormalization())\n",
    "basic_cnn_model.add(Dropout(0.5))\n",
    "\n",
    "# Conv. block 2\n",
    "basic_cnn_model.add(Conv2D(filters=50, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "basic_cnn_model.add(BatchNormalization())\n",
    "basic_cnn_model.add(Dropout(0.5))\n",
    "\n",
    "# Conv. block 3\n",
    "basic_cnn_model.add(Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "basic_cnn_model.add(BatchNormalization())\n",
    "basic_cnn_model.add(Dropout(0.5))\n",
    "\n",
    "# Conv. block 4\n",
    "basic_cnn_model.add(Conv2D(filters=200, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "basic_cnn_model.add(BatchNormalization())\n",
    "basic_cnn_model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer with Softmax activation\n",
    "basic_cnn_model.add(Flatten()) # Flattens the input\n",
    "basic_cnn_model.add(Dense(4, activation='softmax')) # Output FC layer with softmax activation\n",
    "\n",
    "\n",
    "# Printing the model summary\n",
    "basic_cnn_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "learning_rate = 1e-3\n",
    "epochs = 50\n",
    "cnn_optimizer = keras.optimizers.Adam(lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "basic_cnn_model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=cnn_optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Training and validating the model\n",
    "basic_cnn_model_results = basic_cnn_model.fit(x_train,\n",
    "             y_train,\n",
    "             batch_size=64,\n",
    "             epochs=epochs,\n",
    "             validation_data=(x_valid, y_valid), verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_score = basic_cnn_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test accuracy of the basic CNN model:',cnn_score[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the dataset\n",
    "\n",
    "\n",
    "X_test = np.load(\"X_test.npy\")\n",
    "y_test = np.load(\"y_test.npy\")\n",
    "person_train_valid = np.load(\"person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"y_train_valid.npy\")\n",
    "person_test = np.load(\"person_test.npy\")\n",
    "\n",
    "## Adjusting the labels so that \n",
    "\n",
    "# Cue onset left - 0\n",
    "# Cue onset right - 1\n",
    "# Cue onset foot - 2\n",
    "# Cue onset tongue - 3\n",
    "\n",
    "y_train_valid -= 769\n",
    "y_test -= 769\n",
    "\n",
    "\n",
    "## Random splitting and reshaping the data\n",
    "# First generating the training and validation indices using random splitting\n",
    "\n",
    "ind_valid = np.random.choice(2115, 375, replace=False)\n",
    "ind_train = np.array(list(set(range(2115)).difference(set(ind_valid))))\n",
    "\n",
    "# Creating the training and validation sets using the generated indices\n",
    "(X_train, X_valid) = X_train_valid[ind_train], X_train_valid[ind_valid] \n",
    "(y_train, y_valid) = y_train_valid[ind_train], y_train_valid[ind_valid]\n",
    "\n",
    "\n",
    "## Preprocessing the dataset\n",
    "x_train,y_train = data_prep(X_train,y_train,2,2,True)\n",
    "x_valid,y_valid = data_prep(X_valid,y_valid,2,2,True)\n",
    "X_test_prep,y_test_prep = data_prep(X_test,y_test,2,2,True)\n",
    "\n",
    "\n",
    "print('Shape of training set:',x_train.shape)\n",
    "print('Shape of validation set:',x_valid.shape)\n",
    "print('Shape of training labels:',y_train.shape)\n",
    "print('Shape of validation labels:',y_valid.shape)\n",
    "print('Shape of testing set:',X_test_prep.shape)\n",
    "print('Shape of testing labels:',y_test_prep.shape)\n",
    "\n",
    "\n",
    "# Converting the labels to categorical variables for multiclass classification\n",
    "y_train = to_categorical(y_train, 4)\n",
    "y_valid = to_categorical(y_valid, 4)\n",
    "y_test = to_categorical(y_test_prep, 4)\n",
    "print('Shape of training labels after categorical conversion:',y_train.shape)\n",
    "print('Shape of validation labels after categorical conversion:',y_valid.shape)\n",
    "print('Shape of test labels after categorical conversion:',y_test.shape)\n",
    "\n",
    "# Adding width of the segment to be 1\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)\n",
    "x_test = X_test_prep.reshape(X_test_prep.shape[0], X_test_prep.shape[1], X_test_prep.shape[2], 1)\n",
    "print('Shape of training set after adding width info:',x_train.shape)\n",
    "print('Shape of validation set after adding width info:',x_valid.shape)\n",
    "print('Shape of test set after adding width info:',x_test.shape)\n",
    "\n",
    "\n",
    "# Reshaping the training and validation dataset\n",
    "x_train = np.swapaxes(x_train, 1,3)\n",
    "x_train = np.swapaxes(x_train, 1,2)\n",
    "x_valid = np.swapaxes(x_valid, 1,3)\n",
    "x_valid = np.swapaxes(x_valid, 1,2)\n",
    "x_test = np.swapaxes(x_test, 1,3)\n",
    "x_test = np.swapaxes(x_test, 1,2)\n",
    "print('Shape of training set after dimension reshaping:',x_train.shape)\n",
    "print('Shape of validation set after dimension reshaping:',x_valid.shape)\n",
    "print('Shape of test set after dimension reshaping:',x_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the CNN model using sequential class\n",
    "hybrid_cnn_lstm_model = Sequential()\n",
    "\n",
    "# Conv. block 1\n",
    "hybrid_cnn_lstm_model.add(Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu', input_shape=(250,1,22)))\n",
    "hybrid_cnn_lstm_model.add(MaxPooling2D(pool_size=(3,1), padding='same')) # Read the keras documentation\n",
    "hybrid_cnn_lstm_model.add(BatchNormalization())\n",
    "hybrid_cnn_lstm_model.add(Dropout(0.5))\n",
    "\n",
    "# Conv. block 2\n",
    "hybrid_cnn_lstm_model.add(Conv2D(filters=50, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "hybrid_cnn_lstm_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "hybrid_cnn_lstm_model.add(BatchNormalization())\n",
    "hybrid_cnn_lstm_model.add(Dropout(0.5))\n",
    "\n",
    "# Conv. block 3\n",
    "hybrid_cnn_lstm_model.add(Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "hybrid_cnn_lstm_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "hybrid_cnn_lstm_model.add(BatchNormalization())\n",
    "hybrid_cnn_lstm_model.add(Dropout(0.5))\n",
    "\n",
    "# Conv. block 4\n",
    "hybrid_cnn_lstm_model.add(Conv2D(filters=200, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "hybrid_cnn_lstm_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "hybrid_cnn_lstm_model.add(BatchNormalization())\n",
    "hybrid_cnn_lstm_model.add(Dropout(0.5))\n",
    "\n",
    "# FC+LSTM layers\n",
    "hybrid_cnn_lstm_model.add(Flatten()) # Adding a flattening operation to the output of CNN block\n",
    "hybrid_cnn_lstm_model.add(Dense((100))) # FC layer with 100 units\n",
    "hybrid_cnn_lstm_model.add(Reshape((100,1))) # Reshape my output of FC layer so that it's compatible\n",
    "hybrid_cnn_lstm_model.add(LSTM(10, dropout=0.6, recurrent_dropout=0.1, input_shape=(100,1), return_sequences=False))\n",
    "\n",
    "\n",
    "# Output layer with Softmax activation \n",
    "hybrid_cnn_lstm_model.add(Dense(4, activation='softmax')) # Output FC layer with softmax activation\n",
    "\n",
    "\n",
    "# Printing the model summary\n",
    "hybrid_cnn_lstm_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "learning_rate = 1e-3\n",
    "epochs = 50\n",
    "hybrid_cnn_lstm_optimizer = keras.optimizers.Adam(lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "hybrid_cnn_lstm_model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=hybrid_cnn_lstm_optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Training and validating the model\n",
    "hybrid_cnn_lstm_model_results = hybrid_cnn_lstm_model.fit(x_train,\n",
    "             y_train,\n",
    "             batch_size=64,\n",
    "             epochs=epochs,\n",
    "             validation_data=(x_valid, y_valid), verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing the hybrid CNN-LSTM model\n",
    "\n",
    "hybrid_cnn_lstm_score = hybrid_cnn_lstm_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test accuracy of the hybrid CNN-LSTM model:',hybrid_cnn_lstm_score[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Optimize the classification accuracy for subject 1. Does it help to train across all subjects?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load(\"X_test.npy\")\n",
    "y_test = np.load(\"y_test.npy\")\n",
    "person_train_valid = np.load(\"person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"y_train_valid.npy\")\n",
    "person_test = np.load(\"person_test.npy\")\n",
    "\n",
    "## Adjusting the labels so that \n",
    "\n",
    "# Cue onset left - 0\n",
    "# Cue onset right - 1\n",
    "# Cue onset foot - 2\n",
    "# Cue onset tongue - 3\n",
    "\n",
    "y_train_valid -= 769\n",
    "y_test -= 769\n",
    "\n",
    "subject = 0\n",
    "subject_test_idx = np.where(person_test==subject)[0]\n",
    "subject_valid_idx = np.where(person_train_valid==subject)[0]\n",
    "\n",
    "\n",
    "subject_X_test = X_test[subject_test_idx]\n",
    "suject_y_test = y_test[subject_test_idx]\n",
    "suject_X_train_valid = X_train_valid[subject_valid_idx]\n",
    "suject_y_train_valid = y_train_valid[subject_valid_idx]\n",
    "\n",
    "print(f'X_test Shape for Subject {subject}: {subject_X_test.shape}')\n",
    "print(f'y_test Shape for Subject {subject}: {suject_y_test.shape}')\n",
    "print(f'X_train_valid Shape for Subject {subject}: {suject_X_train_valid.shape}')\n",
    "print(f'y_train_valid Shape for Subject {subject}: {suject_y_train_valid.shape}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle with 5 fold\n",
    "indicies_valid = np.random.choice(suject_X_train_valid.shape[0], suject_X_train_valid.shape[0] // 5, replace=False)\n",
    "indicies_train = np.array(list(set(range(suject_X_train_valid.shape[0])).difference(set(indicies_valid))))\n",
    "\n",
    "# Creating the training and validation sets using the generated indices\n",
    "X_train, X_valid = suject_X_train_valid[indicies_train], suject_X_train_valid[indicies_valid] \n",
    "y_train, y_valid = suject_y_train_valid[indicies_train], suject_y_train_valid[indicies_valid]\n",
    "\n",
    "\n",
    "# Preprocessing the dataset\n",
    "x_train,y_train = data_prep(X_train,y_train,2,2,True)\n",
    "x_valid,y_valid = data_prep(X_valid,y_valid,2,2,True)\n",
    "X_test_prep,y_test_prep = data_prep(subject_X_test,suject_y_test,2,2,True)\n",
    "\n",
    "\n",
    "\n",
    "print('Shape of training set:',x_train.shape)\n",
    "print('Shape of validation set:',x_valid.shape)\n",
    "print('Shape of training labels:',y_train.shape)\n",
    "print('Shape of validation labels:',y_valid.shape)\n",
    "print('Shape of testing set:',X_test_prep.shape)\n",
    "print('Shape of testing labels:',y_test_prep.shape)\n",
    "\n",
    "\n",
    "# Converting the labels to categorical variables for multiclass classification\n",
    "y_train = to_categorical(y_train, 4)\n",
    "y_valid = to_categorical(y_valid, 4)\n",
    "y_test = to_categorical(y_test_prep, 4)\n",
    "print('Shape of training labels after categorical conversion:',y_train.shape)\n",
    "print('Shape of validation labels after categorical conversion:',y_valid.shape)\n",
    "print('Shape of test labels after categorical conversion:',y_test.shape)\n",
    "\n",
    "# Adding width of the segment to be 1\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)\n",
    "x_test = X_test_prep.reshape(X_test_prep.shape[0], X_test_prep.shape[1], X_test_prep.shape[2], 1)\n",
    "print('Shape of training set after adding width info:',x_train.shape)\n",
    "print('Shape of validation set after adding width info:',x_valid.shape)\n",
    "print('Shape of test set after adding width info:',x_test.shape)\n",
    "\n",
    "\n",
    "# Reshaping the training and validation dataset\n",
    "x_train = np.swapaxes(x_train, 1,3)\n",
    "x_train = np.swapaxes(x_train, 1,2)\n",
    "x_valid = np.swapaxes(x_valid, 1,3)\n",
    "x_valid = np.swapaxes(x_valid, 1,2)\n",
    "x_test = np.swapaxes(x_test, 1,3)\n",
    "x_test = np.swapaxes(x_test, 1,2)\n",
    "print('Shape of training set after dimension reshaping:',x_train.shape)\n",
    "print('Shape of validation set after dimension reshaping:',x_valid.shape)\n",
    "print('Shape of test set after dimension reshaping:',x_test.shape)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Building the CNN model using sequential class\n",
    "cnn_subject_model = Sequential()\n",
    "\n",
    "# Conv. block 1\n",
    "cnn_subject_model.add(Conv2D(filters=10, kernel_size=(5,1), padding='same', activation='elu', input_shape=(250,1,22)))\n",
    "cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same')) \n",
    "cnn_subject_model.add(BatchNormalization())\n",
    "cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "# Conv. block 2\n",
    "cnn_subject_model.add(Conv2D(filters=10, kernel_size=(15,1), padding='same', activation='elu'))\n",
    "cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "cnn_subject_model.add(BatchNormalization())\n",
    "cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "\n",
    "# # Conv. block 4\n",
    "# cnn_subject_model.add(Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "# cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "# cnn_subject_model.add(BatchNormalization())\n",
    "# cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "# # Conv. block 5\n",
    "# cnn_subject_model.add(Conv2D(filters=100, kernel_size=(50,1), padding='same', activation='elu'))\n",
    "# cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "# cnn_subject_model.add(BatchNormalization())\n",
    "# cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "# # Conv. block 6\n",
    "# cnn_subject_model.add(Conv2D(filters=50, kernel_size=(50,1), padding='same', activation='elu'))\n",
    "# cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "# cnn_subject_model.add(BatchNormalization())\n",
    "# cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "# # Conv. block 7\n",
    "# cnn_subject_model.add(Conv2D(filters=25, kernel_size=(50,1), padding='same', activation='elu'))\n",
    "# cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "# cnn_subject_model.add(BatchNormalization())\n",
    "# cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer with Softmax activation\n",
    "cnn_subject_model.add(Flatten()) # Flattens the input\n",
    "cnn_subject_model.add(Dense(4, activation='softmax')) # Output FC layer with softmax activation\n",
    "\n",
    "\n",
    "# Printing the model summary\n",
    "cnn_subject_model.summary()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 100\n",
    "cnn_subject_model_optimizer = keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_subject_model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=cnn_subject_model_optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "cnn_subject_model_results = cnn_subject_model.fit(x_train,\n",
    "             y_train,\n",
    "             batch_size=64,\n",
    "             epochs=epochs,\n",
    "             validation_data=(x_valid, y_valid), verbose=True\n",
    "             )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_subject_model_score = cnn_subject_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Test accuracy of the CNN model for subject {subject}:',cnn_subject_model_score[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Training across all subjects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load(\"X_test.npy\")\n",
    "y_test = np.load(\"y_test.npy\")\n",
    "person_train_valid = np.load(\"person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"y_train_valid.npy\")\n",
    "person_test = np.load(\"person_test.npy\")\n",
    "\n",
    "## Adjusting the labels so that \n",
    "\n",
    "# Cue onset left - 0\n",
    "# Cue onset right - 1\n",
    "# Cue onset foot - 2\n",
    "# Cue onset tongue - 3\n",
    "\n",
    "y_train_valid -= 769\n",
    "y_test -= 769\n",
    "\n",
    "subject = 0\n",
    "subject_test_idx = np.where(person_test==subject)[0]\n",
    "subject_valid_idx = np.where(person_train_valid==subject)[0]\n",
    "\n",
    "\n",
    "subject_X_test = X_test[subject_test_idx]\n",
    "suject_y_test = y_test[subject_test_idx]\n",
    "suject_X_train_valid = X_train_valid[subject_valid_idx]\n",
    "suject_y_train_valid = y_train_valid[subject_valid_idx]\n",
    "\n",
    "print(f'X_test Shape for Subject {subject}: {subject_X_test.shape}')\n",
    "print(f'y_test Shape for Subject {subject}: {suject_y_test.shape}')\n",
    "print(f'X_train_valid Shape for Subject {subject}: {suject_X_train_valid.shape}')\n",
    "print(f'y_train_valid Shape for Subject {subject}: {suject_y_train_valid.shape}')\n",
    "\n",
    "# shuffle with 5 fold\n",
    "indicies_valid = np.random.choice(X_train_valid.shape[0], X_train_valid.shape[0] // 5, replace=False)\n",
    "indicies_train = np.array(list(set(range(X_train_valid.shape[0])).difference(set(indicies_valid))))\n",
    "\n",
    "# Creating the training and validation sets using the generated indices\n",
    "X_train, X_valid = X_train_valid[indicies_train], X_train_valid[indicies_valid] \n",
    "y_train, y_valid = y_train_valid[indicies_train], y_train_valid[indicies_valid]\n",
    "\n",
    "\n",
    "# Preprocessing the dataset\n",
    "x_train,y_train = data_prep(X_train,y_train,2,2,True)\n",
    "x_valid,y_valid = data_prep(X_valid,y_valid,2,2,True)\n",
    "X_test_prep,y_test_prep = data_prep(subject_X_test,suject_y_test,2,2,True)\n",
    "\n",
    "\n",
    "\n",
    "print('Shape of training set:',x_train.shape)\n",
    "print('Shape of validation set:',x_valid.shape)\n",
    "print('Shape of training labels:',y_train.shape)\n",
    "print('Shape of validation labels:',y_valid.shape)\n",
    "print('Shape of testing set:',X_test_prep.shape)\n",
    "print('Shape of testing labels:',y_test_prep.shape)\n",
    "\n",
    "\n",
    "# Converting the labels to categorical variables for multiclass classification\n",
    "y_train = to_categorical(y_train, 4)\n",
    "y_valid = to_categorical(y_valid, 4)\n",
    "y_test = to_categorical(y_test_prep, 4)\n",
    "print('Shape of training labels after categorical conversion:',y_train.shape)\n",
    "print('Shape of validation labels after categorical conversion:',y_valid.shape)\n",
    "print('Shape of test labels after categorical conversion:',y_test.shape)\n",
    "\n",
    "# Adding width of the segment to be 1\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)\n",
    "x_test = X_test_prep.reshape(X_test_prep.shape[0], X_test_prep.shape[1], X_test_prep.shape[2], 1)\n",
    "print('Shape of training set after adding width info:',x_train.shape)\n",
    "print('Shape of validation set after adding width info:',x_valid.shape)\n",
    "print('Shape of test set after adding width info:',x_test.shape)\n",
    "\n",
    "\n",
    "# Reshaping the training and validation dataset\n",
    "x_train = np.swapaxes(x_train, 1,3)\n",
    "x_train = np.swapaxes(x_train, 1,2)\n",
    "x_valid = np.swapaxes(x_valid, 1,3)\n",
    "x_valid = np.swapaxes(x_valid, 1,2)\n",
    "x_test = np.swapaxes(x_test, 1,3)\n",
    "x_test = np.swapaxes(x_test, 1,2)\n",
    "print('Shape of training set after dimension reshaping:',x_train.shape)\n",
    "print('Shape of validation set after dimension reshaping:',x_valid.shape)\n",
    "print('Shape of test set after dimension reshaping:',x_test.shape)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Building the CNN model using sequential class\n",
    "cnn_subject_model = Sequential()\n",
    "\n",
    "# Conv. block 1\n",
    "cnn_subject_model.add(Conv2D(filters=10, kernel_size=(5,1), padding='same', activation='elu', input_shape=(250,1,22)))\n",
    "cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same')) \n",
    "cnn_subject_model.add(BatchNormalization())\n",
    "cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "# Conv. block 2\n",
    "cnn_subject_model.add(Conv2D(filters=10, kernel_size=(15,1), padding='same', activation='elu'))\n",
    "cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "cnn_subject_model.add(BatchNormalization())\n",
    "cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "\n",
    "# # Conv. block 4\n",
    "# cnn_subject_model.add(Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "# cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "# cnn_subject_model.add(BatchNormalization())\n",
    "# cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "# # Conv. block 5\n",
    "# cnn_subject_model.add(Conv2D(filters=100, kernel_size=(50,1), padding='same', activation='elu'))\n",
    "# cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "# cnn_subject_model.add(BatchNormalization())\n",
    "# cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "# # Conv. block 6\n",
    "# cnn_subject_model.add(Conv2D(filters=50, kernel_size=(50,1), padding='same', activation='elu'))\n",
    "# cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "# cnn_subject_model.add(BatchNormalization())\n",
    "# cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "# # Conv. block 7\n",
    "# cnn_subject_model.add(Conv2D(filters=25, kernel_size=(50,1), padding='same', activation='elu'))\n",
    "# cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "# cnn_subject_model.add(BatchNormalization())\n",
    "# cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer with Softmax activation\n",
    "cnn_subject_model.add(Flatten()) # Flattens the input\n",
    "cnn_subject_model.add(Dense(4, activation='softmax')) # Output FC layer with softmax activation\n",
    "\n",
    "\n",
    "# Printing the model summary\n",
    "cnn_subject_model.summary()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 100\n",
    "cnn_subject_model_optimizer = keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_subject_model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=cnn_subject_model_optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "cnn_subject_model_results = cnn_subject_model.fit(x_train,\n",
    "             y_train,\n",
    "             batch_size=64,\n",
    "             epochs=epochs,\n",
    "             validation_data=(x_valid, y_valid), verbose=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_subject_model_score = cnn_subject_model.evaluate(x_test, y_test, verbose=False)\n",
    "print(f'Test accuracy of the CNN model for subject {subject}:',cnn_subject_model_score[1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Optimize the classification accuracy across all subjects. How does the classifier do? Do you notice any interesting trends?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_subjects(subject):\n",
    "    X_test = np.load(\"X_test.npy\")\n",
    "    y_test = np.load(\"y_test.npy\")\n",
    "    person_train_valid = np.load(\"person_train_valid.npy\")\n",
    "    X_train_valid = np.load(\"X_train_valid.npy\")\n",
    "    y_train_valid = np.load(\"y_train_valid.npy\")\n",
    "    person_test = np.load(\"person_test.npy\")\n",
    "\n",
    "    ## Adjusting the labels so that \n",
    "\n",
    "    # Cue onset left - 0\n",
    "    # Cue onset right - 1\n",
    "    # Cue onset foot - 2\n",
    "    # Cue onset tongue - 3\n",
    "\n",
    "    y_train_valid -= 769\n",
    "    y_test -= 769\n",
    "    \n",
    "\n",
    "\n",
    "    subject_test_idx = np.where(person_test==subject)[0]\n",
    "    subject_valid_idx = np.where(person_train_valid==subject)[0]\n",
    "\n",
    "\n",
    "    subject_X_test = X_test[subject_test_idx]\n",
    "    suject_y_test = y_test[subject_test_idx]\n",
    "    suject_X_train_valid = X_train_valid[subject_valid_idx]\n",
    "    suject_y_train_valid = y_train_valid[subject_valid_idx]\n",
    "\n",
    "    # print(f'X_test Shape for Subject {subject}: {subject_X_test.shape}')\n",
    "    # print(f'y_test Shape for Subject {subject}: {suject_y_test.shape}')\n",
    "    # print(f'X_train_valid Shape for Subject {subject}: {suject_X_train_valid.shape}')\n",
    "    # print(f'y_train_valid Shape for Subject {subject}: {suject_y_train_valid.shape}')\n",
    "\n",
    "    # shuffle with 5 fold\n",
    "    indicies_valid = np.random.choice(X_train_valid.shape[0], X_train_valid.shape[0] // 5, replace=False)\n",
    "    indicies_train = np.array(list(set(range(X_train_valid.shape[0])).difference(set(indicies_valid))))\n",
    "\n",
    "    # Creating the training and validation sets using the generated indices\n",
    "    X_train, X_valid = X_train_valid[indicies_train], X_train_valid[indicies_valid] \n",
    "    y_train, y_valid = y_train_valid[indicies_train], y_train_valid[indicies_valid]\n",
    "\n",
    "\n",
    "    # Preprocessing the dataset\n",
    "    x_train,y_train = data_prep(X_train,y_train,2,2,True)\n",
    "    x_valid,y_valid = data_prep(X_valid,y_valid,2,2,True)\n",
    "    X_test_prep,y_test_prep = data_prep(subject_X_test,suject_y_test,2,2,True)\n",
    "\n",
    "\n",
    "\n",
    "    # print('Shape of training set:',x_train.shape)\n",
    "    # print('Shape of validation set:',x_valid.shape)\n",
    "    # print('Shape of training labels:',y_train.shape)\n",
    "    # print('Shape of validation labels:',y_valid.shape)\n",
    "    # print('Shape of testing set:',X_test_prep.shape)\n",
    "    # print('Shape of testing labels:',y_test_prep.shape)\n",
    "\n",
    "\n",
    "    # Converting the labels to categorical variables for multiclass classification\n",
    "    y_train = to_categorical(y_train, 4)\n",
    "    y_valid = to_categorical(y_valid, 4)\n",
    "    y_test = to_categorical(y_test_prep, 4)\n",
    "    # print('Shape of training labels after categorical conversion:',y_train.shape)\n",
    "    # print('Shape of validation labels after categorical conversion:',y_valid.shape)\n",
    "    # print('Shape of test labels after categorical conversion:',y_test.shape)\n",
    "\n",
    "    # Adding width of the segment to be 1\n",
    "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "    x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)\n",
    "    x_test = X_test_prep.reshape(X_test_prep.shape[0], X_test_prep.shape[1], X_test_prep.shape[2], 1)\n",
    "    # print('Shape of training set after adding width info:',x_train.shape)\n",
    "    # print('Shape of validation set after adding width info:',x_valid.shape)\n",
    "    # print('Shape of test set after adding width info:',x_test.shape)\n",
    "\n",
    "\n",
    "    # Reshaping the training and validation dataset\n",
    "    x_train = np.swapaxes(x_train, 1,3)\n",
    "    x_train = np.swapaxes(x_train, 1,2)\n",
    "    x_valid = np.swapaxes(x_valid, 1,3)\n",
    "    x_valid = np.swapaxes(x_valid, 1,2)\n",
    "    x_test = np.swapaxes(x_test, 1,3)\n",
    "    x_test = np.swapaxes(x_test, 1,2)\n",
    "    # print('Shape of training set after dimension reshaping:',x_train.shape)\n",
    "    # print('Shape of validation set after dimension reshaping:',x_valid.shape)\n",
    "    # print('Shape of test set after dimension reshaping:',x_test.shape)\n",
    "\n",
    "    return (x_train, y_train, x_valid, y_valid, x_test, y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 1.8722 - accuracy: 0.2914 - val_loss: 1.4435 - val_accuracy: 0.3322\n",
      "Epoch 2/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 1.5134 - accuracy: 0.3468 - val_loss: 1.2604 - val_accuracy: 0.3936\n",
      "Epoch 3/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 1.3594 - accuracy: 0.3840 - val_loss: 1.2390 - val_accuracy: 0.3995\n",
      "Epoch 4/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 1.2570 - accuracy: 0.4289 - val_loss: 1.1870 - val_accuracy: 0.4687\n",
      "Epoch 5/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 1.2129 - accuracy: 0.4542 - val_loss: 1.1776 - val_accuracy: 0.4929\n",
      "Epoch 6/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 1.1811 - accuracy: 0.4690 - val_loss: 1.1546 - val_accuracy: 0.5118\n",
      "Epoch 7/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 1.1562 - accuracy: 0.4866 - val_loss: 1.1436 - val_accuracy: 0.5378\n",
      "Epoch 8/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 1.1365 - accuracy: 0.5069 - val_loss: 1.1213 - val_accuracy: 0.5520\n",
      "Epoch 9/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 1.1140 - accuracy: 0.5134 - val_loss: 1.1082 - val_accuracy: 0.5544\n",
      "Epoch 10/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 1.0987 - accuracy: 0.5247 - val_loss: 1.0994 - val_accuracy: 0.5691\n",
      "Epoch 11/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 1.0882 - accuracy: 0.5337 - val_loss: 1.0765 - val_accuracy: 0.5804\n",
      "Epoch 12/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 1.0551 - accuracy: 0.5536 - val_loss: 1.0490 - val_accuracy: 0.5969\n",
      "Epoch 13/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 1.0336 - accuracy: 0.5624 - val_loss: 1.0084 - val_accuracy: 0.6040\n",
      "Epoch 14/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 1.0279 - accuracy: 0.5708 - val_loss: 0.9843 - val_accuracy: 0.6070\n",
      "Epoch 15/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 1.0032 - accuracy: 0.5876 - val_loss: 0.9934 - val_accuracy: 0.6058\n",
      "Epoch 16/100\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.9861 - accuracy: 0.5907 - val_loss: 0.9357 - val_accuracy: 0.6330\n",
      "Epoch 17/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.9746 - accuracy: 0.5968 - val_loss: 0.9256 - val_accuracy: 0.6383\n",
      "Epoch 18/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.9495 - accuracy: 0.6036 - val_loss: 0.9168 - val_accuracy: 0.6306\n",
      "Epoch 19/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.9459 - accuracy: 0.6068 - val_loss: 0.9168 - val_accuracy: 0.6212\n",
      "Epoch 20/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.9365 - accuracy: 0.6201 - val_loss: 0.8872 - val_accuracy: 0.6548\n",
      "Epoch 21/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.9219 - accuracy: 0.6185 - val_loss: 0.8918 - val_accuracy: 0.6472\n",
      "Epoch 22/100\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.9061 - accuracy: 0.6316 - val_loss: 0.9025 - val_accuracy: 0.6407\n",
      "Epoch 23/100\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.8961 - accuracy: 0.6299 - val_loss: 0.9061 - val_accuracy: 0.6288\n",
      "Epoch 24/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.8974 - accuracy: 0.6281 - val_loss: 0.8744 - val_accuracy: 0.6590\n",
      "Epoch 25/100\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.8746 - accuracy: 0.6488 - val_loss: 0.8701 - val_accuracy: 0.6578\n",
      "Epoch 26/100\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.8840 - accuracy: 0.6383 - val_loss: 0.8624 - val_accuracy: 0.6714\n",
      "Epoch 27/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.8676 - accuracy: 0.6520 - val_loss: 0.8637 - val_accuracy: 0.6667\n",
      "Epoch 28/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.8738 - accuracy: 0.6452 - val_loss: 0.8487 - val_accuracy: 0.6738\n",
      "Epoch 29/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.8650 - accuracy: 0.6520 - val_loss: 0.8579 - val_accuracy: 0.6803\n",
      "Epoch 30/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.8513 - accuracy: 0.6578 - val_loss: 0.8704 - val_accuracy: 0.6531\n",
      "Epoch 31/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.8329 - accuracy: 0.6634 - val_loss: 0.8397 - val_accuracy: 0.6761\n",
      "Epoch 32/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.8426 - accuracy: 0.6608 - val_loss: 0.8369 - val_accuracy: 0.6714\n",
      "Epoch 33/100\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.8231 - accuracy: 0.6698 - val_loss: 0.8345 - val_accuracy: 0.6785\n",
      "Epoch 34/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.8134 - accuracy: 0.6717 - val_loss: 0.8424 - val_accuracy: 0.6720\n",
      "Epoch 35/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.8190 - accuracy: 0.6748 - val_loss: 0.8289 - val_accuracy: 0.6927\n",
      "Epoch 36/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.8098 - accuracy: 0.6776 - val_loss: 0.8198 - val_accuracy: 0.6826\n",
      "Epoch 37/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.8074 - accuracy: 0.6773 - val_loss: 0.8408 - val_accuracy: 0.6637\n",
      "Epoch 38/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.8009 - accuracy: 0.6779 - val_loss: 0.8426 - val_accuracy: 0.6631\n",
      "Epoch 39/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.8023 - accuracy: 0.6809 - val_loss: 0.8319 - val_accuracy: 0.6832\n",
      "Epoch 40/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.8043 - accuracy: 0.6741 - val_loss: 0.8148 - val_accuracy: 0.6956\n",
      "Epoch 41/100\n",
      "106/106 [==============================] - 2s 20ms/step - loss: 0.7922 - accuracy: 0.6887 - val_loss: 0.8299 - val_accuracy: 0.6809\n",
      "Epoch 42/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7845 - accuracy: 0.6791 - val_loss: 0.8024 - val_accuracy: 0.6992\n",
      "Epoch 43/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7882 - accuracy: 0.6850 - val_loss: 0.8214 - val_accuracy: 0.6832\n",
      "Epoch 44/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7877 - accuracy: 0.6884 - val_loss: 0.8515 - val_accuracy: 0.6566\n",
      "Epoch 45/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7699 - accuracy: 0.6950 - val_loss: 0.8334 - val_accuracy: 0.6820\n",
      "Epoch 46/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7727 - accuracy: 0.6884 - val_loss: 0.8113 - val_accuracy: 0.6944\n",
      "Epoch 47/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7543 - accuracy: 0.7001 - val_loss: 0.8008 - val_accuracy: 0.6980\n",
      "Epoch 48/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7724 - accuracy: 0.6905 - val_loss: 0.7975 - val_accuracy: 0.6974\n",
      "Epoch 49/100\n",
      "106/106 [==============================] - 2s 14ms/step - loss: 0.7779 - accuracy: 0.6947 - val_loss: 0.7979 - val_accuracy: 0.7045\n",
      "Epoch 50/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7698 - accuracy: 0.7002 - val_loss: 0.8088 - val_accuracy: 0.6927\n",
      "Epoch 51/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7557 - accuracy: 0.6968 - val_loss: 0.8275 - val_accuracy: 0.6832\n",
      "Epoch 52/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7375 - accuracy: 0.7043 - val_loss: 0.7972 - val_accuracy: 0.7057\n",
      "Epoch 53/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7540 - accuracy: 0.7083 - val_loss: 0.8061 - val_accuracy: 0.6933\n",
      "Epoch 54/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7540 - accuracy: 0.6970 - val_loss: 0.7891 - val_accuracy: 0.7110\n",
      "Epoch 55/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7494 - accuracy: 0.6989 - val_loss: 0.8163 - val_accuracy: 0.6838\n",
      "Epoch 56/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7509 - accuracy: 0.7021 - val_loss: 0.7793 - val_accuracy: 0.7169\n",
      "Epoch 57/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7581 - accuracy: 0.6996 - val_loss: 0.7764 - val_accuracy: 0.7039\n",
      "Epoch 58/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7443 - accuracy: 0.7054 - val_loss: 0.8050 - val_accuracy: 0.7039\n",
      "Epoch 59/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7287 - accuracy: 0.7057 - val_loss: 0.7760 - val_accuracy: 0.7163\n",
      "Epoch 60/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7424 - accuracy: 0.6962 - val_loss: 0.7841 - val_accuracy: 0.6944\n",
      "Epoch 61/100\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 0.7361 - accuracy: 0.7100 - val_loss: 0.7929 - val_accuracy: 0.6879\n",
      "Epoch 62/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7298 - accuracy: 0.7094 - val_loss: 0.7853 - val_accuracy: 0.7222\n",
      "Epoch 63/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7345 - accuracy: 0.7024 - val_loss: 0.7922 - val_accuracy: 0.7092\n",
      "Epoch 64/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7327 - accuracy: 0.7092 - val_loss: 0.7846 - val_accuracy: 0.7021\n",
      "Epoch 65/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7225 - accuracy: 0.7138 - val_loss: 0.7976 - val_accuracy: 0.6921\n",
      "Epoch 66/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7229 - accuracy: 0.7137 - val_loss: 0.7787 - val_accuracy: 0.7169\n",
      "Epoch 67/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7144 - accuracy: 0.7147 - val_loss: 0.7991 - val_accuracy: 0.7045\n",
      "Epoch 68/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7203 - accuracy: 0.7135 - val_loss: 0.7901 - val_accuracy: 0.7015\n",
      "Epoch 69/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7221 - accuracy: 0.7185 - val_loss: 0.7767 - val_accuracy: 0.7116\n",
      "Epoch 70/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7309 - accuracy: 0.7032 - val_loss: 0.7870 - val_accuracy: 0.7021\n",
      "Epoch 71/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7157 - accuracy: 0.7188 - val_loss: 0.7809 - val_accuracy: 0.6950\n",
      "Epoch 72/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7229 - accuracy: 0.7119 - val_loss: 0.8007 - val_accuracy: 0.6891\n",
      "Epoch 73/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7092 - accuracy: 0.7129 - val_loss: 0.7958 - val_accuracy: 0.6933\n",
      "Epoch 74/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7163 - accuracy: 0.7142 - val_loss: 0.7697 - val_accuracy: 0.7069\n",
      "Epoch 75/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7144 - accuracy: 0.7125 - val_loss: 0.7856 - val_accuracy: 0.7128\n",
      "Epoch 76/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7195 - accuracy: 0.7113 - val_loss: 0.7823 - val_accuracy: 0.7051\n",
      "Epoch 77/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7066 - accuracy: 0.7190 - val_loss: 0.7811 - val_accuracy: 0.7021\n",
      "Epoch 78/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7116 - accuracy: 0.7157 - val_loss: 0.7829 - val_accuracy: 0.7021\n",
      "Epoch 79/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7033 - accuracy: 0.7247 - val_loss: 0.7802 - val_accuracy: 0.7015\n",
      "Epoch 80/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7089 - accuracy: 0.7168 - val_loss: 0.7696 - val_accuracy: 0.7169\n",
      "Epoch 81/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7148 - accuracy: 0.7215 - val_loss: 0.7959 - val_accuracy: 0.7051\n",
      "Epoch 82/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7211 - accuracy: 0.7148 - val_loss: 0.7807 - val_accuracy: 0.7086\n",
      "Epoch 83/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.6984 - accuracy: 0.7225 - val_loss: 0.7900 - val_accuracy: 0.7074\n",
      "Epoch 84/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7036 - accuracy: 0.7194 - val_loss: 0.7775 - val_accuracy: 0.7175\n",
      "Epoch 85/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7033 - accuracy: 0.7213 - val_loss: 0.7755 - val_accuracy: 0.7080\n",
      "Epoch 86/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.6927 - accuracy: 0.7193 - val_loss: 0.7567 - val_accuracy: 0.7204\n",
      "Epoch 87/100\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 0.6970 - accuracy: 0.7272 - val_loss: 0.7765 - val_accuracy: 0.7169\n",
      "Epoch 88/100\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 0.7086 - accuracy: 0.7196 - val_loss: 0.7478 - val_accuracy: 0.7270\n",
      "Epoch 89/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7042 - accuracy: 0.7206 - val_loss: 0.7644 - val_accuracy: 0.7323\n",
      "Epoch 90/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.6919 - accuracy: 0.7309 - val_loss: 0.7870 - val_accuracy: 0.7122\n",
      "Epoch 91/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.6894 - accuracy: 0.7302 - val_loss: 0.7637 - val_accuracy: 0.7175\n",
      "Epoch 92/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6875 - accuracy: 0.7253 - val_loss: 0.7626 - val_accuracy: 0.7134\n",
      "Epoch 93/100\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 0.7071 - accuracy: 0.7289 - val_loss: 0.7661 - val_accuracy: 0.7199\n",
      "Epoch 94/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.6801 - accuracy: 0.7292 - val_loss: 0.7773 - val_accuracy: 0.7051\n",
      "Epoch 95/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7080 - accuracy: 0.7221 - val_loss: 0.7735 - val_accuracy: 0.7163\n",
      "Epoch 96/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.6975 - accuracy: 0.7196 - val_loss: 0.7566 - val_accuracy: 0.7222\n",
      "Epoch 97/100\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 0.6958 - accuracy: 0.7228 - val_loss: 0.7537 - val_accuracy: 0.7193\n",
      "Epoch 98/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.6900 - accuracy: 0.7280 - val_loss: 0.7544 - val_accuracy: 0.7092\n",
      "Epoch 99/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.6737 - accuracy: 0.7374 - val_loss: 0.7379 - val_accuracy: 0.7388\n",
      "Epoch 100/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.6692 - accuracy: 0.7376 - val_loss: 0.7615 - val_accuracy: 0.7139\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 100\n",
    "cnn_subject_model_optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "x_train, y_train, x_valid, y_valid, _, _ = preprocess_subjects(subject=2)\n",
    "\n",
    "# Building the CNN model using sequential class\n",
    "cnn_subject_model = Sequential()\n",
    "\n",
    "# Conv. block 1\n",
    "cnn_subject_model.add(Conv2D(filters=20, kernel_size=(5,1), padding='same', activation='elu', input_shape=(250,1,22)))\n",
    "cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same')) \n",
    "cnn_subject_model.add(BatchNormalization())\n",
    "cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "# Conv. block 2\n",
    "cnn_subject_model.add(Conv2D(filters=20, kernel_size=(15,1), padding='same', activation='elu'))\n",
    "cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "cnn_subject_model.add(BatchNormalization())\n",
    "cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "# Conv. block 3\n",
    "cnn_subject_model.add(Conv2D(filters=10, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "cnn_subject_model.add(BatchNormalization())\n",
    "cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "# Output layer with Softmax activation\n",
    "cnn_subject_model.add(Flatten()) # Flattens the input\n",
    "cnn_subject_model.add(Dense(4, activation='softmax')) # Output FC layer with softmax activation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the model summary\n",
    "# cnn_subject_model.summary()\n",
    "cnn_subject_model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=cnn_subject_model_optimizer,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "cnn_subject_model_results = cnn_subject_model.fit(x_train,\n",
    "            y_train,\n",
    "            batch_size=64,\n",
    "            epochs=epochs,\n",
    "            validation_data=(x_valid, y_valid), verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 3ms/step - loss: 0.8490 - accuracy: 0.6400\n",
      "Test accuracy of the CNN model for subject 0: 0.6399999856948853\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 1.0048 - accuracy: 0.5600\n",
      "Test accuracy of the CNN model for subject 1: 0.5600000023841858\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5531 - accuracy: 0.7800\n",
      "Test accuracy of the CNN model for subject 2: 0.7799999713897705\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.7179 - accuracy: 0.6800\n",
      "Test accuracy of the CNN model for subject 3: 0.6800000071525574\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6179 - accuracy: 0.8138\n",
      "Test accuracy of the CNN model for subject 4: 0.813829779624939\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.7460 - accuracy: 0.7245\n",
      "Test accuracy of the CNN model for subject 5: 0.7244898080825806\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.8005 - accuracy: 0.6750\n",
      "Test accuracy of the CNN model for subject 6: 0.675000011920929\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6560 - accuracy: 0.7300\n",
      "Test accuracy of the CNN model for subject 7: 0.7300000190734863\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6320 - accuracy: 0.7660\n",
      "Test accuracy of the CNN model for subject 8: 0.7659574747085571\n"
     ]
    }
   ],
   "source": [
    "subjects = 9\n",
    "for subject in range(subjects):\n",
    "    # tf.keras.backend.clear_session()\n",
    "    _, _, _, _, x_test, y_test = preprocess_subjects(subject=subject)\n",
    "    cnn_subject_model_score = cnn_subject_model.evaluate(x_test, y_test, verbose=True)\n",
    "    print(f'Test accuracy of the CNN model for subject {subject}:',cnn_subject_model_score[1])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate the classification accuracy as a function of time (e.g., does it increase as you have data over longer periods of time? how much time is required to get a reasonable classification accuracy?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load(\"X_test.npy\")\n",
    "y_test = np.load(\"y_test.npy\")\n",
    "person_train_valid = np.load(\"person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"y_train_valid.npy\")\n",
    "person_test = np.load(\"person_test.npy\")\n",
    "\n",
    "## Adjusting the labels so that \n",
    "\n",
    "# Cue onset left - 0\n",
    "# Cue onset right - 1\n",
    "# Cue onset foot - 2\n",
    "# Cue onset tongue - 3\n",
    "\n",
    "y_train_valid -= 769\n",
    "y_test -= 769\n",
    "\n",
    "\n",
    "# shuffle with 5 fold\n",
    "indicies_valid = np.random.choice(X_train_valid.shape[0], X_train_valid.shape[0] // 5, replace=False)\n",
    "indicies_train = np.array(list(set(range(X_train_valid.shape[0])).difference(set(indicies_valid))))\n",
    "\n",
    "# Creating the training and validation sets using the generated indices\n",
    "X_train, X_valid = X_train_valid[indicies_train], X_train_valid[indicies_valid] \n",
    "y_train, y_valid = y_train_valid[indicies_train], y_train_valid[indicies_valid]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep_modular(X,y,sub_sample,average,noise, trim_ratio=0.5):\n",
    "    \n",
    "    total_X = None\n",
    "    total_y = None\n",
    "    \n",
    "    # Trimming the data (sample,22,1000) -> (sample,22,500)\n",
    "    X = X[:,:, 0:(int(X.shape[2] * trim_ratio))]\n",
    "    print('Shape of X after trimming:',X.shape)\n",
    "    \n",
    "    # Maxpooling the data (sample,22,1000) -> (sample,22,500/sub_sample)\n",
    "    X_max = np.max(X.reshape(X.shape[0], X.shape[1], -1, sub_sample), axis=3)\n",
    "    \n",
    "    \n",
    "    total_X = X_max\n",
    "    total_y = y\n",
    "    # print('Shape of X after maxpooling:',total_X.shape)\n",
    "    \n",
    "    # Averaging + noise \n",
    "    X_average = np.mean(X.reshape(X.shape[0], X.shape[1], -1, average), axis=3)\n",
    "    X_average = X_average + np.random.normal(0.0, 0.5, X_average.shape)\n",
    "    \n",
    "    total_X = np.vstack((total_X, X_average))\n",
    "    total_y = np.hstack((total_y, y))\n",
    "    # print('Shape of X after averaging+noise and concatenating:',total_X.shape)\n",
    "    \n",
    "    # Subsampling\n",
    "    \n",
    "    for i in range(sub_sample):\n",
    "        \n",
    "        X_subsample = X[:, :, i::sub_sample] + (np.random.normal(0.0, 0.5, X[:, :,i::sub_sample].shape) if noise else 0.0)\n",
    "        total_X = np.vstack((total_X, X_subsample))\n",
    "        total_y = np.hstack((total_y, y))\n",
    "        \n",
    "    \n",
    "    # print('Shape of X after subsampling and concatenating:',total_X.shape)\n",
    "    return total_X,total_y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_time(trim_ratio):\n",
    "    X_test = np.load(\"X_test.npy\")\n",
    "    y_test = np.load(\"y_test.npy\")\n",
    "    person_train_valid = np.load(\"person_train_valid.npy\")\n",
    "    X_train_valid = np.load(\"X_train_valid.npy\")\n",
    "    y_train_valid = np.load(\"y_train_valid.npy\")\n",
    "    person_test = np.load(\"person_test.npy\")\n",
    "\n",
    "    ## Adjusting the labels so that \n",
    "\n",
    "    # Cue onset left - 0\n",
    "    # Cue onset right - 1\n",
    "    # Cue onset foot - 2\n",
    "    # Cue onset tongue - 3\n",
    "\n",
    "    y_train_valid -= 769\n",
    "    y_test -= 769\n",
    "\n",
    "\n",
    "    # shuffle with 5 fold\n",
    "    indicies_valid = np.random.choice(X_train_valid.shape[0], X_train_valid.shape[0] // 5, replace=False)\n",
    "    indicies_train = np.array(list(set(range(X_train_valid.shape[0])).difference(set(indicies_valid))))\n",
    "\n",
    "    # Creating the training and validation sets using the generated indices\n",
    "    X_train, X_valid = X_train_valid[indicies_train], X_train_valid[indicies_valid] \n",
    "    y_train, y_valid = y_train_valid[indicies_train], y_train_valid[indicies_valid]\n",
    "\n",
    "\n",
    "\n",
    "    # Preprocessing the dataset\n",
    "    x_train,y_train = data_prep_modular(X_train,y_train,2,2,True, trim_ratio=trim_ratio)\n",
    "    x_valid,y_valid = data_prep_modular(X_valid,y_valid,2,2,True, trim_ratio=trim_ratio)\n",
    "    X_test_prep,y_test_prep = data_prep_modular(X_test,y_test,2,2,True, trim_ratio=trim_ratio)\n",
    "\n",
    "    print('Shape of training set:',x_train.shape)\n",
    "    print('Shape of validation set:',x_valid.shape)\n",
    "    print('Shape of training labels:',y_train.shape)\n",
    "    print('Shape of validation labels:',y_valid.shape)\n",
    "    print('Shape of testing set:',X_test_prep.shape)\n",
    "    print('Shape of testing labels:',y_test_prep.shape)\n",
    "\n",
    "    # Converting the labels to categorical variables for multiclass classification\n",
    "    y_train = to_categorical(y_train, 4)\n",
    "    y_valid = to_categorical(y_valid, 4)\n",
    "    y_test = to_categorical(y_test_prep, 4)\n",
    "    print('Shape of training labels after categorical conversion:',y_train.shape)\n",
    "    print('Shape of validation labels after categorical conversion:',y_valid.shape)\n",
    "    print('Shape of test labels after categorical conversion:',y_test.shape)\n",
    "\n",
    "    # Adding width of the segment to be 1\n",
    "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "    x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)\n",
    "    x_test = X_test_prep.reshape(X_test_prep.shape[0], X_test_prep.shape[1], X_test_prep.shape[2], 1)\n",
    "    print('Shape of training set after adding width info:',x_train.shape)\n",
    "    print('Shape of validation set after adding width info:',x_valid.shape)\n",
    "    print('Shape of test set after adding width info:',x_test.shape)\n",
    "\n",
    "    # Reshaping the training and validation dataset\n",
    "    x_train = np.swapaxes(x_train, 1,3)\n",
    "    x_train = np.swapaxes(x_train, 1,2)\n",
    "    x_valid = np.swapaxes(x_valid, 1,3)\n",
    "    x_valid = np.swapaxes(x_valid, 1,2)\n",
    "    x_test = np.swapaxes(x_test, 1,3)\n",
    "    x_test = np.swapaxes(x_test, 1,2)\n",
    "    print('Shape of training set after dimension reshaping:',x_train.shape)\n",
    "    print('Shape of validation set after dimension reshaping:',x_valid.shape)\n",
    "    print('Shape of test set after dimension reshaping:',x_test.shape)\n",
    "\n",
    "\n",
    "    return (x_train, y_train, x_valid, y_valid, x_test, y_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model / Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(trim_ratio):\n",
    "    learning_rate = 1e-3\n",
    "    epochs = 100\n",
    "    cnn_subject_model_optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test = preprocess_time(trim_ratio=trim_ratio)\n",
    "\n",
    "    # Building the CNN model using sequential class\n",
    "    cnn = Sequential()\n",
    "\n",
    "    # Conv. block 1\n",
    "    cnn.add(Conv2D(filters=20, kernel_size=(5,1), padding='same', activation='elu', input_shape=(x_train.shape[1],1,22)))\n",
    "    cnn.add(MaxPooling2D(pool_size=(3,1), padding='same')) \n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(Dropout(0.5))\n",
    "\n",
    "    # Conv. block 2\n",
    "    cnn.add(Conv2D(filters=20, kernel_size=(15,1), padding='same', activation='elu'))\n",
    "    cnn.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(Dropout(0.5))\n",
    "\n",
    "    # Conv. block 3\n",
    "    cnn.add(Conv2D(filters=10, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "    cnn.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "    # Output layer with Softmax activation\n",
    "    cnn.add(Flatten()) # Flattens the input\n",
    "    cnn.add(Dense(4, activation='softmax')) # Output FC layer with softmax activation\n",
    "\n",
    "    cnn.compile(loss='categorical_crossentropy',\n",
    "                    optimizer=cnn_subject_model_optimizer,\n",
    "                    metrics=['accuracy'])\n",
    "    \n",
    "    cnn_training_results = cnn.fit(x_train,\n",
    "            y_train,\n",
    "            batch_size=64,\n",
    "            epochs=epochs,\n",
    "            validation_data=(x_valid, y_valid), verbose=True)\n",
    "    \n",
    "    cnn_test_score = cnn.evaluate(x_test, y_test, verbose=True)\n",
    "    print(f'Test Accuracy: {cnn_test_score[1]}')\n",
    "    return cnn, cnn_training_results, cnn_test_score\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training /Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X after trimming: (1692, 22, 100)\n",
      "Shape of X after trimming: (423, 22, 100)\n",
      "Shape of X after trimming: (443, 22, 100)\n",
      "Shape of training set: (6768, 22, 50)\n",
      "Shape of validation set: (1692, 22, 50)\n",
      "Shape of training labels: (6768,)\n",
      "Shape of validation labels: (1692,)\n",
      "Shape of testing set: (1772, 22, 50)\n",
      "Shape of testing labels: (1772,)\n",
      "Shape of training labels after categorical conversion: (6768, 4)\n",
      "Shape of validation labels after categorical conversion: (1692, 4)\n",
      "Shape of test labels after categorical conversion: (1772, 4)\n",
      "Shape of training set after adding width info: (6768, 22, 50, 1)\n",
      "Shape of validation set after adding width info: (1692, 22, 50, 1)\n",
      "Shape of test set after adding width info: (1772, 22, 50, 1)\n",
      "Shape of training set after dimension reshaping: (6768, 50, 1, 22)\n",
      "Shape of validation set after dimension reshaping: (1692, 50, 1, 22)\n",
      "Shape of test set after dimension reshaping: (1772, 50, 1, 22)\n",
      "Epoch 1/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.9433 - accuracy: 0.2574 - val_loss: 1.4690 - val_accuracy: 0.3056\n",
      "Epoch 2/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.6423 - accuracy: 0.2766 - val_loss: 1.3885 - val_accuracy: 0.3150\n",
      "Epoch 3/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.5308 - accuracy: 0.2757 - val_loss: 1.3760 - val_accuracy: 0.3327\n",
      "Epoch 4/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.4444 - accuracy: 0.2986 - val_loss: 1.3654 - val_accuracy: 0.3440\n",
      "Epoch 5/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.4062 - accuracy: 0.3082 - val_loss: 1.3544 - val_accuracy: 0.3647\n",
      "Epoch 6/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.3823 - accuracy: 0.3143 - val_loss: 1.3446 - val_accuracy: 0.3989\n",
      "Epoch 7/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.3652 - accuracy: 0.3304 - val_loss: 1.3384 - val_accuracy: 0.3972\n",
      "Epoch 8/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.3471 - accuracy: 0.3462 - val_loss: 1.3312 - val_accuracy: 0.3913\n",
      "Epoch 9/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.3383 - accuracy: 0.3528 - val_loss: 1.3297 - val_accuracy: 0.3830\n",
      "Epoch 10/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.3235 - accuracy: 0.3568 - val_loss: 1.3136 - val_accuracy: 0.4108\n",
      "Epoch 11/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.3141 - accuracy: 0.3726 - val_loss: 1.3038 - val_accuracy: 0.4184\n",
      "Epoch 12/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.3097 - accuracy: 0.3800 - val_loss: 1.2887 - val_accuracy: 0.4462\n",
      "Epoch 13/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.2931 - accuracy: 0.3976 - val_loss: 1.3132 - val_accuracy: 0.3995\n",
      "Epoch 14/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.2824 - accuracy: 0.4128 - val_loss: 1.2799 - val_accuracy: 0.4391\n",
      "Epoch 15/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.2804 - accuracy: 0.4085 - val_loss: 1.2855 - val_accuracy: 0.4279\n",
      "Epoch 16/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.2636 - accuracy: 0.4181 - val_loss: 1.2526 - val_accuracy: 0.4527\n",
      "Epoch 17/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.2596 - accuracy: 0.4285 - val_loss: 1.2511 - val_accuracy: 0.4462\n",
      "Epoch 18/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.2445 - accuracy: 0.4365 - val_loss: 1.2346 - val_accuracy: 0.4781\n",
      "Epoch 19/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.2395 - accuracy: 0.4428 - val_loss: 1.3006 - val_accuracy: 0.3712\n",
      "Epoch 20/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.2268 - accuracy: 0.4410 - val_loss: 1.2290 - val_accuracy: 0.4569\n",
      "Epoch 21/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.2085 - accuracy: 0.4594 - val_loss: 1.2036 - val_accuracy: 0.5030\n",
      "Epoch 22/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.1996 - accuracy: 0.4663 - val_loss: 1.1992 - val_accuracy: 0.4734\n",
      "Epoch 23/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.1884 - accuracy: 0.4743 - val_loss: 1.2167 - val_accuracy: 0.4545\n",
      "Epoch 24/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.1828 - accuracy: 0.4843 - val_loss: 1.1892 - val_accuracy: 0.4775\n",
      "Epoch 25/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.1801 - accuracy: 0.4768 - val_loss: 1.1932 - val_accuracy: 0.4574\n",
      "Epoch 26/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.1804 - accuracy: 0.4752 - val_loss: 1.1771 - val_accuracy: 0.4811\n",
      "Epoch 27/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.1748 - accuracy: 0.4814 - val_loss: 1.2221 - val_accuracy: 0.4161\n",
      "Epoch 28/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.1787 - accuracy: 0.4786 - val_loss: 1.1751 - val_accuracy: 0.4870\n",
      "Epoch 29/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.1575 - accuracy: 0.4929 - val_loss: 1.1676 - val_accuracy: 0.4787\n",
      "Epoch 30/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.1509 - accuracy: 0.4957 - val_loss: 1.1633 - val_accuracy: 0.4935\n",
      "Epoch 31/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.1459 - accuracy: 0.4991 - val_loss: 1.1620 - val_accuracy: 0.4811\n",
      "Epoch 32/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.1296 - accuracy: 0.5087 - val_loss: 1.1711 - val_accuracy: 0.4823\n",
      "Epoch 33/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.1389 - accuracy: 0.5069 - val_loss: 1.1472 - val_accuracy: 0.4852\n",
      "Epoch 34/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.1289 - accuracy: 0.5016 - val_loss: 1.1683 - val_accuracy: 0.4746\n",
      "Epoch 35/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.1215 - accuracy: 0.5129 - val_loss: 1.1470 - val_accuracy: 0.4870\n",
      "Epoch 36/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.1122 - accuracy: 0.5194 - val_loss: 1.1463 - val_accuracy: 0.4953\n",
      "Epoch 37/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.1200 - accuracy: 0.5117 - val_loss: 1.1474 - val_accuracy: 0.4840\n",
      "Epoch 38/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.1066 - accuracy: 0.5270 - val_loss: 1.1317 - val_accuracy: 0.5006\n",
      "Epoch 39/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0975 - accuracy: 0.5263 - val_loss: 1.1683 - val_accuracy: 0.4704\n",
      "Epoch 40/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.1025 - accuracy: 0.5291 - val_loss: 1.1390 - val_accuracy: 0.4823\n",
      "Epoch 41/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.1035 - accuracy: 0.5300 - val_loss: 1.1184 - val_accuracy: 0.5083\n",
      "Epoch 42/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.1009 - accuracy: 0.5232 - val_loss: 1.1209 - val_accuracy: 0.5100\n",
      "Epoch 43/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.0917 - accuracy: 0.5313 - val_loss: 1.1379 - val_accuracy: 0.4870\n",
      "Epoch 44/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.0845 - accuracy: 0.5328 - val_loss: 1.1255 - val_accuracy: 0.5171\n",
      "Epoch 45/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.0835 - accuracy: 0.5341 - val_loss: 1.1206 - val_accuracy: 0.5118\n",
      "Epoch 46/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.0727 - accuracy: 0.5452 - val_loss: 1.1265 - val_accuracy: 0.4976\n",
      "Epoch 47/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0782 - accuracy: 0.5384 - val_loss: 1.0979 - val_accuracy: 0.5272\n",
      "Epoch 48/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0777 - accuracy: 0.5424 - val_loss: 1.1129 - val_accuracy: 0.5118\n",
      "Epoch 49/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0728 - accuracy: 0.5493 - val_loss: 1.1163 - val_accuracy: 0.5041\n",
      "Epoch 50/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0645 - accuracy: 0.5526 - val_loss: 1.1221 - val_accuracy: 0.5112\n",
      "Epoch 51/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0663 - accuracy: 0.5437 - val_loss: 1.1267 - val_accuracy: 0.4929\n",
      "Epoch 52/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0578 - accuracy: 0.5533 - val_loss: 1.1019 - val_accuracy: 0.5154\n",
      "Epoch 53/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0697 - accuracy: 0.5455 - val_loss: 1.1052 - val_accuracy: 0.5059\n",
      "Epoch 54/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0699 - accuracy: 0.5400 - val_loss: 1.1382 - val_accuracy: 0.4923\n",
      "Epoch 55/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0567 - accuracy: 0.5547 - val_loss: 1.1171 - val_accuracy: 0.5053\n",
      "Epoch 56/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0513 - accuracy: 0.5569 - val_loss: 1.1184 - val_accuracy: 0.5165\n",
      "Epoch 57/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.0529 - accuracy: 0.5559 - val_loss: 1.1182 - val_accuracy: 0.5089\n",
      "Epoch 58/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0501 - accuracy: 0.5517 - val_loss: 1.1654 - val_accuracy: 0.4710\n",
      "Epoch 59/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0483 - accuracy: 0.5553 - val_loss: 1.1414 - val_accuracy: 0.4846\n",
      "Epoch 60/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0490 - accuracy: 0.5513 - val_loss: 1.1208 - val_accuracy: 0.5041\n",
      "Epoch 61/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0549 - accuracy: 0.5567 - val_loss: 1.1446 - val_accuracy: 0.4894\n",
      "Epoch 62/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.0432 - accuracy: 0.5647 - val_loss: 1.1218 - val_accuracy: 0.5053\n",
      "Epoch 63/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0363 - accuracy: 0.5683 - val_loss: 1.1235 - val_accuracy: 0.5000\n",
      "Epoch 64/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0375 - accuracy: 0.5643 - val_loss: 1.1021 - val_accuracy: 0.5124\n",
      "Epoch 65/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0365 - accuracy: 0.5590 - val_loss: 1.0886 - val_accuracy: 0.5189\n",
      "Epoch 66/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0301 - accuracy: 0.5624 - val_loss: 1.0872 - val_accuracy: 0.5219\n",
      "Epoch 67/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.0359 - accuracy: 0.5601 - val_loss: 1.1593 - val_accuracy: 0.4722\n",
      "Epoch 68/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0314 - accuracy: 0.5647 - val_loss: 1.1261 - val_accuracy: 0.5177\n",
      "Epoch 69/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0289 - accuracy: 0.5615 - val_loss: 1.0935 - val_accuracy: 0.5236\n",
      "Epoch 70/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0151 - accuracy: 0.5759 - val_loss: 1.1061 - val_accuracy: 0.5195\n",
      "Epoch 71/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0211 - accuracy: 0.5672 - val_loss: 1.1118 - val_accuracy: 0.5071\n",
      "Epoch 72/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0341 - accuracy: 0.5624 - val_loss: 1.1565 - val_accuracy: 0.4835\n",
      "Epoch 73/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.0225 - accuracy: 0.5706 - val_loss: 1.1128 - val_accuracy: 0.5024\n",
      "Epoch 74/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0159 - accuracy: 0.5696 - val_loss: 1.1209 - val_accuracy: 0.5071\n",
      "Epoch 75/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0269 - accuracy: 0.5730 - val_loss: 1.0910 - val_accuracy: 0.5254\n",
      "Epoch 76/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0000 - accuracy: 0.5799 - val_loss: 1.1501 - val_accuracy: 0.4911\n",
      "Epoch 77/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0220 - accuracy: 0.5659 - val_loss: 1.1109 - val_accuracy: 0.5100\n",
      "Epoch 78/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.0176 - accuracy: 0.5715 - val_loss: 1.0832 - val_accuracy: 0.5225\n",
      "Epoch 79/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0198 - accuracy: 0.5718 - val_loss: 1.0912 - val_accuracy: 0.5313\n",
      "Epoch 80/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0082 - accuracy: 0.5792 - val_loss: 1.1300 - val_accuracy: 0.4888\n",
      "Epoch 81/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0090 - accuracy: 0.5771 - val_loss: 1.1055 - val_accuracy: 0.5248\n",
      "Epoch 82/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 0.9893 - accuracy: 0.5885 - val_loss: 1.1195 - val_accuracy: 0.5041\n",
      "Epoch 83/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 1.0258 - accuracy: 0.5660 - val_loss: 1.1326 - val_accuracy: 0.4941\n",
      "Epoch 84/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0067 - accuracy: 0.5758 - val_loss: 1.0884 - val_accuracy: 0.5307\n",
      "Epoch 85/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0132 - accuracy: 0.5720 - val_loss: 1.1262 - val_accuracy: 0.5000\n",
      "Epoch 86/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 0.9881 - accuracy: 0.5830 - val_loss: 1.1308 - val_accuracy: 0.5024\n",
      "Epoch 87/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0103 - accuracy: 0.5736 - val_loss: 1.1196 - val_accuracy: 0.5024\n",
      "Epoch 88/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 0.9998 - accuracy: 0.5807 - val_loss: 1.1046 - val_accuracy: 0.5000\n",
      "Epoch 89/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 0.9987 - accuracy: 0.5804 - val_loss: 1.0867 - val_accuracy: 0.5171\n",
      "Epoch 90/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0066 - accuracy: 0.5822 - val_loss: 1.1125 - val_accuracy: 0.5154\n",
      "Epoch 91/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 1.0027 - accuracy: 0.5783 - val_loss: 1.0963 - val_accuracy: 0.5219\n",
      "Epoch 92/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 0.9954 - accuracy: 0.5792 - val_loss: 1.0833 - val_accuracy: 0.5307\n",
      "Epoch 93/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 0.9828 - accuracy: 0.5819 - val_loss: 1.0999 - val_accuracy: 0.5189\n",
      "Epoch 94/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 0.9964 - accuracy: 0.5731 - val_loss: 1.0985 - val_accuracy: 0.5171\n",
      "Epoch 95/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 0.9965 - accuracy: 0.5885 - val_loss: 1.1119 - val_accuracy: 0.5030\n",
      "Epoch 96/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 0.9786 - accuracy: 0.5903 - val_loss: 1.1191 - val_accuracy: 0.5189\n",
      "Epoch 97/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 0.9980 - accuracy: 0.5805 - val_loss: 1.1292 - val_accuracy: 0.4959\n",
      "Epoch 98/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 0.9896 - accuracy: 0.5795 - val_loss: 1.0989 - val_accuracy: 0.5361\n",
      "Epoch 99/100\n",
      "106/106 [==============================] - 0s 5ms/step - loss: 0.9884 - accuracy: 0.5900 - val_loss: 1.0810 - val_accuracy: 0.5301\n",
      "Epoch 100/100\n",
      "106/106 [==============================] - 1s 5ms/step - loss: 0.9782 - accuracy: 0.5906 - val_loss: 1.0877 - val_accuracy: 0.5296\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 1.0414 - accuracy: 0.5440\n",
      "Shape of X after trimming: (1692, 22, 200)\n",
      "Shape of X after trimming: (423, 22, 200)\n",
      "Shape of X after trimming: (443, 22, 200)\n",
      "Shape of training set: (6768, 22, 100)\n",
      "Shape of validation set: (1692, 22, 100)\n",
      "Shape of training labels: (6768,)\n",
      "Shape of validation labels: (1692,)\n",
      "Shape of testing set: (1772, 22, 100)\n",
      "Shape of testing labels: (1772,)\n",
      "Shape of training labels after categorical conversion: (6768, 4)\n",
      "Shape of validation labels after categorical conversion: (1692, 4)\n",
      "Shape of test labels after categorical conversion: (1772, 4)\n",
      "Shape of training set after adding width info: (6768, 22, 100, 1)\n",
      "Shape of validation set after adding width info: (1692, 22, 100, 1)\n",
      "Shape of test set after adding width info: (1772, 22, 100, 1)\n",
      "Shape of training set after dimension reshaping: (6768, 100, 1, 22)\n",
      "Shape of validation set after dimension reshaping: (1692, 100, 1, 22)\n",
      "Shape of test set after dimension reshaping: (1772, 100, 1, 22)\n",
      "Epoch 1/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 1.8739 - accuracy: 0.2807 - val_loss: 1.4497 - val_accuracy: 0.3322\n",
      "Epoch 2/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 1.5863 - accuracy: 0.2961 - val_loss: 1.3445 - val_accuracy: 0.3410\n",
      "Epoch 3/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.4638 - accuracy: 0.3112 - val_loss: 1.3284 - val_accuracy: 0.3676\n",
      "Epoch 4/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.3829 - accuracy: 0.3423 - val_loss: 1.3283 - val_accuracy: 0.3682\n",
      "Epoch 5/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 1.3341 - accuracy: 0.3611 - val_loss: 1.3030 - val_accuracy: 0.3995\n",
      "Epoch 6/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.3112 - accuracy: 0.3849 - val_loss: 1.2855 - val_accuracy: 0.4119\n",
      "Epoch 7/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.2895 - accuracy: 0.4025 - val_loss: 1.2678 - val_accuracy: 0.4385\n",
      "Epoch 8/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.2603 - accuracy: 0.4193 - val_loss: 1.2644 - val_accuracy: 0.4409\n",
      "Epoch 9/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 1.2572 - accuracy: 0.4306 - val_loss: 1.2860 - val_accuracy: 0.4178\n",
      "Epoch 10/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.2371 - accuracy: 0.4480 - val_loss: 1.2303 - val_accuracy: 0.4527\n",
      "Epoch 11/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.2153 - accuracy: 0.4594 - val_loss: 1.2246 - val_accuracy: 0.4397\n",
      "Epoch 12/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 1.2075 - accuracy: 0.4576 - val_loss: 1.2036 - val_accuracy: 0.4569\n",
      "Epoch 13/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.1920 - accuracy: 0.4733 - val_loss: 1.1934 - val_accuracy: 0.4687\n",
      "Epoch 14/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.1847 - accuracy: 0.4780 - val_loss: 1.1726 - val_accuracy: 0.4970\n",
      "Epoch 15/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.1720 - accuracy: 0.4768 - val_loss: 1.1639 - val_accuracy: 0.5035\n",
      "Epoch 16/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.1599 - accuracy: 0.4898 - val_loss: 1.1467 - val_accuracy: 0.5030\n",
      "Epoch 17/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.1443 - accuracy: 0.5095 - val_loss: 1.1822 - val_accuracy: 0.4586\n",
      "Epoch 18/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.1308 - accuracy: 0.5132 - val_loss: 1.1482 - val_accuracy: 0.4876\n",
      "Epoch 19/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 1.1252 - accuracy: 0.5140 - val_loss: 1.1132 - val_accuracy: 0.5431\n",
      "Epoch 20/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.1286 - accuracy: 0.5173 - val_loss: 1.1081 - val_accuracy: 0.5301\n",
      "Epoch 21/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.1043 - accuracy: 0.5270 - val_loss: 1.0936 - val_accuracy: 0.5307\n",
      "Epoch 22/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.0910 - accuracy: 0.5414 - val_loss: 1.1091 - val_accuracy: 0.4982\n",
      "Epoch 23/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 1.0925 - accuracy: 0.5369 - val_loss: 1.0848 - val_accuracy: 0.5225\n",
      "Epoch 24/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.0805 - accuracy: 0.5443 - val_loss: 1.0810 - val_accuracy: 0.5307\n",
      "Epoch 25/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.0724 - accuracy: 0.5483 - val_loss: 1.0877 - val_accuracy: 0.5183\n",
      "Epoch 26/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 1.0652 - accuracy: 0.5542 - val_loss: 1.0898 - val_accuracy: 0.5095\n",
      "Epoch 27/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 1.0521 - accuracy: 0.5536 - val_loss: 1.0719 - val_accuracy: 0.5402\n",
      "Epoch 28/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.0469 - accuracy: 0.5671 - val_loss: 1.0899 - val_accuracy: 0.5077\n",
      "Epoch 29/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 1.0442 - accuracy: 0.5606 - val_loss: 1.0606 - val_accuracy: 0.5337\n",
      "Epoch 30/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.0364 - accuracy: 0.5709 - val_loss: 1.0643 - val_accuracy: 0.5461\n",
      "Epoch 31/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.0322 - accuracy: 0.5697 - val_loss: 1.0729 - val_accuracy: 0.5366\n",
      "Epoch 32/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.0301 - accuracy: 0.5788 - val_loss: 1.0512 - val_accuracy: 0.5278\n",
      "Epoch 33/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 1.0258 - accuracy: 0.5748 - val_loss: 1.0426 - val_accuracy: 0.5313\n",
      "Epoch 34/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.0061 - accuracy: 0.5770 - val_loss: 1.0644 - val_accuracy: 0.5219\n",
      "Epoch 35/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 1.0007 - accuracy: 0.5900 - val_loss: 1.0370 - val_accuracy: 0.5561\n",
      "Epoch 36/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 1.0028 - accuracy: 0.5850 - val_loss: 1.0198 - val_accuracy: 0.5626\n",
      "Epoch 37/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.9978 - accuracy: 0.5817 - val_loss: 1.0413 - val_accuracy: 0.5550\n",
      "Epoch 38/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.9871 - accuracy: 0.5960 - val_loss: 1.0333 - val_accuracy: 0.5455\n",
      "Epoch 39/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.9777 - accuracy: 0.5953 - val_loss: 1.0353 - val_accuracy: 0.5626\n",
      "Epoch 40/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.9754 - accuracy: 0.5996 - val_loss: 1.0375 - val_accuracy: 0.5544\n",
      "Epoch 41/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.9807 - accuracy: 0.5919 - val_loss: 1.0347 - val_accuracy: 0.5609\n",
      "Epoch 42/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.9821 - accuracy: 0.5830 - val_loss: 1.0303 - val_accuracy: 0.5691\n",
      "Epoch 43/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.9745 - accuracy: 0.5978 - val_loss: 1.0076 - val_accuracy: 0.5579\n",
      "Epoch 44/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.9768 - accuracy: 0.5997 - val_loss: 1.0099 - val_accuracy: 0.5479\n",
      "Epoch 45/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.9820 - accuracy: 0.5937 - val_loss: 0.9978 - val_accuracy: 0.5615\n",
      "Epoch 46/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.9649 - accuracy: 0.6012 - val_loss: 0.9972 - val_accuracy: 0.5745\n",
      "Epoch 47/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.9614 - accuracy: 0.6028 - val_loss: 0.9994 - val_accuracy: 0.5875\n",
      "Epoch 48/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.9329 - accuracy: 0.6200 - val_loss: 0.9997 - val_accuracy: 0.5751\n",
      "Epoch 49/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.9579 - accuracy: 0.6071 - val_loss: 0.9804 - val_accuracy: 0.5869\n",
      "Epoch 50/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.9495 - accuracy: 0.6116 - val_loss: 0.9885 - val_accuracy: 0.5804\n",
      "Epoch 51/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.9569 - accuracy: 0.6045 - val_loss: 0.9670 - val_accuracy: 0.5816\n",
      "Epoch 52/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.9424 - accuracy: 0.6210 - val_loss: 0.9707 - val_accuracy: 0.6070\n",
      "Epoch 53/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.9495 - accuracy: 0.6104 - val_loss: 0.9779 - val_accuracy: 0.5940\n",
      "Epoch 54/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.9451 - accuracy: 0.6104 - val_loss: 0.9770 - val_accuracy: 0.5827\n",
      "Epoch 55/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.9370 - accuracy: 0.6136 - val_loss: 0.9720 - val_accuracy: 0.6028\n",
      "Epoch 56/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.9424 - accuracy: 0.6113 - val_loss: 0.9997 - val_accuracy: 0.5786\n",
      "Epoch 57/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.9350 - accuracy: 0.6200 - val_loss: 0.9597 - val_accuracy: 0.6070\n",
      "Epoch 58/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.9352 - accuracy: 0.6175 - val_loss: 0.9853 - val_accuracy: 0.5922\n",
      "Epoch 59/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.9216 - accuracy: 0.6163 - val_loss: 0.9991 - val_accuracy: 0.5686\n",
      "Epoch 60/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.9219 - accuracy: 0.6192 - val_loss: 0.9866 - val_accuracy: 0.5934\n",
      "Epoch 61/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.9307 - accuracy: 0.6152 - val_loss: 0.9734 - val_accuracy: 0.6058\n",
      "Epoch 62/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.9169 - accuracy: 0.6226 - val_loss: 0.9790 - val_accuracy: 0.5969\n",
      "Epoch 63/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.9312 - accuracy: 0.6215 - val_loss: 0.9816 - val_accuracy: 0.5804\n",
      "Epoch 64/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.9197 - accuracy: 0.6288 - val_loss: 0.9570 - val_accuracy: 0.5963\n",
      "Epoch 65/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.9147 - accuracy: 0.6260 - val_loss: 0.9659 - val_accuracy: 0.5981\n",
      "Epoch 66/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.9170 - accuracy: 0.6293 - val_loss: 0.9557 - val_accuracy: 0.5946\n",
      "Epoch 67/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.9209 - accuracy: 0.6184 - val_loss: 0.9643 - val_accuracy: 0.5963\n",
      "Epoch 68/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.9349 - accuracy: 0.6132 - val_loss: 0.9858 - val_accuracy: 0.5904\n",
      "Epoch 69/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.9151 - accuracy: 0.6246 - val_loss: 0.9567 - val_accuracy: 0.6076\n",
      "Epoch 70/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.9055 - accuracy: 0.6240 - val_loss: 0.9712 - val_accuracy: 0.5987\n",
      "Epoch 71/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.9168 - accuracy: 0.6278 - val_loss: 0.9523 - val_accuracy: 0.6052\n",
      "Epoch 72/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.9160 - accuracy: 0.6232 - val_loss: 0.9571 - val_accuracy: 0.6082\n",
      "Epoch 73/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.9034 - accuracy: 0.6302 - val_loss: 0.9609 - val_accuracy: 0.6117\n",
      "Epoch 74/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.8916 - accuracy: 0.6386 - val_loss: 0.9559 - val_accuracy: 0.6046\n",
      "Epoch 75/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.8969 - accuracy: 0.6331 - val_loss: 0.9602 - val_accuracy: 0.5881\n",
      "Epoch 76/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.8914 - accuracy: 0.6376 - val_loss: 0.9694 - val_accuracy: 0.5892\n",
      "Epoch 77/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.8887 - accuracy: 0.6389 - val_loss: 0.9348 - val_accuracy: 0.6099\n",
      "Epoch 78/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.8928 - accuracy: 0.6325 - val_loss: 0.9690 - val_accuracy: 0.5987\n",
      "Epoch 79/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.8928 - accuracy: 0.6415 - val_loss: 0.9542 - val_accuracy: 0.6017\n",
      "Epoch 80/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.8968 - accuracy: 0.6325 - val_loss: 0.9606 - val_accuracy: 0.6082\n",
      "Epoch 81/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.8978 - accuracy: 0.6325 - val_loss: 0.9528 - val_accuracy: 0.6070\n",
      "Epoch 82/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.9020 - accuracy: 0.6305 - val_loss: 0.9623 - val_accuracy: 0.6034\n",
      "Epoch 83/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.8971 - accuracy: 0.6361 - val_loss: 0.9426 - val_accuracy: 0.6105\n",
      "Epoch 84/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.8929 - accuracy: 0.6411 - val_loss: 0.9290 - val_accuracy: 0.6271\n",
      "Epoch 85/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.8799 - accuracy: 0.6463 - val_loss: 0.9668 - val_accuracy: 0.6099\n",
      "Epoch 86/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.8864 - accuracy: 0.6413 - val_loss: 1.0036 - val_accuracy: 0.5922\n",
      "Epoch 87/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.8926 - accuracy: 0.6331 - val_loss: 0.9482 - val_accuracy: 0.6147\n",
      "Epoch 88/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.8783 - accuracy: 0.6382 - val_loss: 0.9556 - val_accuracy: 0.6087\n",
      "Epoch 89/100\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.8799 - accuracy: 0.6435 - val_loss: 0.9443 - val_accuracy: 0.6052\n",
      "Epoch 90/100\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.8889 - accuracy: 0.6325 - val_loss: 0.9266 - val_accuracy: 0.6277\n",
      "Epoch 91/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.8908 - accuracy: 0.6356 - val_loss: 0.9402 - val_accuracy: 0.6188\n",
      "Epoch 92/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.8879 - accuracy: 0.6485 - val_loss: 0.9700 - val_accuracy: 0.6105\n",
      "Epoch 93/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.8767 - accuracy: 0.6478 - val_loss: 0.9456 - val_accuracy: 0.6182\n",
      "Epoch 94/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.8818 - accuracy: 0.6424 - val_loss: 0.9326 - val_accuracy: 0.6265\n",
      "Epoch 95/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.8702 - accuracy: 0.6438 - val_loss: 0.9348 - val_accuracy: 0.6111\n",
      "Epoch 96/100\n",
      "106/106 [==============================] - 1s 6ms/step - loss: 0.8821 - accuracy: 0.6424 - val_loss: 0.9371 - val_accuracy: 0.6259\n",
      "Epoch 97/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.8610 - accuracy: 0.6506 - val_loss: 0.9544 - val_accuracy: 0.6087\n",
      "Epoch 98/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.8787 - accuracy: 0.6451 - val_loss: 0.9671 - val_accuracy: 0.6170\n",
      "Epoch 99/100\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.8778 - accuracy: 0.6417 - val_loss: 0.9491 - val_accuracy: 0.6117\n",
      "Epoch 100/100\n",
      "106/106 [==============================] - 1s 7ms/step - loss: 0.8708 - accuracy: 0.6469 - val_loss: 0.9327 - val_accuracy: 0.6259\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.9075 - accuracy: 0.6247\n",
      "Shape of X after trimming: (1692, 22, 300)\n",
      "Shape of X after trimming: (423, 22, 300)\n",
      "Shape of X after trimming: (443, 22, 300)\n",
      "Shape of training set: (6768, 22, 150)\n",
      "Shape of validation set: (1692, 22, 150)\n",
      "Shape of training labels: (6768,)\n",
      "Shape of validation labels: (1692,)\n",
      "Shape of testing set: (1772, 22, 150)\n",
      "Shape of testing labels: (1772,)\n",
      "Shape of training labels after categorical conversion: (6768, 4)\n",
      "Shape of validation labels after categorical conversion: (1692, 4)\n",
      "Shape of test labels after categorical conversion: (1772, 4)\n",
      "Shape of training set after adding width info: (6768, 22, 150, 1)\n",
      "Shape of validation set after adding width info: (1692, 22, 150, 1)\n",
      "Shape of test set after adding width info: (1772, 22, 150, 1)\n",
      "Shape of training set after dimension reshaping: (6768, 150, 1, 22)\n",
      "Shape of validation set after dimension reshaping: (1692, 150, 1, 22)\n",
      "Shape of test set after dimension reshaping: (1772, 150, 1, 22)\n",
      "Epoch 1/100\n",
      "106/106 [==============================] - 2s 11ms/step - loss: 1.9796 - accuracy: 0.2639 - val_loss: 1.4382 - val_accuracy: 0.3050\n",
      "Epoch 2/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 1.6357 - accuracy: 0.2920 - val_loss: 1.3484 - val_accuracy: 0.3286\n",
      "Epoch 3/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 1.4858 - accuracy: 0.3051 - val_loss: 1.3252 - val_accuracy: 0.3670\n",
      "Epoch 4/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 1.3883 - accuracy: 0.3369 - val_loss: 1.3081 - val_accuracy: 0.3735\n",
      "Epoch 5/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.3392 - accuracy: 0.3610 - val_loss: 1.2765 - val_accuracy: 0.4066\n",
      "Epoch 6/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.2972 - accuracy: 0.3874 - val_loss: 1.2506 - val_accuracy: 0.4433\n",
      "Epoch 7/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.2641 - accuracy: 0.4048 - val_loss: 1.2044 - val_accuracy: 0.4710\n",
      "Epoch 8/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.2397 - accuracy: 0.4356 - val_loss: 1.1901 - val_accuracy: 0.4634\n",
      "Epoch 9/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.2219 - accuracy: 0.4400 - val_loss: 1.1719 - val_accuracy: 0.5065\n",
      "Epoch 10/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.1936 - accuracy: 0.4694 - val_loss: 1.1410 - val_accuracy: 0.5260\n",
      "Epoch 11/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.1713 - accuracy: 0.4874 - val_loss: 1.1162 - val_accuracy: 0.5313\n",
      "Epoch 12/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.1623 - accuracy: 0.4913 - val_loss: 1.0693 - val_accuracy: 0.5496\n",
      "Epoch 13/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.1482 - accuracy: 0.5010 - val_loss: 1.0740 - val_accuracy: 0.5556\n",
      "Epoch 14/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.1275 - accuracy: 0.5084 - val_loss: 1.0567 - val_accuracy: 0.5703\n",
      "Epoch 15/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.1247 - accuracy: 0.5130 - val_loss: 1.0498 - val_accuracy: 0.5561\n",
      "Epoch 16/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.1060 - accuracy: 0.5155 - val_loss: 1.0232 - val_accuracy: 0.5816\n",
      "Epoch 17/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.0881 - accuracy: 0.5327 - val_loss: 1.0191 - val_accuracy: 0.5703\n",
      "Epoch 18/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.0801 - accuracy: 0.5366 - val_loss: 0.9961 - val_accuracy: 0.6064\n",
      "Epoch 19/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.0665 - accuracy: 0.5442 - val_loss: 0.9847 - val_accuracy: 0.5963\n",
      "Epoch 20/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.0538 - accuracy: 0.5526 - val_loss: 0.9671 - val_accuracy: 0.6306\n",
      "Epoch 21/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.0504 - accuracy: 0.5569 - val_loss: 0.9677 - val_accuracy: 0.6212\n",
      "Epoch 22/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.0391 - accuracy: 0.5641 - val_loss: 0.9787 - val_accuracy: 0.6099\n",
      "Epoch 23/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.0320 - accuracy: 0.5669 - val_loss: 0.9632 - val_accuracy: 0.6277\n",
      "Epoch 24/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.0281 - accuracy: 0.5649 - val_loss: 0.9614 - val_accuracy: 0.6123\n",
      "Epoch 25/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.0090 - accuracy: 0.5749 - val_loss: 0.9280 - val_accuracy: 0.6253\n",
      "Epoch 26/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.0153 - accuracy: 0.5754 - val_loss: 0.9373 - val_accuracy: 0.6223\n",
      "Epoch 27/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.0029 - accuracy: 0.5786 - val_loss: 0.9278 - val_accuracy: 0.6430\n",
      "Epoch 28/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.9821 - accuracy: 0.5861 - val_loss: 0.9106 - val_accuracy: 0.6472\n",
      "Epoch 29/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.9802 - accuracy: 0.5910 - val_loss: 0.9283 - val_accuracy: 0.6265\n",
      "Epoch 30/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.9729 - accuracy: 0.5981 - val_loss: 0.9141 - val_accuracy: 0.6241\n",
      "Epoch 31/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.9650 - accuracy: 0.6020 - val_loss: 0.9142 - val_accuracy: 0.6223\n",
      "Epoch 32/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.9559 - accuracy: 0.6020 - val_loss: 0.8764 - val_accuracy: 0.6460\n",
      "Epoch 33/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.9528 - accuracy: 0.6071 - val_loss: 0.8951 - val_accuracy: 0.6401\n",
      "Epoch 34/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.9482 - accuracy: 0.6037 - val_loss: 0.8878 - val_accuracy: 0.6466\n",
      "Epoch 35/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.9464 - accuracy: 0.6068 - val_loss: 0.9268 - val_accuracy: 0.6164\n",
      "Epoch 36/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.9273 - accuracy: 0.6135 - val_loss: 0.8824 - val_accuracy: 0.6472\n",
      "Epoch 37/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.9303 - accuracy: 0.6127 - val_loss: 0.8987 - val_accuracy: 0.6306\n",
      "Epoch 38/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.9260 - accuracy: 0.6200 - val_loss: 0.8675 - val_accuracy: 0.6560\n",
      "Epoch 39/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.9041 - accuracy: 0.6284 - val_loss: 0.8649 - val_accuracy: 0.6501\n",
      "Epoch 40/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.9203 - accuracy: 0.6219 - val_loss: 0.8457 - val_accuracy: 0.6596\n",
      "Epoch 41/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.9110 - accuracy: 0.6263 - val_loss: 0.8629 - val_accuracy: 0.6554\n",
      "Epoch 42/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8911 - accuracy: 0.6452 - val_loss: 0.8460 - val_accuracy: 0.6548\n",
      "Epoch 43/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.9145 - accuracy: 0.6275 - val_loss: 0.8364 - val_accuracy: 0.6637\n",
      "Epoch 44/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8915 - accuracy: 0.6420 - val_loss: 0.8226 - val_accuracy: 0.6773\n",
      "Epoch 45/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8906 - accuracy: 0.6343 - val_loss: 0.8603 - val_accuracy: 0.6519\n",
      "Epoch 46/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8826 - accuracy: 0.6398 - val_loss: 0.8718 - val_accuracy: 0.6454\n",
      "Epoch 47/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8898 - accuracy: 0.6387 - val_loss: 0.8401 - val_accuracy: 0.6720\n",
      "Epoch 48/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8646 - accuracy: 0.6492 - val_loss: 0.8553 - val_accuracy: 0.6537\n",
      "Epoch 49/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8702 - accuracy: 0.6486 - val_loss: 0.8186 - val_accuracy: 0.6755\n",
      "Epoch 50/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.8603 - accuracy: 0.6513 - val_loss: 0.8328 - val_accuracy: 0.6749\n",
      "Epoch 51/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.8580 - accuracy: 0.6520 - val_loss: 0.8189 - val_accuracy: 0.6743\n",
      "Epoch 52/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.8634 - accuracy: 0.6498 - val_loss: 0.8080 - val_accuracy: 0.6856\n",
      "Epoch 53/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8583 - accuracy: 0.6473 - val_loss: 0.8314 - val_accuracy: 0.6625\n",
      "Epoch 54/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.8645 - accuracy: 0.6483 - val_loss: 0.8274 - val_accuracy: 0.6749\n",
      "Epoch 55/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8655 - accuracy: 0.6461 - val_loss: 0.8220 - val_accuracy: 0.6749\n",
      "Epoch 56/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8648 - accuracy: 0.6463 - val_loss: 0.8268 - val_accuracy: 0.6743\n",
      "Epoch 57/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8512 - accuracy: 0.6528 - val_loss: 0.8033 - val_accuracy: 0.6820\n",
      "Epoch 58/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8494 - accuracy: 0.6585 - val_loss: 0.8005 - val_accuracy: 0.6891\n",
      "Epoch 59/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8350 - accuracy: 0.6611 - val_loss: 0.8072 - val_accuracy: 0.6755\n",
      "Epoch 60/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8338 - accuracy: 0.6622 - val_loss: 0.8142 - val_accuracy: 0.6868\n",
      "Epoch 61/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8362 - accuracy: 0.6600 - val_loss: 0.7907 - val_accuracy: 0.6927\n",
      "Epoch 62/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8425 - accuracy: 0.6619 - val_loss: 0.8226 - val_accuracy: 0.6844\n",
      "Epoch 63/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8414 - accuracy: 0.6646 - val_loss: 0.7942 - val_accuracy: 0.6927\n",
      "Epoch 64/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8205 - accuracy: 0.6742 - val_loss: 0.8042 - val_accuracy: 0.6791\n",
      "Epoch 65/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.8356 - accuracy: 0.6667 - val_loss: 0.7933 - val_accuracy: 0.6974\n",
      "Epoch 66/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8210 - accuracy: 0.6670 - val_loss: 0.7949 - val_accuracy: 0.6844\n",
      "Epoch 67/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8141 - accuracy: 0.6721 - val_loss: 0.7833 - val_accuracy: 0.6868\n",
      "Epoch 68/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8136 - accuracy: 0.6650 - val_loss: 0.7858 - val_accuracy: 0.6767\n",
      "Epoch 69/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8020 - accuracy: 0.6741 - val_loss: 0.7827 - val_accuracy: 0.6897\n",
      "Epoch 70/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8224 - accuracy: 0.6665 - val_loss: 0.7837 - val_accuracy: 0.6885\n",
      "Epoch 71/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.8234 - accuracy: 0.6690 - val_loss: 0.7843 - val_accuracy: 0.6998\n",
      "Epoch 72/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.8143 - accuracy: 0.6686 - val_loss: 0.8077 - val_accuracy: 0.6749\n",
      "Epoch 73/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.8154 - accuracy: 0.6670 - val_loss: 0.7818 - val_accuracy: 0.6956\n",
      "Epoch 74/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.8045 - accuracy: 0.6760 - val_loss: 0.7918 - val_accuracy: 0.6797\n",
      "Epoch 75/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.8250 - accuracy: 0.6745 - val_loss: 0.8056 - val_accuracy: 0.6690\n",
      "Epoch 76/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8210 - accuracy: 0.6665 - val_loss: 0.7856 - val_accuracy: 0.6915\n",
      "Epoch 77/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7973 - accuracy: 0.6823 - val_loss: 0.7675 - val_accuracy: 0.6980\n",
      "Epoch 78/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8115 - accuracy: 0.6732 - val_loss: 0.7813 - val_accuracy: 0.6921\n",
      "Epoch 79/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.8102 - accuracy: 0.6704 - val_loss: 0.7801 - val_accuracy: 0.6950\n",
      "Epoch 80/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7978 - accuracy: 0.6751 - val_loss: 0.7685 - val_accuracy: 0.7027\n",
      "Epoch 81/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7960 - accuracy: 0.6834 - val_loss: 0.8011 - val_accuracy: 0.6708\n",
      "Epoch 82/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.7863 - accuracy: 0.6866 - val_loss: 0.7809 - val_accuracy: 0.7045\n",
      "Epoch 83/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.8109 - accuracy: 0.6712 - val_loss: 0.7683 - val_accuracy: 0.6927\n",
      "Epoch 84/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.8000 - accuracy: 0.6809 - val_loss: 0.7728 - val_accuracy: 0.6998\n",
      "Epoch 85/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8140 - accuracy: 0.6767 - val_loss: 0.7617 - val_accuracy: 0.7074\n",
      "Epoch 86/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7919 - accuracy: 0.6866 - val_loss: 0.7630 - val_accuracy: 0.6998\n",
      "Epoch 87/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7941 - accuracy: 0.6775 - val_loss: 0.7826 - val_accuracy: 0.6962\n",
      "Epoch 88/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7999 - accuracy: 0.6789 - val_loss: 0.7774 - val_accuracy: 0.6921\n",
      "Epoch 89/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7920 - accuracy: 0.6837 - val_loss: 0.7859 - val_accuracy: 0.6868\n",
      "Epoch 90/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7946 - accuracy: 0.6760 - val_loss: 0.7766 - val_accuracy: 0.7009\n",
      "Epoch 91/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.7906 - accuracy: 0.6903 - val_loss: 0.7689 - val_accuracy: 0.6909\n",
      "Epoch 92/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7758 - accuracy: 0.6933 - val_loss: 0.7862 - val_accuracy: 0.6915\n",
      "Epoch 93/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.7881 - accuracy: 0.6847 - val_loss: 0.7511 - val_accuracy: 0.7092\n",
      "Epoch 94/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7839 - accuracy: 0.6859 - val_loss: 0.7665 - val_accuracy: 0.7009\n",
      "Epoch 95/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7973 - accuracy: 0.6838 - val_loss: 0.7555 - val_accuracy: 0.7051\n",
      "Epoch 96/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7805 - accuracy: 0.6912 - val_loss: 0.7954 - val_accuracy: 0.6826\n",
      "Epoch 97/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.7876 - accuracy: 0.6854 - val_loss: 0.7958 - val_accuracy: 0.6755\n",
      "Epoch 98/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7729 - accuracy: 0.6928 - val_loss: 0.7545 - val_accuracy: 0.7021\n",
      "Epoch 99/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.7766 - accuracy: 0.6881 - val_loss: 0.7565 - val_accuracy: 0.7009\n",
      "Epoch 100/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.7785 - accuracy: 0.6941 - val_loss: 0.7599 - val_accuracy: 0.6968\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.8021 - accuracy: 0.6721\n",
      "Shape of X after trimming: (1692, 22, 400)\n",
      "Shape of X after trimming: (423, 22, 400)\n",
      "Shape of X after trimming: (443, 22, 400)\n",
      "Shape of training set: (6768, 22, 200)\n",
      "Shape of validation set: (1692, 22, 200)\n",
      "Shape of training labels: (6768,)\n",
      "Shape of validation labels: (1692,)\n",
      "Shape of testing set: (1772, 22, 200)\n",
      "Shape of testing labels: (1772,)\n",
      "Shape of training labels after categorical conversion: (6768, 4)\n",
      "Shape of validation labels after categorical conversion: (1692, 4)\n",
      "Shape of test labels after categorical conversion: (1772, 4)\n",
      "Shape of training set after adding width info: (6768, 22, 200, 1)\n",
      "Shape of validation set after adding width info: (1692, 22, 200, 1)\n",
      "Shape of test set after adding width info: (1772, 22, 200, 1)\n",
      "Shape of training set after dimension reshaping: (6768, 200, 1, 22)\n",
      "Shape of validation set after dimension reshaping: (1692, 200, 1, 22)\n",
      "Shape of test set after dimension reshaping: (1772, 200, 1, 22)\n",
      "Epoch 1/100\n",
      "106/106 [==============================] - 2s 12ms/step - loss: 1.9566 - accuracy: 0.2834 - val_loss: 1.3897 - val_accuracy: 0.3481\n",
      "Epoch 2/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 1.5718 - accuracy: 0.3146 - val_loss: 1.2945 - val_accuracy: 0.3913\n",
      "Epoch 3/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 1.4128 - accuracy: 0.3503 - val_loss: 1.2696 - val_accuracy: 0.4066\n",
      "Epoch 4/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 1.3222 - accuracy: 0.3842 - val_loss: 1.2215 - val_accuracy: 0.4917\n",
      "Epoch 5/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 1.2634 - accuracy: 0.4116 - val_loss: 1.1949 - val_accuracy: 0.4728\n",
      "Epoch 6/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 1.2183 - accuracy: 0.4421 - val_loss: 1.1823 - val_accuracy: 0.5018\n",
      "Epoch 7/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 1.1945 - accuracy: 0.4523 - val_loss: 1.1804 - val_accuracy: 0.4781\n",
      "Epoch 8/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 1.1849 - accuracy: 0.4666 - val_loss: 1.1664 - val_accuracy: 0.4965\n",
      "Epoch 9/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 1.1776 - accuracy: 0.4713 - val_loss: 1.1642 - val_accuracy: 0.4900\n",
      "Epoch 10/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 1.1570 - accuracy: 0.4805 - val_loss: 1.1627 - val_accuracy: 0.4840\n",
      "Epoch 11/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 1.1326 - accuracy: 0.4991 - val_loss: 1.1148 - val_accuracy: 0.5414\n",
      "Epoch 12/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 1.1112 - accuracy: 0.5146 - val_loss: 1.0991 - val_accuracy: 0.5437\n",
      "Epoch 13/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 1.0908 - accuracy: 0.5241 - val_loss: 1.0845 - val_accuracy: 0.5496\n",
      "Epoch 14/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 1.0838 - accuracy: 0.5375 - val_loss: 1.0630 - val_accuracy: 0.5739\n",
      "Epoch 15/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 1.0590 - accuracy: 0.5495 - val_loss: 1.0759 - val_accuracy: 0.5449\n",
      "Epoch 16/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 1.0405 - accuracy: 0.5612 - val_loss: 1.0480 - val_accuracy: 0.5585\n",
      "Epoch 17/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 1.0262 - accuracy: 0.5674 - val_loss: 1.0165 - val_accuracy: 0.5857\n",
      "Epoch 18/100\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 1.0209 - accuracy: 0.5697 - val_loss: 0.9908 - val_accuracy: 0.6087\n",
      "Epoch 19/100\n",
      "106/106 [==============================] - 2s 14ms/step - loss: 1.0093 - accuracy: 0.5811 - val_loss: 1.0174 - val_accuracy: 0.5898\n",
      "Epoch 20/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 1.0020 - accuracy: 0.5848 - val_loss: 0.9790 - val_accuracy: 0.5993\n",
      "Epoch 21/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.9813 - accuracy: 0.5903 - val_loss: 0.9455 - val_accuracy: 0.6377\n",
      "Epoch 22/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.9472 - accuracy: 0.6070 - val_loss: 0.9365 - val_accuracy: 0.6200\n",
      "Epoch 23/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.9684 - accuracy: 0.5977 - val_loss: 0.9434 - val_accuracy: 0.6200\n",
      "Epoch 24/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.9499 - accuracy: 0.6113 - val_loss: 0.9389 - val_accuracy: 0.6306\n",
      "Epoch 25/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.9265 - accuracy: 0.6160 - val_loss: 0.9155 - val_accuracy: 0.6478\n",
      "Epoch 26/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.9120 - accuracy: 0.6209 - val_loss: 0.9070 - val_accuracy: 0.6507\n",
      "Epoch 27/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.9222 - accuracy: 0.6268 - val_loss: 0.9167 - val_accuracy: 0.6483\n",
      "Epoch 28/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.9020 - accuracy: 0.6297 - val_loss: 0.8993 - val_accuracy: 0.6472\n",
      "Epoch 29/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.8857 - accuracy: 0.6370 - val_loss: 0.8966 - val_accuracy: 0.6365\n",
      "Epoch 30/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.8901 - accuracy: 0.6376 - val_loss: 0.8840 - val_accuracy: 0.6560\n",
      "Epoch 31/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.8909 - accuracy: 0.6398 - val_loss: 0.8884 - val_accuracy: 0.6489\n",
      "Epoch 32/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.8848 - accuracy: 0.6365 - val_loss: 0.9042 - val_accuracy: 0.6277\n",
      "Epoch 33/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.8704 - accuracy: 0.6470 - val_loss: 0.8824 - val_accuracy: 0.6501\n",
      "Epoch 34/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.8682 - accuracy: 0.6523 - val_loss: 0.8828 - val_accuracy: 0.6596\n",
      "Epoch 35/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.8529 - accuracy: 0.6590 - val_loss: 0.8715 - val_accuracy: 0.6584\n",
      "Epoch 36/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.8568 - accuracy: 0.6526 - val_loss: 0.8552 - val_accuracy: 0.6667\n",
      "Epoch 37/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.8563 - accuracy: 0.6534 - val_loss: 0.8801 - val_accuracy: 0.6637\n",
      "Epoch 38/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.8358 - accuracy: 0.6609 - val_loss: 0.8676 - val_accuracy: 0.6478\n",
      "Epoch 39/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.8334 - accuracy: 0.6613 - val_loss: 0.8681 - val_accuracy: 0.6572\n",
      "Epoch 40/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.8395 - accuracy: 0.6615 - val_loss: 0.8852 - val_accuracy: 0.6489\n",
      "Epoch 41/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.8339 - accuracy: 0.6646 - val_loss: 0.8383 - val_accuracy: 0.6779\n",
      "Epoch 42/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.8252 - accuracy: 0.6662 - val_loss: 0.8558 - val_accuracy: 0.6678\n",
      "Epoch 43/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.8196 - accuracy: 0.6646 - val_loss: 0.8967 - val_accuracy: 0.6513\n",
      "Epoch 44/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.8187 - accuracy: 0.6681 - val_loss: 0.8746 - val_accuracy: 0.6554\n",
      "Epoch 45/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.8031 - accuracy: 0.6764 - val_loss: 0.8496 - val_accuracy: 0.6608\n",
      "Epoch 46/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.8122 - accuracy: 0.6677 - val_loss: 0.8459 - val_accuracy: 0.6643\n",
      "Epoch 47/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.8038 - accuracy: 0.6795 - val_loss: 0.8949 - val_accuracy: 0.6300\n",
      "Epoch 48/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.8028 - accuracy: 0.6749 - val_loss: 0.8520 - val_accuracy: 0.6690\n",
      "Epoch 49/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7900 - accuracy: 0.6840 - val_loss: 0.8578 - val_accuracy: 0.6678\n",
      "Epoch 50/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.8027 - accuracy: 0.6723 - val_loss: 0.8346 - val_accuracy: 0.6613\n",
      "Epoch 51/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.7932 - accuracy: 0.6789 - val_loss: 0.8498 - val_accuracy: 0.6637\n",
      "Epoch 52/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7853 - accuracy: 0.6896 - val_loss: 0.8333 - val_accuracy: 0.6613\n",
      "Epoch 53/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7892 - accuracy: 0.6834 - val_loss: 0.8509 - val_accuracy: 0.6678\n",
      "Epoch 54/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7957 - accuracy: 0.6801 - val_loss: 0.8197 - val_accuracy: 0.6773\n",
      "Epoch 55/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7695 - accuracy: 0.6973 - val_loss: 0.8507 - val_accuracy: 0.6531\n",
      "Epoch 56/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7789 - accuracy: 0.6860 - val_loss: 0.8184 - val_accuracy: 0.6850\n",
      "Epoch 57/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7704 - accuracy: 0.6899 - val_loss: 0.8345 - val_accuracy: 0.6684\n",
      "Epoch 58/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7740 - accuracy: 0.6894 - val_loss: 0.8320 - val_accuracy: 0.6708\n",
      "Epoch 59/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7743 - accuracy: 0.6872 - val_loss: 0.8208 - val_accuracy: 0.6885\n",
      "Epoch 60/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7720 - accuracy: 0.6905 - val_loss: 0.8292 - val_accuracy: 0.6755\n",
      "Epoch 61/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.7619 - accuracy: 0.6990 - val_loss: 0.8484 - val_accuracy: 0.6755\n",
      "Epoch 62/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7615 - accuracy: 0.6967 - val_loss: 0.8231 - val_accuracy: 0.6814\n",
      "Epoch 63/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7685 - accuracy: 0.6893 - val_loss: 0.8355 - val_accuracy: 0.6696\n",
      "Epoch 64/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7460 - accuracy: 0.7052 - val_loss: 0.8075 - val_accuracy: 0.6897\n",
      "Epoch 65/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.7644 - accuracy: 0.6967 - val_loss: 0.8409 - val_accuracy: 0.6543\n",
      "Epoch 66/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7491 - accuracy: 0.7009 - val_loss: 0.8176 - val_accuracy: 0.6844\n",
      "Epoch 67/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7515 - accuracy: 0.7011 - val_loss: 0.8280 - val_accuracy: 0.6749\n",
      "Epoch 68/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7549 - accuracy: 0.6983 - val_loss: 0.8079 - val_accuracy: 0.6968\n",
      "Epoch 69/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7446 - accuracy: 0.6995 - val_loss: 0.8245 - val_accuracy: 0.6767\n",
      "Epoch 70/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.7578 - accuracy: 0.6953 - val_loss: 0.8041 - val_accuracy: 0.6956\n",
      "Epoch 71/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7531 - accuracy: 0.6937 - val_loss: 0.8078 - val_accuracy: 0.6844\n",
      "Epoch 72/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.7324 - accuracy: 0.7074 - val_loss: 0.8336 - val_accuracy: 0.6608\n",
      "Epoch 73/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7389 - accuracy: 0.7017 - val_loss: 0.8086 - val_accuracy: 0.6933\n",
      "Epoch 74/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7472 - accuracy: 0.7052 - val_loss: 0.8418 - val_accuracy: 0.6690\n",
      "Epoch 75/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7390 - accuracy: 0.6977 - val_loss: 0.8439 - val_accuracy: 0.6507\n",
      "Epoch 76/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7364 - accuracy: 0.7083 - val_loss: 0.8241 - val_accuracy: 0.6897\n",
      "Epoch 77/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7417 - accuracy: 0.7035 - val_loss: 0.8203 - val_accuracy: 0.6826\n",
      "Epoch 78/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7478 - accuracy: 0.7024 - val_loss: 0.7979 - val_accuracy: 0.7015\n",
      "Epoch 79/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7313 - accuracy: 0.7069 - val_loss: 0.8027 - val_accuracy: 0.7004\n",
      "Epoch 80/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7324 - accuracy: 0.7101 - val_loss: 0.8289 - val_accuracy: 0.6738\n",
      "Epoch 81/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7318 - accuracy: 0.7116 - val_loss: 0.8156 - val_accuracy: 0.6803\n",
      "Epoch 82/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7216 - accuracy: 0.7134 - val_loss: 0.8113 - val_accuracy: 0.6891\n",
      "Epoch 83/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7535 - accuracy: 0.7014 - val_loss: 0.8123 - val_accuracy: 0.6767\n",
      "Epoch 84/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.7355 - accuracy: 0.7085 - val_loss: 0.8097 - val_accuracy: 0.6986\n",
      "Epoch 85/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7232 - accuracy: 0.7141 - val_loss: 0.8133 - val_accuracy: 0.6814\n",
      "Epoch 86/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7419 - accuracy: 0.7058 - val_loss: 0.8181 - val_accuracy: 0.6809\n",
      "Epoch 87/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7325 - accuracy: 0.7070 - val_loss: 0.7953 - val_accuracy: 0.6992\n",
      "Epoch 88/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7245 - accuracy: 0.7144 - val_loss: 0.8089 - val_accuracy: 0.6939\n",
      "Epoch 89/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7309 - accuracy: 0.7057 - val_loss: 0.7988 - val_accuracy: 0.7057\n",
      "Epoch 90/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7372 - accuracy: 0.7076 - val_loss: 0.8157 - val_accuracy: 0.6868\n",
      "Epoch 91/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.7393 - accuracy: 0.6992 - val_loss: 0.8346 - val_accuracy: 0.6803\n",
      "Epoch 92/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7330 - accuracy: 0.7061 - val_loss: 0.7989 - val_accuracy: 0.7057\n",
      "Epoch 93/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7105 - accuracy: 0.7204 - val_loss: 0.8107 - val_accuracy: 0.6944\n",
      "Epoch 94/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7124 - accuracy: 0.7219 - val_loss: 0.7910 - val_accuracy: 0.7015\n",
      "Epoch 95/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7243 - accuracy: 0.7178 - val_loss: 0.8001 - val_accuracy: 0.6950\n",
      "Epoch 96/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7077 - accuracy: 0.7207 - val_loss: 0.8021 - val_accuracy: 0.7045\n",
      "Epoch 97/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7204 - accuracy: 0.7215 - val_loss: 0.7955 - val_accuracy: 0.7063\n",
      "Epoch 98/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7142 - accuracy: 0.7199 - val_loss: 0.8111 - val_accuracy: 0.6885\n",
      "Epoch 99/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7066 - accuracy: 0.7154 - val_loss: 0.8133 - val_accuracy: 0.6891\n",
      "Epoch 100/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7228 - accuracy: 0.7132 - val_loss: 0.7798 - val_accuracy: 0.7252\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.7654 - accuracy: 0.6823\n",
      "Shape of X after trimming: (1692, 22, 500)\n",
      "Shape of X after trimming: (423, 22, 500)\n",
      "Shape of X after trimming: (443, 22, 500)\n",
      "Shape of training set: (6768, 22, 250)\n",
      "Shape of validation set: (1692, 22, 250)\n",
      "Shape of training labels: (6768,)\n",
      "Shape of validation labels: (1692,)\n",
      "Shape of testing set: (1772, 22, 250)\n",
      "Shape of testing labels: (1772,)\n",
      "Shape of training labels after categorical conversion: (6768, 4)\n",
      "Shape of validation labels after categorical conversion: (1692, 4)\n",
      "Shape of test labels after categorical conversion: (1772, 4)\n",
      "Shape of training set after adding width info: (6768, 22, 250, 1)\n",
      "Shape of validation set after adding width info: (1692, 22, 250, 1)\n",
      "Shape of test set after adding width info: (1772, 22, 250, 1)\n",
      "Shape of training set after dimension reshaping: (6768, 250, 1, 22)\n",
      "Shape of validation set after dimension reshaping: (1692, 250, 1, 22)\n",
      "Shape of test set after dimension reshaping: (1772, 250, 1, 22)\n",
      "Epoch 1/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 2.0150 - accuracy: 0.2668 - val_loss: 1.4260 - val_accuracy: 0.3617\n",
      "Epoch 2/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 1.5742 - accuracy: 0.3267 - val_loss: 1.3119 - val_accuracy: 0.3623\n",
      "Epoch 3/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 1.4027 - accuracy: 0.3624 - val_loss: 1.2532 - val_accuracy: 0.4155\n",
      "Epoch 4/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 1.2863 - accuracy: 0.4133 - val_loss: 1.1956 - val_accuracy: 0.4450\n",
      "Epoch 5/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 1.2291 - accuracy: 0.4549 - val_loss: 1.1436 - val_accuracy: 0.5030\n",
      "Epoch 6/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 1.1891 - accuracy: 0.4778 - val_loss: 1.1082 - val_accuracy: 0.5343\n",
      "Epoch 7/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 1.1540 - accuracy: 0.5001 - val_loss: 1.0899 - val_accuracy: 0.5508\n",
      "Epoch 8/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 1.1346 - accuracy: 0.5145 - val_loss: 1.0681 - val_accuracy: 0.5798\n",
      "Epoch 9/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 1.1085 - accuracy: 0.5290 - val_loss: 1.0602 - val_accuracy: 0.5715\n",
      "Epoch 10/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 1.0865 - accuracy: 0.5363 - val_loss: 1.0536 - val_accuracy: 0.5703\n",
      "Epoch 11/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 1.0645 - accuracy: 0.5539 - val_loss: 1.0379 - val_accuracy: 0.5798\n",
      "Epoch 12/100\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 1.0593 - accuracy: 0.5516 - val_loss: 1.0048 - val_accuracy: 0.6164\n",
      "Epoch 13/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 1.0440 - accuracy: 0.5693 - val_loss: 1.0166 - val_accuracy: 0.6070\n",
      "Epoch 14/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 1.0214 - accuracy: 0.5728 - val_loss: 0.9981 - val_accuracy: 0.5999\n",
      "Epoch 15/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 1.0175 - accuracy: 0.5757 - val_loss: 1.0249 - val_accuracy: 0.6028\n",
      "Epoch 16/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 1.0002 - accuracy: 0.5891 - val_loss: 0.9931 - val_accuracy: 0.6164\n",
      "Epoch 17/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.9912 - accuracy: 0.5954 - val_loss: 0.9759 - val_accuracy: 0.6182\n",
      "Epoch 18/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.9737 - accuracy: 0.6034 - val_loss: 0.9495 - val_accuracy: 0.6259\n",
      "Epoch 19/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.9515 - accuracy: 0.6145 - val_loss: 0.9321 - val_accuracy: 0.6371\n",
      "Epoch 20/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.9513 - accuracy: 0.6065 - val_loss: 0.9128 - val_accuracy: 0.6602\n",
      "Epoch 21/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.9391 - accuracy: 0.6135 - val_loss: 0.9204 - val_accuracy: 0.6584\n",
      "Epoch 22/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.9285 - accuracy: 0.6198 - val_loss: 0.9665 - val_accuracy: 0.6288\n",
      "Epoch 23/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.9197 - accuracy: 0.6294 - val_loss: 0.9051 - val_accuracy: 0.6472\n",
      "Epoch 24/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.9154 - accuracy: 0.6254 - val_loss: 0.8959 - val_accuracy: 0.6643\n",
      "Epoch 25/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.8999 - accuracy: 0.6334 - val_loss: 0.8662 - val_accuracy: 0.6749\n",
      "Epoch 26/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.8923 - accuracy: 0.6340 - val_loss: 0.8710 - val_accuracy: 0.6862\n",
      "Epoch 27/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.8828 - accuracy: 0.6426 - val_loss: 0.8712 - val_accuracy: 0.6755\n",
      "Epoch 28/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8655 - accuracy: 0.6467 - val_loss: 0.8494 - val_accuracy: 0.6903\n",
      "Epoch 29/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.8788 - accuracy: 0.6423 - val_loss: 0.8228 - val_accuracy: 0.6897\n",
      "Epoch 30/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.8480 - accuracy: 0.6616 - val_loss: 0.8492 - val_accuracy: 0.6755\n",
      "Epoch 31/100\n",
      "106/106 [==============================] - 2s 14ms/step - loss: 0.8525 - accuracy: 0.6519 - val_loss: 0.8386 - val_accuracy: 0.6903\n",
      "Epoch 32/100\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 0.8436 - accuracy: 0.6621 - val_loss: 0.8321 - val_accuracy: 0.6879\n",
      "Epoch 33/100\n",
      "106/106 [==============================] - 2s 14ms/step - loss: 0.8519 - accuracy: 0.6612 - val_loss: 0.8159 - val_accuracy: 0.6885\n",
      "Epoch 34/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.8400 - accuracy: 0.6606 - val_loss: 0.8092 - val_accuracy: 0.7027\n",
      "Epoch 35/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.8346 - accuracy: 0.6690 - val_loss: 0.8175 - val_accuracy: 0.6944\n",
      "Epoch 36/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.8218 - accuracy: 0.6656 - val_loss: 0.8062 - val_accuracy: 0.6933\n",
      "Epoch 37/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.8141 - accuracy: 0.6637 - val_loss: 0.7929 - val_accuracy: 0.7021\n",
      "Epoch 38/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.8237 - accuracy: 0.6699 - val_loss: 0.7904 - val_accuracy: 0.6785\n",
      "Epoch 39/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.8086 - accuracy: 0.6748 - val_loss: 0.7833 - val_accuracy: 0.7092\n",
      "Epoch 40/100\n",
      "106/106 [==============================] - 2s 14ms/step - loss: 0.8036 - accuracy: 0.6772 - val_loss: 0.7731 - val_accuracy: 0.7169\n",
      "Epoch 41/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7962 - accuracy: 0.6791 - val_loss: 0.7770 - val_accuracy: 0.7128\n",
      "Epoch 42/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7999 - accuracy: 0.6856 - val_loss: 0.7740 - val_accuracy: 0.7092\n",
      "Epoch 43/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7895 - accuracy: 0.6881 - val_loss: 0.7984 - val_accuracy: 0.6909\n",
      "Epoch 44/100\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 0.7882 - accuracy: 0.6819 - val_loss: 0.7757 - val_accuracy: 0.7116\n",
      "Epoch 45/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7993 - accuracy: 0.6785 - val_loss: 0.7784 - val_accuracy: 0.6986\n",
      "Epoch 46/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7785 - accuracy: 0.6853 - val_loss: 0.7631 - val_accuracy: 0.7210\n",
      "Epoch 47/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7891 - accuracy: 0.6816 - val_loss: 0.7499 - val_accuracy: 0.7157\n",
      "Epoch 48/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7804 - accuracy: 0.6865 - val_loss: 0.8044 - val_accuracy: 0.6944\n",
      "Epoch 49/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7743 - accuracy: 0.6872 - val_loss: 0.7490 - val_accuracy: 0.7193\n",
      "Epoch 50/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7785 - accuracy: 0.6859 - val_loss: 0.7698 - val_accuracy: 0.7045\n",
      "Epoch 51/100\n",
      "106/106 [==============================] - 2s 14ms/step - loss: 0.7514 - accuracy: 0.7007 - val_loss: 0.7432 - val_accuracy: 0.7287\n",
      "Epoch 52/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7699 - accuracy: 0.6953 - val_loss: 0.7682 - val_accuracy: 0.7116\n",
      "Epoch 53/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7770 - accuracy: 0.6900 - val_loss: 0.7452 - val_accuracy: 0.7210\n",
      "Epoch 54/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7657 - accuracy: 0.6928 - val_loss: 0.7605 - val_accuracy: 0.7069\n",
      "Epoch 55/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7696 - accuracy: 0.6913 - val_loss: 0.7752 - val_accuracy: 0.7039\n",
      "Epoch 56/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7593 - accuracy: 0.6992 - val_loss: 0.7619 - val_accuracy: 0.7074\n",
      "Epoch 57/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7695 - accuracy: 0.6937 - val_loss: 0.7452 - val_accuracy: 0.7151\n",
      "Epoch 58/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7432 - accuracy: 0.7049 - val_loss: 0.7477 - val_accuracy: 0.7293\n",
      "Epoch 59/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7657 - accuracy: 0.6968 - val_loss: 0.7518 - val_accuracy: 0.7015\n",
      "Epoch 60/100\n",
      "106/106 [==============================] - 2s 14ms/step - loss: 0.7529 - accuracy: 0.6989 - val_loss: 0.7515 - val_accuracy: 0.7021\n",
      "Epoch 61/100\n",
      "106/106 [==============================] - 2s 14ms/step - loss: 0.7646 - accuracy: 0.6930 - val_loss: 0.7472 - val_accuracy: 0.7104\n",
      "Epoch 62/100\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 0.7623 - accuracy: 0.6921 - val_loss: 0.7456 - val_accuracy: 0.7122\n",
      "Epoch 63/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7353 - accuracy: 0.7163 - val_loss: 0.7444 - val_accuracy: 0.7299\n",
      "Epoch 64/100\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 0.7393 - accuracy: 0.7035 - val_loss: 0.7790 - val_accuracy: 0.7092\n",
      "Epoch 65/100\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 0.7453 - accuracy: 0.6981 - val_loss: 0.7453 - val_accuracy: 0.7092\n",
      "Epoch 66/100\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 0.7393 - accuracy: 0.7082 - val_loss: 0.7415 - val_accuracy: 0.7204\n",
      "Epoch 67/100\n",
      "106/106 [==============================] - 2s 14ms/step - loss: 0.7396 - accuracy: 0.7070 - val_loss: 0.7158 - val_accuracy: 0.7293\n",
      "Epoch 68/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7283 - accuracy: 0.7114 - val_loss: 0.7460 - val_accuracy: 0.7240\n",
      "Epoch 69/100\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 0.7508 - accuracy: 0.6989 - val_loss: 0.7321 - val_accuracy: 0.7157\n",
      "Epoch 70/100\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 0.7200 - accuracy: 0.7178 - val_loss: 0.7426 - val_accuracy: 0.7086\n",
      "Epoch 71/100\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 0.7202 - accuracy: 0.7134 - val_loss: 0.7388 - val_accuracy: 0.7139\n",
      "Epoch 72/100\n",
      "106/106 [==============================] - 2s 14ms/step - loss: 0.7394 - accuracy: 0.7039 - val_loss: 0.7418 - val_accuracy: 0.7134\n",
      "Epoch 73/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7293 - accuracy: 0.7145 - val_loss: 0.7353 - val_accuracy: 0.7175\n",
      "Epoch 74/100\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 0.7295 - accuracy: 0.7142 - val_loss: 0.7365 - val_accuracy: 0.7358\n",
      "Epoch 75/100\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 0.7270 - accuracy: 0.7101 - val_loss: 0.7563 - val_accuracy: 0.7110\n",
      "Epoch 76/100\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 0.7371 - accuracy: 0.7148 - val_loss: 0.7358 - val_accuracy: 0.7110\n",
      "Epoch 77/100\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 0.7178 - accuracy: 0.7160 - val_loss: 0.7304 - val_accuracy: 0.7240\n",
      "Epoch 78/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7257 - accuracy: 0.7148 - val_loss: 0.7505 - val_accuracy: 0.7074\n",
      "Epoch 79/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.7180 - accuracy: 0.7123 - val_loss: 0.7380 - val_accuracy: 0.7181\n",
      "Epoch 80/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7256 - accuracy: 0.7092 - val_loss: 0.7312 - val_accuracy: 0.7193\n",
      "Epoch 81/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7162 - accuracy: 0.7184 - val_loss: 0.7198 - val_accuracy: 0.7234\n",
      "Epoch 82/100\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 0.7147 - accuracy: 0.7179 - val_loss: 0.7186 - val_accuracy: 0.7193\n",
      "Epoch 83/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7268 - accuracy: 0.7119 - val_loss: 0.7289 - val_accuracy: 0.7169\n",
      "Epoch 84/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7137 - accuracy: 0.7162 - val_loss: 0.7444 - val_accuracy: 0.7175\n",
      "Epoch 85/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7133 - accuracy: 0.7134 - val_loss: 0.7238 - val_accuracy: 0.7293\n",
      "Epoch 86/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7145 - accuracy: 0.7137 - val_loss: 0.7193 - val_accuracy: 0.7364\n",
      "Epoch 87/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7294 - accuracy: 0.7113 - val_loss: 0.7105 - val_accuracy: 0.7258\n",
      "Epoch 88/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7210 - accuracy: 0.7168 - val_loss: 0.7132 - val_accuracy: 0.7264\n",
      "Epoch 89/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7059 - accuracy: 0.7179 - val_loss: 0.7208 - val_accuracy: 0.7323\n",
      "Epoch 90/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7230 - accuracy: 0.7144 - val_loss: 0.6998 - val_accuracy: 0.7388\n",
      "Epoch 91/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7167 - accuracy: 0.7153 - val_loss: 0.7156 - val_accuracy: 0.7258\n",
      "Epoch 92/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7188 - accuracy: 0.7135 - val_loss: 0.7248 - val_accuracy: 0.7110\n",
      "Epoch 93/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7028 - accuracy: 0.7224 - val_loss: 0.7083 - val_accuracy: 0.7358\n",
      "Epoch 94/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7278 - accuracy: 0.7171 - val_loss: 0.7096 - val_accuracy: 0.7346\n",
      "Epoch 95/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7036 - accuracy: 0.7219 - val_loss: 0.7097 - val_accuracy: 0.7264\n",
      "Epoch 96/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7106 - accuracy: 0.7225 - val_loss: 0.6973 - val_accuracy: 0.7441\n",
      "Epoch 97/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7015 - accuracy: 0.7240 - val_loss: 0.7286 - val_accuracy: 0.7305\n",
      "Epoch 98/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7029 - accuracy: 0.7219 - val_loss: 0.7343 - val_accuracy: 0.7199\n",
      "Epoch 99/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.6921 - accuracy: 0.7284 - val_loss: 0.7036 - val_accuracy: 0.7264\n",
      "Epoch 100/100\n",
      "106/106 [==============================] - 1s 14ms/step - loss: 0.7055 - accuracy: 0.7163 - val_loss: 0.7109 - val_accuracy: 0.7335\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.7512 - accuracy: 0.7026\n",
      "Shape of X after trimming: (1692, 22, 600)\n",
      "Shape of X after trimming: (423, 22, 600)\n",
      "Shape of X after trimming: (443, 22, 600)\n",
      "Shape of training set: (6768, 22, 300)\n",
      "Shape of validation set: (1692, 22, 300)\n",
      "Shape of training labels: (6768,)\n",
      "Shape of validation labels: (1692,)\n",
      "Shape of testing set: (1772, 22, 300)\n",
      "Shape of testing labels: (1772,)\n",
      "Shape of training labels after categorical conversion: (6768, 4)\n",
      "Shape of validation labels after categorical conversion: (1692, 4)\n",
      "Shape of test labels after categorical conversion: (1772, 4)\n",
      "Shape of training set after adding width info: (6768, 22, 300, 1)\n",
      "Shape of validation set after adding width info: (1692, 22, 300, 1)\n",
      "Shape of test set after adding width info: (1772, 22, 300, 1)\n",
      "Shape of training set after dimension reshaping: (6768, 300, 1, 22)\n",
      "Shape of validation set after dimension reshaping: (1692, 300, 1, 22)\n",
      "Shape of test set after dimension reshaping: (1772, 300, 1, 22)\n",
      "Epoch 1/100\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 1.9584 - accuracy: 0.2764 - val_loss: 1.4596 - val_accuracy: 0.3505\n",
      "Epoch 2/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 1.5703 - accuracy: 0.3110 - val_loss: 1.3024 - val_accuracy: 0.3806\n",
      "Epoch 3/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 1.4262 - accuracy: 0.3364 - val_loss: 1.2777 - val_accuracy: 0.4143\n",
      "Epoch 4/100\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 1.3484 - accuracy: 0.3593 - val_loss: 1.2619 - val_accuracy: 0.4184\n",
      "Epoch 5/100\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 1.2828 - accuracy: 0.4032 - val_loss: 1.2302 - val_accuracy: 0.4663\n",
      "Epoch 6/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 1.2357 - accuracy: 0.4282 - val_loss: 1.1972 - val_accuracy: 0.4699\n",
      "Epoch 7/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 1.1987 - accuracy: 0.4614 - val_loss: 1.1614 - val_accuracy: 0.5100\n",
      "Epoch 8/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 1.1708 - accuracy: 0.4914 - val_loss: 1.1521 - val_accuracy: 0.5195\n",
      "Epoch 9/100\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 1.1490 - accuracy: 0.4902 - val_loss: 1.1188 - val_accuracy: 0.5319\n",
      "Epoch 10/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 1.1320 - accuracy: 0.5118 - val_loss: 1.1064 - val_accuracy: 0.5112\n",
      "Epoch 11/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 1.1004 - accuracy: 0.5276 - val_loss: 1.0680 - val_accuracy: 0.5437\n",
      "Epoch 12/100\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 1.0722 - accuracy: 0.5516 - val_loss: 1.0180 - val_accuracy: 0.6011\n",
      "Epoch 13/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 1.0526 - accuracy: 0.5588 - val_loss: 0.9864 - val_accuracy: 0.6164\n",
      "Epoch 14/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 1.0271 - accuracy: 0.5671 - val_loss: 0.9942 - val_accuracy: 0.5887\n",
      "Epoch 15/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.9984 - accuracy: 0.5884 - val_loss: 0.9638 - val_accuracy: 0.6052\n",
      "Epoch 16/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.9933 - accuracy: 0.5949 - val_loss: 0.9221 - val_accuracy: 0.6389\n",
      "Epoch 17/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.9787 - accuracy: 0.5915 - val_loss: 0.9058 - val_accuracy: 0.6176\n",
      "Epoch 18/100\n",
      "106/106 [==============================] - 2s 21ms/step - loss: 0.9509 - accuracy: 0.6028 - val_loss: 0.8890 - val_accuracy: 0.6348\n",
      "Epoch 19/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.9309 - accuracy: 0.6192 - val_loss: 0.8530 - val_accuracy: 0.6797\n",
      "Epoch 20/100\n",
      "106/106 [==============================] - 2s 21ms/step - loss: 0.9294 - accuracy: 0.6144 - val_loss: 0.8743 - val_accuracy: 0.6466\n",
      "Epoch 21/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.9192 - accuracy: 0.6262 - val_loss: 0.8678 - val_accuracy: 0.6543\n",
      "Epoch 22/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.9123 - accuracy: 0.6305 - val_loss: 0.8357 - val_accuracy: 0.6590\n",
      "Epoch 23/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.8940 - accuracy: 0.6399 - val_loss: 0.8436 - val_accuracy: 0.6643\n",
      "Epoch 24/100\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.8793 - accuracy: 0.6407 - val_loss: 0.8340 - val_accuracy: 0.6608\n",
      "Epoch 25/100\n",
      "106/106 [==============================] - 2s 21ms/step - loss: 0.8723 - accuracy: 0.6414 - val_loss: 0.8116 - val_accuracy: 0.6809\n",
      "Epoch 26/100\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.8691 - accuracy: 0.6467 - val_loss: 0.8561 - val_accuracy: 0.6507\n",
      "Epoch 27/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.8519 - accuracy: 0.6571 - val_loss: 0.8094 - val_accuracy: 0.6874\n",
      "Epoch 28/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.8586 - accuracy: 0.6492 - val_loss: 0.7875 - val_accuracy: 0.6950\n",
      "Epoch 29/100\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.8430 - accuracy: 0.6571 - val_loss: 0.7904 - val_accuracy: 0.6968\n",
      "Epoch 30/100\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.8213 - accuracy: 0.6726 - val_loss: 0.7920 - val_accuracy: 0.6856\n",
      "Epoch 31/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.8454 - accuracy: 0.6563 - val_loss: 0.8122 - val_accuracy: 0.6738\n",
      "Epoch 32/100\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.8264 - accuracy: 0.6624 - val_loss: 0.7978 - val_accuracy: 0.6708\n",
      "Epoch 33/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.8139 - accuracy: 0.6693 - val_loss: 0.8050 - val_accuracy: 0.6809\n",
      "Epoch 34/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.8187 - accuracy: 0.6667 - val_loss: 0.7827 - val_accuracy: 0.7009\n",
      "Epoch 35/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7992 - accuracy: 0.6844 - val_loss: 0.7824 - val_accuracy: 0.6927\n",
      "Epoch 36/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.8034 - accuracy: 0.6814 - val_loss: 0.7958 - val_accuracy: 0.6838\n",
      "Epoch 37/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.8084 - accuracy: 0.6761 - val_loss: 0.7852 - val_accuracy: 0.6832\n",
      "Epoch 38/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.7876 - accuracy: 0.6859 - val_loss: 0.7715 - val_accuracy: 0.6939\n",
      "Epoch 39/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.7891 - accuracy: 0.6887 - val_loss: 0.7800 - val_accuracy: 0.6944\n",
      "Epoch 40/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.7875 - accuracy: 0.6820 - val_loss: 0.7768 - val_accuracy: 0.6868\n",
      "Epoch 41/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.7824 - accuracy: 0.6890 - val_loss: 0.7996 - val_accuracy: 0.6779\n",
      "Epoch 42/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.7830 - accuracy: 0.6865 - val_loss: 0.7825 - val_accuracy: 0.6862\n",
      "Epoch 43/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7739 - accuracy: 0.6837 - val_loss: 0.7747 - val_accuracy: 0.6909\n",
      "Epoch 44/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.7697 - accuracy: 0.6919 - val_loss: 0.7860 - val_accuracy: 0.6791\n",
      "Epoch 45/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.7786 - accuracy: 0.6894 - val_loss: 0.7635 - val_accuracy: 0.6956\n",
      "Epoch 46/100\n",
      "106/106 [==============================] - 2s 20ms/step - loss: 0.7720 - accuracy: 0.6925 - val_loss: 0.7795 - val_accuracy: 0.6962\n",
      "Epoch 47/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.7496 - accuracy: 0.7033 - val_loss: 0.7609 - val_accuracy: 0.6897\n",
      "Epoch 48/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.7611 - accuracy: 0.6996 - val_loss: 0.7654 - val_accuracy: 0.6921\n",
      "Epoch 49/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.7547 - accuracy: 0.6949 - val_loss: 0.7622 - val_accuracy: 0.6939\n",
      "Epoch 50/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.7363 - accuracy: 0.7095 - val_loss: 0.7715 - val_accuracy: 0.6950\n",
      "Epoch 51/100\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.7532 - accuracy: 0.7004 - val_loss: 0.7439 - val_accuracy: 0.7027\n",
      "Epoch 52/100\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.7519 - accuracy: 0.6986 - val_loss: 0.7754 - val_accuracy: 0.6933\n",
      "Epoch 53/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.7517 - accuracy: 0.7049 - val_loss: 0.7384 - val_accuracy: 0.7063\n",
      "Epoch 54/100\n",
      "106/106 [==============================] - 2s 20ms/step - loss: 0.7492 - accuracy: 0.7108 - val_loss: 0.7525 - val_accuracy: 0.6927\n",
      "Epoch 55/100\n",
      "106/106 [==============================] - 2s 20ms/step - loss: 0.7563 - accuracy: 0.6958 - val_loss: 0.7429 - val_accuracy: 0.7027\n",
      "Epoch 56/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.7377 - accuracy: 0.7083 - val_loss: 0.7626 - val_accuracy: 0.6891\n",
      "Epoch 57/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.7233 - accuracy: 0.7126 - val_loss: 0.7545 - val_accuracy: 0.7009\n",
      "Epoch 58/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.7483 - accuracy: 0.7012 - val_loss: 0.7564 - val_accuracy: 0.6885\n",
      "Epoch 59/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.7256 - accuracy: 0.7086 - val_loss: 0.7487 - val_accuracy: 0.7069\n",
      "Epoch 60/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.7312 - accuracy: 0.7117 - val_loss: 0.7620 - val_accuracy: 0.6820\n",
      "Epoch 61/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.7245 - accuracy: 0.7166 - val_loss: 0.7604 - val_accuracy: 0.6879\n",
      "Epoch 62/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.7303 - accuracy: 0.7105 - val_loss: 0.7439 - val_accuracy: 0.7086\n",
      "Epoch 63/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7229 - accuracy: 0.7169 - val_loss: 0.7548 - val_accuracy: 0.6950\n",
      "Epoch 64/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.7018 - accuracy: 0.7253 - val_loss: 0.7664 - val_accuracy: 0.6844\n",
      "Epoch 65/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7159 - accuracy: 0.7139 - val_loss: 0.7626 - val_accuracy: 0.6950\n",
      "Epoch 66/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7196 - accuracy: 0.7117 - val_loss: 0.7473 - val_accuracy: 0.6998\n",
      "Epoch 67/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7143 - accuracy: 0.7190 - val_loss: 0.7391 - val_accuracy: 0.7015\n",
      "Epoch 68/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7119 - accuracy: 0.7172 - val_loss: 0.7697 - val_accuracy: 0.6814\n",
      "Epoch 69/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.7232 - accuracy: 0.7138 - val_loss: 0.7559 - val_accuracy: 0.7039\n",
      "Epoch 70/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6951 - accuracy: 0.7287 - val_loss: 0.7501 - val_accuracy: 0.6974\n",
      "Epoch 71/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7049 - accuracy: 0.7207 - val_loss: 0.7657 - val_accuracy: 0.6986\n",
      "Epoch 72/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.7057 - accuracy: 0.7145 - val_loss: 0.7795 - val_accuracy: 0.6909\n",
      "Epoch 73/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7010 - accuracy: 0.7244 - val_loss: 0.7316 - val_accuracy: 0.7015\n",
      "Epoch 74/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7085 - accuracy: 0.7151 - val_loss: 0.7553 - val_accuracy: 0.7015\n",
      "Epoch 75/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6987 - accuracy: 0.7233 - val_loss: 0.7329 - val_accuracy: 0.7092\n",
      "Epoch 76/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6971 - accuracy: 0.7270 - val_loss: 0.7475 - val_accuracy: 0.7151\n",
      "Epoch 77/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.7032 - accuracy: 0.7219 - val_loss: 0.7545 - val_accuracy: 0.6998\n",
      "Epoch 78/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6953 - accuracy: 0.7277 - val_loss: 0.7756 - val_accuracy: 0.6862\n",
      "Epoch 79/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6953 - accuracy: 0.7292 - val_loss: 0.7358 - val_accuracy: 0.7039\n",
      "Epoch 80/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6926 - accuracy: 0.7314 - val_loss: 0.7356 - val_accuracy: 0.6962\n",
      "Epoch 81/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6950 - accuracy: 0.7298 - val_loss: 0.7435 - val_accuracy: 0.6980\n",
      "Epoch 82/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6887 - accuracy: 0.7293 - val_loss: 0.7368 - val_accuracy: 0.7039\n",
      "Epoch 83/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6939 - accuracy: 0.7250 - val_loss: 0.7169 - val_accuracy: 0.7151\n",
      "Epoch 84/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6871 - accuracy: 0.7348 - val_loss: 0.7307 - val_accuracy: 0.7098\n",
      "Epoch 85/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6783 - accuracy: 0.7352 - val_loss: 0.7568 - val_accuracy: 0.6868\n",
      "Epoch 86/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6858 - accuracy: 0.7358 - val_loss: 0.7178 - val_accuracy: 0.7116\n",
      "Epoch 87/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6954 - accuracy: 0.7204 - val_loss: 0.7312 - val_accuracy: 0.7199\n",
      "Epoch 88/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6851 - accuracy: 0.7326 - val_loss: 0.7243 - val_accuracy: 0.7139\n",
      "Epoch 89/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.7026 - accuracy: 0.7255 - val_loss: 0.7175 - val_accuracy: 0.7104\n",
      "Epoch 90/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6706 - accuracy: 0.7380 - val_loss: 0.7585 - val_accuracy: 0.6956\n",
      "Epoch 91/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6801 - accuracy: 0.7407 - val_loss: 0.7340 - val_accuracy: 0.7110\n",
      "Epoch 92/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6907 - accuracy: 0.7255 - val_loss: 0.7201 - val_accuracy: 0.7069\n",
      "Epoch 93/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6955 - accuracy: 0.7244 - val_loss: 0.7272 - val_accuracy: 0.7074\n",
      "Epoch 94/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6877 - accuracy: 0.7281 - val_loss: 0.7368 - val_accuracy: 0.7015\n",
      "Epoch 95/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6766 - accuracy: 0.7305 - val_loss: 0.7302 - val_accuracy: 0.7063\n",
      "Epoch 96/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6756 - accuracy: 0.7346 - val_loss: 0.7137 - val_accuracy: 0.7199\n",
      "Epoch 97/100\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.6806 - accuracy: 0.7357 - val_loss: 0.7274 - val_accuracy: 0.7163\n",
      "Epoch 98/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6822 - accuracy: 0.7337 - val_loss: 0.7404 - val_accuracy: 0.7057\n",
      "Epoch 99/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6674 - accuracy: 0.7366 - val_loss: 0.7276 - val_accuracy: 0.7204\n",
      "Epoch 100/100\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6717 - accuracy: 0.7314 - val_loss: 0.7115 - val_accuracy: 0.7258\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.6768 - accuracy: 0.7421\n",
      "Shape of X after trimming: (1692, 22, 700)\n",
      "Shape of X after trimming: (423, 22, 700)\n",
      "Shape of X after trimming: (443, 22, 700)\n",
      "Shape of training set: (6768, 22, 350)\n",
      "Shape of validation set: (1692, 22, 350)\n",
      "Shape of training labels: (6768,)\n",
      "Shape of validation labels: (1692,)\n",
      "Shape of testing set: (1772, 22, 350)\n",
      "Shape of testing labels: (1772,)\n",
      "Shape of training labels after categorical conversion: (6768, 4)\n",
      "Shape of validation labels after categorical conversion: (1692, 4)\n",
      "Shape of test labels after categorical conversion: (1772, 4)\n",
      "Shape of training set after adding width info: (6768, 22, 350, 1)\n",
      "Shape of validation set after adding width info: (1692, 22, 350, 1)\n",
      "Shape of test set after adding width info: (1772, 22, 350, 1)\n",
      "Shape of training set after dimension reshaping: (6768, 350, 1, 22)\n",
      "Shape of validation set after dimension reshaping: (1692, 350, 1, 22)\n",
      "Shape of test set after dimension reshaping: (1772, 350, 1, 22)\n",
      "Epoch 1/100\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 1.9998 - accuracy: 0.2726 - val_loss: 1.4920 - val_accuracy: 0.3233\n",
      "Epoch 2/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 1.5765 - accuracy: 0.3156 - val_loss: 1.3295 - val_accuracy: 0.3487\n",
      "Epoch 3/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 1.4222 - accuracy: 0.3452 - val_loss: 1.2804 - val_accuracy: 0.3889\n",
      "Epoch 4/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 1.3066 - accuracy: 0.4019 - val_loss: 1.2331 - val_accuracy: 0.4303\n",
      "Epoch 5/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 1.2433 - accuracy: 0.4331 - val_loss: 1.2017 - val_accuracy: 0.4628\n",
      "Epoch 6/100\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 1.2032 - accuracy: 0.4659 - val_loss: 1.1826 - val_accuracy: 0.4728\n",
      "Epoch 7/100\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 1.1739 - accuracy: 0.4874 - val_loss: 1.1309 - val_accuracy: 0.5083\n",
      "Epoch 8/100\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 1.1444 - accuracy: 0.4987 - val_loss: 1.1124 - val_accuracy: 0.5361\n",
      "Epoch 9/100\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 1.1200 - accuracy: 0.5179 - val_loss: 1.0857 - val_accuracy: 0.5414\n",
      "Epoch 10/100\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 1.0981 - accuracy: 0.5232 - val_loss: 1.0945 - val_accuracy: 0.5325\n",
      "Epoch 11/100\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 1.0864 - accuracy: 0.5381 - val_loss: 1.0342 - val_accuracy: 0.5881\n",
      "Epoch 12/100\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 1.0707 - accuracy: 0.5443 - val_loss: 1.0403 - val_accuracy: 0.5727\n",
      "Epoch 13/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 1.0497 - accuracy: 0.5588 - val_loss: 1.0255 - val_accuracy: 0.5922\n",
      "Epoch 14/100\n",
      "106/106 [==============================] - 2s 20ms/step - loss: 1.0439 - accuracy: 0.5624 - val_loss: 1.0348 - val_accuracy: 0.5804\n",
      "Epoch 15/100\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 1.0168 - accuracy: 0.5720 - val_loss: 0.9838 - val_accuracy: 0.6123\n",
      "Epoch 16/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 1.0094 - accuracy: 0.5822 - val_loss: 0.9830 - val_accuracy: 0.6099\n",
      "Epoch 17/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 1.0099 - accuracy: 0.5810 - val_loss: 0.9630 - val_accuracy: 0.6235\n",
      "Epoch 18/100\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.9712 - accuracy: 0.5983 - val_loss: 0.9708 - val_accuracy: 0.5969\n",
      "Epoch 19/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.9505 - accuracy: 0.6150 - val_loss: 0.9345 - val_accuracy: 0.6353\n",
      "Epoch 20/100\n",
      "106/106 [==============================] - 2s 21ms/step - loss: 0.9460 - accuracy: 0.6132 - val_loss: 0.9720 - val_accuracy: 0.5887\n",
      "Epoch 21/100\n",
      "106/106 [==============================] - 2s 21ms/step - loss: 0.9343 - accuracy: 0.6204 - val_loss: 0.8973 - val_accuracy: 0.6407\n",
      "Epoch 22/100\n",
      "106/106 [==============================] - 2s 21ms/step - loss: 0.9317 - accuracy: 0.6175 - val_loss: 0.9428 - val_accuracy: 0.6152\n",
      "Epoch 23/100\n",
      "106/106 [==============================] - 2s 20ms/step - loss: 0.9072 - accuracy: 0.6296 - val_loss: 0.8663 - val_accuracy: 0.6673\n",
      "Epoch 24/100\n",
      "106/106 [==============================] - 2s 20ms/step - loss: 0.8929 - accuracy: 0.6399 - val_loss: 0.8552 - val_accuracy: 0.6749\n",
      "Epoch 25/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.8872 - accuracy: 0.6395 - val_loss: 0.8715 - val_accuracy: 0.6525\n",
      "Epoch 26/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.8987 - accuracy: 0.6325 - val_loss: 0.8576 - val_accuracy: 0.6602\n",
      "Epoch 27/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.8815 - accuracy: 0.6436 - val_loss: 0.8645 - val_accuracy: 0.6501\n",
      "Epoch 28/100\n",
      "106/106 [==============================] - 2s 21ms/step - loss: 0.8687 - accuracy: 0.6495 - val_loss: 0.8323 - val_accuracy: 0.6761\n",
      "Epoch 29/100\n",
      "106/106 [==============================] - 2s 20ms/step - loss: 0.8574 - accuracy: 0.6483 - val_loss: 0.8394 - val_accuracy: 0.6832\n",
      "Epoch 30/100\n",
      "106/106 [==============================] - 2s 20ms/step - loss: 0.8618 - accuracy: 0.6554 - val_loss: 0.8359 - val_accuracy: 0.6684\n",
      "Epoch 31/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.8301 - accuracy: 0.6615 - val_loss: 0.8230 - val_accuracy: 0.6726\n",
      "Epoch 32/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.8360 - accuracy: 0.6596 - val_loss: 0.8743 - val_accuracy: 0.6424\n",
      "Epoch 33/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.8376 - accuracy: 0.6622 - val_loss: 0.8228 - val_accuracy: 0.6738\n",
      "Epoch 34/100\n",
      "106/106 [==============================] - 2s 20ms/step - loss: 0.8264 - accuracy: 0.6692 - val_loss: 0.8164 - val_accuracy: 0.6767\n",
      "Epoch 35/100\n",
      "106/106 [==============================] - 2s 20ms/step - loss: 0.8302 - accuracy: 0.6637 - val_loss: 0.8146 - val_accuracy: 0.6785\n",
      "Epoch 36/100\n",
      "106/106 [==============================] - 2s 21ms/step - loss: 0.8154 - accuracy: 0.6687 - val_loss: 0.8177 - val_accuracy: 0.6909\n",
      "Epoch 37/100\n",
      "106/106 [==============================] - 2s 21ms/step - loss: 0.8109 - accuracy: 0.6723 - val_loss: 0.8059 - val_accuracy: 0.6879\n",
      "Epoch 38/100\n",
      "106/106 [==============================] - 2s 21ms/step - loss: 0.8295 - accuracy: 0.6624 - val_loss: 0.8288 - val_accuracy: 0.6631\n",
      "Epoch 39/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.8068 - accuracy: 0.6775 - val_loss: 0.8055 - val_accuracy: 0.6850\n",
      "Epoch 40/100\n",
      "106/106 [==============================] - 2s 20ms/step - loss: 0.7979 - accuracy: 0.6865 - val_loss: 0.8174 - val_accuracy: 0.6814\n",
      "Epoch 41/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.8009 - accuracy: 0.6766 - val_loss: 0.8090 - val_accuracy: 0.6838\n",
      "Epoch 42/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.8013 - accuracy: 0.6825 - val_loss: 0.7789 - val_accuracy: 0.7057\n",
      "Epoch 43/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.7954 - accuracy: 0.6814 - val_loss: 0.7788 - val_accuracy: 0.7104\n",
      "Epoch 44/100\n",
      "106/106 [==============================] - 2s 20ms/step - loss: 0.7908 - accuracy: 0.6856 - val_loss: 0.8370 - val_accuracy: 0.6655\n",
      "Epoch 45/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.7835 - accuracy: 0.6875 - val_loss: 0.8170 - val_accuracy: 0.6631\n",
      "Epoch 46/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.7883 - accuracy: 0.6801 - val_loss: 0.8076 - val_accuracy: 0.6885\n",
      "Epoch 47/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.7875 - accuracy: 0.6819 - val_loss: 0.7836 - val_accuracy: 0.7128\n",
      "Epoch 48/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.7782 - accuracy: 0.6882 - val_loss: 0.8463 - val_accuracy: 0.6466\n",
      "Epoch 49/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.7690 - accuracy: 0.6900 - val_loss: 0.7698 - val_accuracy: 0.7157\n",
      "Epoch 50/100\n",
      "106/106 [==============================] - 2s 20ms/step - loss: 0.7683 - accuracy: 0.6925 - val_loss: 0.7776 - val_accuracy: 0.7134\n",
      "Epoch 51/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.7652 - accuracy: 0.6927 - val_loss: 0.7937 - val_accuracy: 0.6968\n",
      "Epoch 52/100\n",
      "106/106 [==============================] - 2s 20ms/step - loss: 0.7730 - accuracy: 0.6874 - val_loss: 0.7728 - val_accuracy: 0.7009\n",
      "Epoch 53/100\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.7736 - accuracy: 0.6922 - val_loss: 0.7774 - val_accuracy: 0.6992\n",
      "Epoch 54/100\n",
      "106/106 [==============================] - 2s 20ms/step - loss: 0.7585 - accuracy: 0.7005 - val_loss: 0.7665 - val_accuracy: 0.7033\n",
      "Epoch 55/100\n",
      "106/106 [==============================] - 2s 20ms/step - loss: 0.7623 - accuracy: 0.6918 - val_loss: 0.7706 - val_accuracy: 0.7027\n",
      "Epoch 56/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.7544 - accuracy: 0.6984 - val_loss: 0.7647 - val_accuracy: 0.7015\n",
      "Epoch 57/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.7455 - accuracy: 0.7030 - val_loss: 0.7811 - val_accuracy: 0.7069\n",
      "Epoch 58/100\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.7500 - accuracy: 0.7045 - val_loss: 0.7993 - val_accuracy: 0.6891\n",
      "Epoch 59/100\n",
      "106/106 [==============================] - 2s 21ms/step - loss: 0.7449 - accuracy: 0.7018 - val_loss: 0.7717 - val_accuracy: 0.7128\n",
      "Epoch 60/100\n",
      "106/106 [==============================] - 2s 20ms/step - loss: 0.7450 - accuracy: 0.6987 - val_loss: 0.7629 - val_accuracy: 0.7199\n",
      "Epoch 61/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.7514 - accuracy: 0.6975 - val_loss: 0.8045 - val_accuracy: 0.6743\n",
      "Epoch 62/100\n",
      "106/106 [==============================] - 2s 21ms/step - loss: 0.7471 - accuracy: 0.7007 - val_loss: 0.7656 - val_accuracy: 0.7181\n",
      "Epoch 63/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.7342 - accuracy: 0.7042 - val_loss: 0.7747 - val_accuracy: 0.6998\n",
      "Epoch 64/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.7274 - accuracy: 0.7042 - val_loss: 0.7654 - val_accuracy: 0.7134\n",
      "Epoch 65/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.7232 - accuracy: 0.7100 - val_loss: 0.7663 - val_accuracy: 0.7027\n",
      "Epoch 66/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.7204 - accuracy: 0.7154 - val_loss: 0.7745 - val_accuracy: 0.7063\n",
      "Epoch 67/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.7239 - accuracy: 0.7092 - val_loss: 0.7629 - val_accuracy: 0.7039\n",
      "Epoch 68/100\n",
      "106/106 [==============================] - 2s 21ms/step - loss: 0.7340 - accuracy: 0.7027 - val_loss: 0.7802 - val_accuracy: 0.6962\n",
      "Epoch 69/100\n",
      "106/106 [==============================] - 2s 20ms/step - loss: 0.7299 - accuracy: 0.7131 - val_loss: 0.7575 - val_accuracy: 0.7187\n",
      "Epoch 70/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.7134 - accuracy: 0.7108 - val_loss: 0.7718 - val_accuracy: 0.6974\n",
      "Epoch 71/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.7162 - accuracy: 0.7194 - val_loss: 0.7397 - val_accuracy: 0.7335\n",
      "Epoch 72/100\n",
      "106/106 [==============================] - 2s 20ms/step - loss: 0.7233 - accuracy: 0.7131 - val_loss: 0.7678 - val_accuracy: 0.6974\n",
      "Epoch 73/100\n",
      "106/106 [==============================] - 2s 20ms/step - loss: 0.7201 - accuracy: 0.7153 - val_loss: 0.7453 - val_accuracy: 0.7193\n",
      "Epoch 74/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.7104 - accuracy: 0.7202 - val_loss: 0.7361 - val_accuracy: 0.7323\n",
      "Epoch 75/100\n",
      "106/106 [==============================] - 2s 20ms/step - loss: 0.7250 - accuracy: 0.7091 - val_loss: 0.7539 - val_accuracy: 0.7139\n",
      "Epoch 76/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.7185 - accuracy: 0.7148 - val_loss: 0.7460 - val_accuracy: 0.7240\n",
      "Epoch 77/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.7161 - accuracy: 0.7168 - val_loss: 0.7617 - val_accuracy: 0.7027\n",
      "Epoch 78/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.7235 - accuracy: 0.7126 - val_loss: 0.7538 - val_accuracy: 0.7234\n",
      "Epoch 79/100\n",
      "106/106 [==============================] - 2s 21ms/step - loss: 0.7124 - accuracy: 0.7185 - val_loss: 0.7480 - val_accuracy: 0.7216\n",
      "Epoch 80/100\n",
      "106/106 [==============================] - 2s 24ms/step - loss: 0.7050 - accuracy: 0.7175 - val_loss: 0.7385 - val_accuracy: 0.7329\n",
      "Epoch 81/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.7062 - accuracy: 0.7240 - val_loss: 0.7321 - val_accuracy: 0.7299\n",
      "Epoch 82/100\n",
      "106/106 [==============================] - 2s 20ms/step - loss: 0.7039 - accuracy: 0.7187 - val_loss: 0.7344 - val_accuracy: 0.7252\n",
      "Epoch 83/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.7060 - accuracy: 0.7227 - val_loss: 0.7532 - val_accuracy: 0.7080\n",
      "Epoch 84/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.7002 - accuracy: 0.7219 - val_loss: 0.7525 - val_accuracy: 0.7015\n",
      "Epoch 85/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.7061 - accuracy: 0.7153 - val_loss: 0.7293 - val_accuracy: 0.7293\n",
      "Epoch 86/100\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.6859 - accuracy: 0.7357 - val_loss: 0.7352 - val_accuracy: 0.7163\n",
      "Epoch 87/100\n",
      "106/106 [==============================] - 2s 20ms/step - loss: 0.6912 - accuracy: 0.7243 - val_loss: 0.7427 - val_accuracy: 0.7175\n",
      "Epoch 88/100\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.6935 - accuracy: 0.7240 - val_loss: 0.7315 - val_accuracy: 0.7299\n",
      "Epoch 89/100\n",
      "106/106 [==============================] - 4s 34ms/step - loss: 0.6763 - accuracy: 0.7337 - val_loss: 0.7372 - val_accuracy: 0.7364\n",
      "Epoch 90/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.6961 - accuracy: 0.7219 - val_loss: 0.7383 - val_accuracy: 0.7317\n",
      "Epoch 91/100\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7064 - accuracy: 0.7156 - val_loss: 0.7487 - val_accuracy: 0.7275\n",
      "Epoch 92/100\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.6978 - accuracy: 0.7244 - val_loss: 0.7307 - val_accuracy: 0.7240\n",
      "Epoch 93/100\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.6891 - accuracy: 0.7231 - val_loss: 0.7315 - val_accuracy: 0.7222\n",
      "Epoch 94/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.6815 - accuracy: 0.7270 - val_loss: 0.7342 - val_accuracy: 0.7287\n",
      "Epoch 95/100\n",
      "106/106 [==============================] - 2s 20ms/step - loss: 0.6802 - accuracy: 0.7315 - val_loss: 0.7166 - val_accuracy: 0.7364\n",
      "Epoch 96/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.6851 - accuracy: 0.7336 - val_loss: 0.7123 - val_accuracy: 0.7346\n",
      "Epoch 97/100\n",
      "106/106 [==============================] - 4s 38ms/step - loss: 0.6707 - accuracy: 0.7267 - val_loss: 0.7336 - val_accuracy: 0.7275\n",
      "Epoch 98/100\n",
      "106/106 [==============================] - 4s 35ms/step - loss: 0.6869 - accuracy: 0.7287 - val_loss: 0.7312 - val_accuracy: 0.7299\n",
      "Epoch 99/100\n",
      "106/106 [==============================] - 2s 21ms/step - loss: 0.6882 - accuracy: 0.7246 - val_loss: 0.7267 - val_accuracy: 0.7240\n",
      "Epoch 100/100\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.6878 - accuracy: 0.7357 - val_loss: 0.7147 - val_accuracy: 0.7429\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 0.7463 - accuracy: 0.7054\n",
      "Shape of X after trimming: (1692, 22, 800)\n",
      "Shape of X after trimming: (423, 22, 800)\n",
      "Shape of X after trimming: (443, 22, 800)\n",
      "Shape of training set: (6768, 22, 400)\n",
      "Shape of validation set: (1692, 22, 400)\n",
      "Shape of training labels: (6768,)\n",
      "Shape of validation labels: (1692,)\n",
      "Shape of testing set: (1772, 22, 400)\n",
      "Shape of testing labels: (1772,)\n",
      "Shape of training labels after categorical conversion: (6768, 4)\n",
      "Shape of validation labels after categorical conversion: (1692, 4)\n",
      "Shape of test labels after categorical conversion: (1772, 4)\n",
      "Shape of training set after adding width info: (6768, 22, 400, 1)\n",
      "Shape of validation set after adding width info: (1692, 22, 400, 1)\n",
      "Shape of test set after adding width info: (1772, 22, 400, 1)\n",
      "Shape of training set after dimension reshaping: (6768, 400, 1, 22)\n",
      "Shape of validation set after dimension reshaping: (1692, 400, 1, 22)\n",
      "Shape of test set after dimension reshaping: (1772, 400, 1, 22)\n",
      "Epoch 1/100\n",
      "106/106 [==============================] - 5s 38ms/step - loss: 2.0216 - accuracy: 0.2688 - val_loss: 1.4099 - val_accuracy: 0.3688\n",
      "Epoch 2/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 1.6251 - accuracy: 0.3042 - val_loss: 1.3154 - val_accuracy: 0.3936\n",
      "Epoch 3/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 1.4674 - accuracy: 0.3361 - val_loss: 1.2974 - val_accuracy: 0.4084\n",
      "Epoch 4/100\n",
      "106/106 [==============================] - 4s 36ms/step - loss: 1.3857 - accuracy: 0.3494 - val_loss: 1.2989 - val_accuracy: 0.3741\n",
      "Epoch 5/100\n",
      "106/106 [==============================] - 4s 33ms/step - loss: 1.3255 - accuracy: 0.3778 - val_loss: 1.2660 - val_accuracy: 0.4108\n",
      "Epoch 6/100\n",
      "106/106 [==============================] - 4s 36ms/step - loss: 1.2481 - accuracy: 0.4317 - val_loss: 1.2034 - val_accuracy: 0.4728\n",
      "Epoch 7/100\n",
      "106/106 [==============================] - 4s 34ms/step - loss: 1.2069 - accuracy: 0.4607 - val_loss: 1.1511 - val_accuracy: 0.4876\n",
      "Epoch 8/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 1.1632 - accuracy: 0.4932 - val_loss: 1.1217 - val_accuracy: 0.5136\n",
      "Epoch 9/100\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 1.1325 - accuracy: 0.5056 - val_loss: 1.0651 - val_accuracy: 0.5278\n",
      "Epoch 10/100\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 1.1092 - accuracy: 0.5137 - val_loss: 1.0698 - val_accuracy: 0.5219\n",
      "Epoch 11/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 1.0892 - accuracy: 0.5334 - val_loss: 1.0782 - val_accuracy: 0.5254\n",
      "Epoch 12/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 1.0715 - accuracy: 0.5445 - val_loss: 1.0475 - val_accuracy: 0.5355\n",
      "Epoch 13/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 1.0591 - accuracy: 0.5529 - val_loss: 1.0125 - val_accuracy: 0.5597\n",
      "Epoch 14/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 1.0325 - accuracy: 0.5649 - val_loss: 1.0118 - val_accuracy: 0.5621\n",
      "Epoch 15/100\n",
      "106/106 [==============================] - 4s 37ms/step - loss: 1.0217 - accuracy: 0.5801 - val_loss: 1.0069 - val_accuracy: 0.5709\n",
      "Epoch 16/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 1.0068 - accuracy: 0.5881 - val_loss: 1.0358 - val_accuracy: 0.5485\n",
      "Epoch 17/100\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.9766 - accuracy: 0.5888 - val_loss: 0.9401 - val_accuracy: 0.6123\n",
      "Epoch 18/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.9691 - accuracy: 0.6033 - val_loss: 0.9402 - val_accuracy: 0.6087\n",
      "Epoch 19/100\n",
      "106/106 [==============================] - 3s 33ms/step - loss: 0.9513 - accuracy: 0.6132 - val_loss: 0.9054 - val_accuracy: 0.6271\n",
      "Epoch 20/100\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.9443 - accuracy: 0.6123 - val_loss: 0.9240 - val_accuracy: 0.6117\n",
      "Epoch 21/100\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.9273 - accuracy: 0.6127 - val_loss: 0.9203 - val_accuracy: 0.6070\n",
      "Epoch 22/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.9220 - accuracy: 0.6226 - val_loss: 0.8574 - val_accuracy: 0.6537\n",
      "Epoch 23/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.9080 - accuracy: 0.6325 - val_loss: 0.9083 - val_accuracy: 0.6265\n",
      "Epoch 24/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.9040 - accuracy: 0.6294 - val_loss: 0.8781 - val_accuracy: 0.6430\n",
      "Epoch 25/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.8948 - accuracy: 0.6287 - val_loss: 0.8424 - val_accuracy: 0.6525\n",
      "Epoch 26/100\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.8885 - accuracy: 0.6374 - val_loss: 0.8090 - val_accuracy: 0.6820\n",
      "Epoch 27/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.8737 - accuracy: 0.6486 - val_loss: 0.7973 - val_accuracy: 0.6649\n",
      "Epoch 28/100\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.8629 - accuracy: 0.6553 - val_loss: 0.8230 - val_accuracy: 0.6773\n",
      "Epoch 29/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.8557 - accuracy: 0.6529 - val_loss: 0.8216 - val_accuracy: 0.6732\n",
      "Epoch 30/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.8379 - accuracy: 0.6643 - val_loss: 0.8748 - val_accuracy: 0.6478\n",
      "Epoch 31/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.8495 - accuracy: 0.6590 - val_loss: 0.8273 - val_accuracy: 0.6809\n",
      "Epoch 32/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.8394 - accuracy: 0.6630 - val_loss: 0.7903 - val_accuracy: 0.6921\n",
      "Epoch 33/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.8273 - accuracy: 0.6602 - val_loss: 0.8338 - val_accuracy: 0.6613\n",
      "Epoch 34/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.8405 - accuracy: 0.6613 - val_loss: 0.8111 - val_accuracy: 0.6797\n",
      "Epoch 35/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.8234 - accuracy: 0.6732 - val_loss: 0.8175 - val_accuracy: 0.6637\n",
      "Epoch 36/100\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.8149 - accuracy: 0.6782 - val_loss: 0.8074 - val_accuracy: 0.6809\n",
      "Epoch 37/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.8010 - accuracy: 0.6775 - val_loss: 0.8504 - val_accuracy: 0.6619\n",
      "Epoch 38/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.8048 - accuracy: 0.6766 - val_loss: 0.8007 - val_accuracy: 0.6779\n",
      "Epoch 39/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.8001 - accuracy: 0.6788 - val_loss: 0.7830 - val_accuracy: 0.6684\n",
      "Epoch 40/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.7933 - accuracy: 0.6856 - val_loss: 0.7722 - val_accuracy: 0.6826\n",
      "Epoch 41/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.7865 - accuracy: 0.6853 - val_loss: 0.8426 - val_accuracy: 0.6342\n",
      "Epoch 42/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.8035 - accuracy: 0.6791 - val_loss: 0.7998 - val_accuracy: 0.6678\n",
      "Epoch 43/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.7798 - accuracy: 0.6835 - val_loss: 0.8323 - val_accuracy: 0.6608\n",
      "Epoch 44/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.7943 - accuracy: 0.6854 - val_loss: 0.8089 - val_accuracy: 0.6708\n",
      "Epoch 45/100\n",
      "106/106 [==============================] - 2s 21ms/step - loss: 0.7781 - accuracy: 0.6878 - val_loss: 0.7940 - val_accuracy: 0.6785\n",
      "Epoch 46/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.7905 - accuracy: 0.6804 - val_loss: 0.7681 - val_accuracy: 0.6885\n",
      "Epoch 47/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.7730 - accuracy: 0.6928 - val_loss: 0.7998 - val_accuracy: 0.6738\n",
      "Epoch 48/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.7561 - accuracy: 0.6949 - val_loss: 0.7862 - val_accuracy: 0.6832\n",
      "Epoch 49/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.7634 - accuracy: 0.6974 - val_loss: 0.8058 - val_accuracy: 0.6643\n",
      "Epoch 50/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.7613 - accuracy: 0.7033 - val_loss: 0.8071 - val_accuracy: 0.6690\n",
      "Epoch 51/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.7519 - accuracy: 0.7017 - val_loss: 0.8032 - val_accuracy: 0.6708\n",
      "Epoch 52/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.7695 - accuracy: 0.6974 - val_loss: 0.7700 - val_accuracy: 0.6820\n",
      "Epoch 53/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.7522 - accuracy: 0.7020 - val_loss: 0.8194 - val_accuracy: 0.6489\n",
      "Epoch 54/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.7446 - accuracy: 0.7057 - val_loss: 0.7533 - val_accuracy: 0.7151\n",
      "Epoch 55/100\n",
      "106/106 [==============================] - 4s 41ms/step - loss: 0.7376 - accuracy: 0.7060 - val_loss: 0.7454 - val_accuracy: 0.6956\n",
      "Epoch 56/100\n",
      "106/106 [==============================] - 5s 45ms/step - loss: 0.7644 - accuracy: 0.6970 - val_loss: 0.7804 - val_accuracy: 0.6838\n",
      "Epoch 57/100\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.7470 - accuracy: 0.7030 - val_loss: 0.7443 - val_accuracy: 0.6891\n",
      "Epoch 58/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.7328 - accuracy: 0.7080 - val_loss: 0.7697 - val_accuracy: 0.6773\n",
      "Epoch 59/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.7290 - accuracy: 0.7166 - val_loss: 0.7778 - val_accuracy: 0.6838\n",
      "Epoch 60/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.7313 - accuracy: 0.7086 - val_loss: 0.7745 - val_accuracy: 0.6879\n",
      "Epoch 61/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.7194 - accuracy: 0.7171 - val_loss: 0.7933 - val_accuracy: 0.6862\n",
      "Epoch 62/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.7374 - accuracy: 0.7045 - val_loss: 0.7503 - val_accuracy: 0.6921\n",
      "Epoch 63/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.7342 - accuracy: 0.7077 - val_loss: 0.8320 - val_accuracy: 0.6602\n",
      "Epoch 64/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.7371 - accuracy: 0.7054 - val_loss: 0.7417 - val_accuracy: 0.6962\n",
      "Epoch 65/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.7383 - accuracy: 0.7128 - val_loss: 0.7184 - val_accuracy: 0.7134\n",
      "Epoch 66/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.7278 - accuracy: 0.7067 - val_loss: 0.7563 - val_accuracy: 0.7039\n",
      "Epoch 67/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.7208 - accuracy: 0.7169 - val_loss: 0.7855 - val_accuracy: 0.6933\n",
      "Epoch 68/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.7192 - accuracy: 0.7141 - val_loss: 0.7459 - val_accuracy: 0.7033\n",
      "Epoch 69/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.7075 - accuracy: 0.7162 - val_loss: 0.7687 - val_accuracy: 0.6885\n",
      "Epoch 70/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.7203 - accuracy: 0.7199 - val_loss: 0.8004 - val_accuracy: 0.6856\n",
      "Epoch 71/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.7107 - accuracy: 0.7249 - val_loss: 0.7671 - val_accuracy: 0.6885\n",
      "Epoch 72/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.7060 - accuracy: 0.7193 - val_loss: 0.7550 - val_accuracy: 0.6974\n",
      "Epoch 73/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.7019 - accuracy: 0.7215 - val_loss: 0.8070 - val_accuracy: 0.6791\n",
      "Epoch 74/100\n",
      "106/106 [==============================] - 2s 22ms/step - loss: 0.7077 - accuracy: 0.7156 - val_loss: 0.7831 - val_accuracy: 0.6874\n",
      "Epoch 75/100\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.7014 - accuracy: 0.7190 - val_loss: 0.7810 - val_accuracy: 0.6844\n",
      "Epoch 76/100\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.7091 - accuracy: 0.7173 - val_loss: 0.7662 - val_accuracy: 0.7004\n",
      "Epoch 77/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.6966 - accuracy: 0.7259 - val_loss: 0.7240 - val_accuracy: 0.6998\n",
      "Epoch 78/100\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.6996 - accuracy: 0.7230 - val_loss: 0.7915 - val_accuracy: 0.6921\n",
      "Epoch 79/100\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.6907 - accuracy: 0.7283 - val_loss: 0.7608 - val_accuracy: 0.6939\n",
      "Epoch 80/100\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.6999 - accuracy: 0.7258 - val_loss: 0.7432 - val_accuracy: 0.6968\n",
      "Epoch 81/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.7121 - accuracy: 0.7210 - val_loss: 0.7182 - val_accuracy: 0.7092\n",
      "Epoch 82/100\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.6880 - accuracy: 0.7289 - val_loss: 0.7606 - val_accuracy: 0.6915\n",
      "Epoch 83/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.6923 - accuracy: 0.7265 - val_loss: 0.7804 - val_accuracy: 0.7015\n",
      "Epoch 84/100\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.6987 - accuracy: 0.7274 - val_loss: 0.7422 - val_accuracy: 0.7009\n",
      "Epoch 85/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.6973 - accuracy: 0.7243 - val_loss: 0.7266 - val_accuracy: 0.7086\n",
      "Epoch 86/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.7079 - accuracy: 0.7202 - val_loss: 0.7401 - val_accuracy: 0.7086\n",
      "Epoch 87/100\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.6903 - accuracy: 0.7335 - val_loss: 0.7235 - val_accuracy: 0.7163\n",
      "Epoch 88/100\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.6860 - accuracy: 0.7357 - val_loss: 0.8028 - val_accuracy: 0.6826\n",
      "Epoch 89/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.6873 - accuracy: 0.7268 - val_loss: 0.7705 - val_accuracy: 0.6962\n",
      "Epoch 90/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.6887 - accuracy: 0.7272 - val_loss: 0.7715 - val_accuracy: 0.6974\n",
      "Epoch 91/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.7003 - accuracy: 0.7253 - val_loss: 0.7621 - val_accuracy: 0.6950\n",
      "Epoch 92/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.6821 - accuracy: 0.7343 - val_loss: 0.7832 - val_accuracy: 0.6820\n",
      "Epoch 93/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.6815 - accuracy: 0.7265 - val_loss: 0.7488 - val_accuracy: 0.7045\n",
      "Epoch 94/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.6785 - accuracy: 0.7329 - val_loss: 0.8305 - val_accuracy: 0.6714\n",
      "Epoch 95/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.6747 - accuracy: 0.7333 - val_loss: 0.7366 - val_accuracy: 0.7187\n",
      "Epoch 96/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.6678 - accuracy: 0.7373 - val_loss: 0.7210 - val_accuracy: 0.7134\n",
      "Epoch 97/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.6727 - accuracy: 0.7351 - val_loss: 0.7264 - val_accuracy: 0.7086\n",
      "Epoch 98/100\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.6611 - accuracy: 0.7435 - val_loss: 0.7527 - val_accuracy: 0.7021\n",
      "Epoch 99/100\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.6693 - accuracy: 0.7312 - val_loss: 0.7583 - val_accuracy: 0.6832\n",
      "Epoch 100/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.6668 - accuracy: 0.7336 - val_loss: 0.7495 - val_accuracy: 0.7004\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.7398 - accuracy: 0.7026\n",
      "Shape of X after trimming: (1692, 22, 900)\n",
      "Shape of X after trimming: (423, 22, 900)\n",
      "Shape of X after trimming: (443, 22, 900)\n",
      "Shape of training set: (6768, 22, 450)\n",
      "Shape of validation set: (1692, 22, 450)\n",
      "Shape of training labels: (6768,)\n",
      "Shape of validation labels: (1692,)\n",
      "Shape of testing set: (1772, 22, 450)\n",
      "Shape of testing labels: (1772,)\n",
      "Shape of training labels after categorical conversion: (6768, 4)\n",
      "Shape of validation labels after categorical conversion: (1692, 4)\n",
      "Shape of test labels after categorical conversion: (1772, 4)\n",
      "Shape of training set after adding width info: (6768, 22, 450, 1)\n",
      "Shape of validation set after adding width info: (1692, 22, 450, 1)\n",
      "Shape of test set after adding width info: (1772, 22, 450, 1)\n",
      "Shape of training set after dimension reshaping: (6768, 450, 1, 22)\n",
      "Shape of validation set after dimension reshaping: (1692, 450, 1, 22)\n",
      "Shape of test set after dimension reshaping: (1772, 450, 1, 22)\n",
      "Epoch 1/100\n",
      "106/106 [==============================] - 4s 27ms/step - loss: 2.0161 - accuracy: 0.2713 - val_loss: 1.5265 - val_accuracy: 0.3416\n",
      "Epoch 2/100\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 1.6102 - accuracy: 0.2968 - val_loss: 1.3316 - val_accuracy: 0.3599\n",
      "Epoch 3/100\n",
      "106/106 [==============================] - 4s 34ms/step - loss: 1.4428 - accuracy: 0.3324 - val_loss: 1.3177 - val_accuracy: 0.3511\n",
      "Epoch 4/100\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 1.3514 - accuracy: 0.3652 - val_loss: 1.2991 - val_accuracy: 0.3712\n",
      "Epoch 5/100\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 1.2896 - accuracy: 0.4035 - val_loss: 1.2367 - val_accuracy: 0.4226\n",
      "Epoch 6/100\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 1.2314 - accuracy: 0.4378 - val_loss: 1.1715 - val_accuracy: 0.4864\n",
      "Epoch 7/100\n",
      "106/106 [==============================] - 4s 34ms/step - loss: 1.1973 - accuracy: 0.4605 - val_loss: 1.1518 - val_accuracy: 0.4799\n",
      "Epoch 8/100\n",
      "106/106 [==============================] - 5s 48ms/step - loss: 1.1616 - accuracy: 0.4835 - val_loss: 1.1209 - val_accuracy: 0.4994\n",
      "Epoch 9/100\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 1.1328 - accuracy: 0.5140 - val_loss: 1.0854 - val_accuracy: 0.5242\n",
      "Epoch 10/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 1.1136 - accuracy: 0.5294 - val_loss: 1.0737 - val_accuracy: 0.5437\n",
      "Epoch 11/100\n",
      "106/106 [==============================] - 4s 34ms/step - loss: 1.0946 - accuracy: 0.5347 - val_loss: 1.0327 - val_accuracy: 0.5561\n",
      "Epoch 12/100\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 1.0863 - accuracy: 0.5381 - val_loss: 1.0146 - val_accuracy: 0.5745\n",
      "Epoch 13/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 1.0538 - accuracy: 0.5541 - val_loss: 0.9778 - val_accuracy: 0.5981\n",
      "Epoch 14/100\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 1.0349 - accuracy: 0.5677 - val_loss: 0.9971 - val_accuracy: 0.5934\n",
      "Epoch 15/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 1.0120 - accuracy: 0.5770 - val_loss: 0.9122 - val_accuracy: 0.6596\n",
      "Epoch 16/100\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 1.0014 - accuracy: 0.5810 - val_loss: 0.9123 - val_accuracy: 0.6525\n",
      "Epoch 17/100\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.9801 - accuracy: 0.5928 - val_loss: 0.9079 - val_accuracy: 0.6489\n",
      "Epoch 18/100\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.9701 - accuracy: 0.6037 - val_loss: 0.9233 - val_accuracy: 0.6288\n",
      "Epoch 19/100\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.9469 - accuracy: 0.6077 - val_loss: 0.9257 - val_accuracy: 0.6223\n",
      "Epoch 20/100\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.9347 - accuracy: 0.6176 - val_loss: 0.8848 - val_accuracy: 0.6330\n",
      "Epoch 21/100\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.9299 - accuracy: 0.6206 - val_loss: 0.8526 - val_accuracy: 0.6797\n",
      "Epoch 22/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.9112 - accuracy: 0.6256 - val_loss: 0.8272 - val_accuracy: 0.6767\n",
      "Epoch 23/100\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.9060 - accuracy: 0.6305 - val_loss: 0.8280 - val_accuracy: 0.6838\n",
      "Epoch 24/100\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.8882 - accuracy: 0.6418 - val_loss: 0.8484 - val_accuracy: 0.6720\n",
      "Epoch 25/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.8975 - accuracy: 0.6377 - val_loss: 0.7933 - val_accuracy: 0.6998\n",
      "Epoch 26/100\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.8828 - accuracy: 0.6376 - val_loss: 0.8209 - val_accuracy: 0.6844\n",
      "Epoch 27/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.8657 - accuracy: 0.6556 - val_loss: 0.8044 - val_accuracy: 0.6909\n",
      "Epoch 28/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.8650 - accuracy: 0.6478 - val_loss: 0.8142 - val_accuracy: 0.6832\n",
      "Epoch 29/100\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8626 - accuracy: 0.6466 - val_loss: 0.7907 - val_accuracy: 0.6944\n",
      "Epoch 30/100\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.8400 - accuracy: 0.6572 - val_loss: 0.7834 - val_accuracy: 0.6903\n",
      "Epoch 31/100\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.8367 - accuracy: 0.6649 - val_loss: 0.7468 - val_accuracy: 0.7252\n",
      "Epoch 32/100\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.8336 - accuracy: 0.6597 - val_loss: 0.7688 - val_accuracy: 0.7074\n",
      "Epoch 33/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.8310 - accuracy: 0.6599 - val_loss: 0.7843 - val_accuracy: 0.6998\n",
      "Epoch 34/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.8142 - accuracy: 0.6684 - val_loss: 0.8189 - val_accuracy: 0.6755\n",
      "Epoch 35/100\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.8228 - accuracy: 0.6718 - val_loss: 0.7432 - val_accuracy: 0.7199\n",
      "Epoch 36/100\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.8175 - accuracy: 0.6677 - val_loss: 0.7378 - val_accuracy: 0.7163\n",
      "Epoch 37/100\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.8150 - accuracy: 0.6698 - val_loss: 0.7699 - val_accuracy: 0.7004\n",
      "Epoch 38/100\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.8097 - accuracy: 0.6724 - val_loss: 0.7308 - val_accuracy: 0.7134\n",
      "Epoch 39/100\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.8071 - accuracy: 0.6693 - val_loss: 0.7489 - val_accuracy: 0.7157\n",
      "Epoch 40/100\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7927 - accuracy: 0.6874 - val_loss: 0.7367 - val_accuracy: 0.7163\n",
      "Epoch 41/100\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7995 - accuracy: 0.6769 - val_loss: 0.7309 - val_accuracy: 0.7193\n",
      "Epoch 42/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.7888 - accuracy: 0.6841 - val_loss: 0.7255 - val_accuracy: 0.7151\n",
      "Epoch 43/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.7899 - accuracy: 0.6804 - val_loss: 0.7058 - val_accuracy: 0.7382\n",
      "Epoch 44/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.7754 - accuracy: 0.6859 - val_loss: 0.7315 - val_accuracy: 0.7139\n",
      "Epoch 45/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.7808 - accuracy: 0.6854 - val_loss: 0.7572 - val_accuracy: 0.6915\n",
      "Epoch 46/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.7681 - accuracy: 0.6965 - val_loss: 0.7299 - val_accuracy: 0.7063\n",
      "Epoch 47/100\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.7541 - accuracy: 0.7023 - val_loss: 0.6944 - val_accuracy: 0.7405\n",
      "Epoch 48/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.7663 - accuracy: 0.6975 - val_loss: 0.6859 - val_accuracy: 0.7317\n",
      "Epoch 49/100\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7592 - accuracy: 0.6981 - val_loss: 0.6908 - val_accuracy: 0.7400\n",
      "Epoch 50/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.7699 - accuracy: 0.6860 - val_loss: 0.7116 - val_accuracy: 0.7246\n",
      "Epoch 51/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.7505 - accuracy: 0.7018 - val_loss: 0.7068 - val_accuracy: 0.7116\n",
      "Epoch 52/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.7674 - accuracy: 0.6913 - val_loss: 0.6767 - val_accuracy: 0.7459\n",
      "Epoch 53/100\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.7660 - accuracy: 0.6961 - val_loss: 0.6976 - val_accuracy: 0.7299\n",
      "Epoch 54/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.7336 - accuracy: 0.7088 - val_loss: 0.6788 - val_accuracy: 0.7465\n",
      "Epoch 55/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.7507 - accuracy: 0.6908 - val_loss: 0.6937 - val_accuracy: 0.7305\n",
      "Epoch 56/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.7357 - accuracy: 0.7045 - val_loss: 0.6952 - val_accuracy: 0.7358\n",
      "Epoch 57/100\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7231 - accuracy: 0.7094 - val_loss: 0.6803 - val_accuracy: 0.7311\n",
      "Epoch 58/100\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.7284 - accuracy: 0.7091 - val_loss: 0.7062 - val_accuracy: 0.7193\n",
      "Epoch 59/100\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.7395 - accuracy: 0.7018 - val_loss: 0.6672 - val_accuracy: 0.7589\n",
      "Epoch 60/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.7349 - accuracy: 0.7035 - val_loss: 0.6936 - val_accuracy: 0.7240\n",
      "Epoch 61/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.7297 - accuracy: 0.7057 - val_loss: 0.6707 - val_accuracy: 0.7459\n",
      "Epoch 62/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.7291 - accuracy: 0.7097 - val_loss: 0.6638 - val_accuracy: 0.7423\n",
      "Epoch 63/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.7202 - accuracy: 0.7163 - val_loss: 0.6749 - val_accuracy: 0.7388\n",
      "Epoch 64/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.7342 - accuracy: 0.7072 - val_loss: 0.6745 - val_accuracy: 0.7518\n",
      "Epoch 65/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.7182 - accuracy: 0.7156 - val_loss: 0.6827 - val_accuracy: 0.7358\n",
      "Epoch 66/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.7212 - accuracy: 0.7148 - val_loss: 0.6853 - val_accuracy: 0.7199\n",
      "Epoch 67/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.7198 - accuracy: 0.7142 - val_loss: 0.6742 - val_accuracy: 0.7346\n",
      "Epoch 68/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.7162 - accuracy: 0.7138 - val_loss: 0.6548 - val_accuracy: 0.7571\n",
      "Epoch 69/100\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.7228 - accuracy: 0.7165 - val_loss: 0.6636 - val_accuracy: 0.7459\n",
      "Epoch 70/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.7147 - accuracy: 0.7134 - val_loss: 0.6850 - val_accuracy: 0.7323\n",
      "Epoch 71/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.7171 - accuracy: 0.7092 - val_loss: 0.6592 - val_accuracy: 0.7482\n",
      "Epoch 72/100\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.6991 - accuracy: 0.7250 - val_loss: 0.6701 - val_accuracy: 0.7317\n",
      "Epoch 73/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.7098 - accuracy: 0.7181 - val_loss: 0.6945 - val_accuracy: 0.7145\n",
      "Epoch 74/100\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.7123 - accuracy: 0.7221 - val_loss: 0.6807 - val_accuracy: 0.7358\n",
      "Epoch 75/100\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.7134 - accuracy: 0.7249 - val_loss: 0.6452 - val_accuracy: 0.7488\n",
      "Epoch 76/100\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.7028 - accuracy: 0.7250 - val_loss: 0.6765 - val_accuracy: 0.7411\n",
      "Epoch 77/100\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.7031 - accuracy: 0.7241 - val_loss: 0.6437 - val_accuracy: 0.7583\n",
      "Epoch 78/100\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.7064 - accuracy: 0.7218 - val_loss: 0.6595 - val_accuracy: 0.7476\n",
      "Epoch 79/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.7146 - accuracy: 0.7160 - val_loss: 0.6492 - val_accuracy: 0.7530\n",
      "Epoch 80/100\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.7105 - accuracy: 0.7148 - val_loss: 0.6723 - val_accuracy: 0.7346\n",
      "Epoch 81/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.6944 - accuracy: 0.7222 - val_loss: 0.6500 - val_accuracy: 0.7352\n",
      "Epoch 82/100\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.7054 - accuracy: 0.7225 - val_loss: 0.6525 - val_accuracy: 0.7470\n",
      "Epoch 83/100\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.6914 - accuracy: 0.7261 - val_loss: 0.6504 - val_accuracy: 0.7488\n",
      "Epoch 84/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.6904 - accuracy: 0.7302 - val_loss: 0.6712 - val_accuracy: 0.7370\n",
      "Epoch 85/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.6839 - accuracy: 0.7246 - val_loss: 0.6376 - val_accuracy: 0.7595\n",
      "Epoch 86/100\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.7041 - accuracy: 0.7162 - val_loss: 0.6718 - val_accuracy: 0.7453\n",
      "Epoch 87/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.6940 - accuracy: 0.7230 - val_loss: 0.6599 - val_accuracy: 0.7488\n",
      "Epoch 88/100\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.6948 - accuracy: 0.7256 - val_loss: 0.6458 - val_accuracy: 0.7636\n",
      "Epoch 89/100\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.6957 - accuracy: 0.7233 - val_loss: 0.6773 - val_accuracy: 0.7228\n",
      "Epoch 90/100\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.6958 - accuracy: 0.7249 - val_loss: 0.6271 - val_accuracy: 0.7606\n",
      "Epoch 91/100\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.6882 - accuracy: 0.7255 - val_loss: 0.6594 - val_accuracy: 0.7370\n",
      "Epoch 92/100\n",
      "106/106 [==============================] - 3s 33ms/step - loss: 0.6735 - accuracy: 0.7326 - val_loss: 0.6379 - val_accuracy: 0.7512\n",
      "Epoch 93/100\n",
      "106/106 [==============================] - 4s 35ms/step - loss: 0.6788 - accuracy: 0.7312 - val_loss: 0.6345 - val_accuracy: 0.7565\n",
      "Epoch 94/100\n",
      "106/106 [==============================] - 4s 34ms/step - loss: 0.6887 - accuracy: 0.7264 - val_loss: 0.6386 - val_accuracy: 0.7541\n",
      "Epoch 95/100\n",
      "106/106 [==============================] - 4s 35ms/step - loss: 0.6929 - accuracy: 0.7277 - val_loss: 0.6373 - val_accuracy: 0.7518\n",
      "Epoch 96/100\n",
      "106/106 [==============================] - 4s 36ms/step - loss: 0.6705 - accuracy: 0.7302 - val_loss: 0.6443 - val_accuracy: 0.7494\n",
      "Epoch 97/100\n",
      "106/106 [==============================] - 5s 50ms/step - loss: 0.6863 - accuracy: 0.7278 - val_loss: 0.6304 - val_accuracy: 0.7476\n",
      "Epoch 98/100\n",
      "106/106 [==============================] - 5s 46ms/step - loss: 0.6875 - accuracy: 0.7236 - val_loss: 0.6591 - val_accuracy: 0.7252\n",
      "Epoch 99/100\n",
      "106/106 [==============================] - 5s 48ms/step - loss: 0.6717 - accuracy: 0.7370 - val_loss: 0.6197 - val_accuracy: 0.7642\n",
      "Epoch 100/100\n",
      "106/106 [==============================] - 4s 40ms/step - loss: 0.6834 - accuracy: 0.7364 - val_loss: 0.6519 - val_accuracy: 0.7299\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 0.7298 - accuracy: 0.7049\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "for i, trim_ratio in enumerate(np.arange(0, 1.1, step=.1)):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    cnn, cnn_training_results, cnn_test_score = cnn_model(trim_ratio=trim_ratio)\n",
    "    accuracies.append(cnn_test_score)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeper exploration and analysis into other architectures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: But with CRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    X_test = np.load(\"X_test.npy\")\n",
    "    y_test = np.load(\"y_test.npy\")\n",
    "    person_train_valid = np.load(\"person_train_valid.npy\")\n",
    "    X_train_valid = np.load(\"X_train_valid.npy\")\n",
    "    y_train_valid = np.load(\"y_train_valid.npy\")\n",
    "    person_test = np.load(\"person_test.npy\")\n",
    "\n",
    "    ## Adjusting the labels so that \n",
    "\n",
    "    # Cue onset left - 0\n",
    "    # Cue onset right - 1\n",
    "    # Cue onset foot - 2\n",
    "    # Cue onset tongue - 3\n",
    "\n",
    "    y_train_valid -= 769\n",
    "    y_test -= 769\n",
    "    \n",
    "\n",
    "    # print(f'X_test Shape for Subject {subject}: {subject_X_test.shape}')\n",
    "    # print(f'y_test Shape for Subject {subject}: {suject_y_test.shape}')\n",
    "    # print(f'X_train_valid Shape for Subject {subject}: {suject_X_train_valid.shape}')\n",
    "    # print(f'y_train_valid Shape for Subject {subject}: {suject_y_train_valid.shape}')\n",
    "\n",
    "    # shuffle with 5 fold\n",
    "    indicies_valid = np.random.choice(X_train_valid.shape[0], X_train_valid.shape[0] // 5, replace=False)\n",
    "    indicies_train = np.array(list(set(range(X_train_valid.shape[0])).difference(set(indicies_valid))))\n",
    "\n",
    "    # Creating the training and validation sets using the generated indices\n",
    "    X_train, X_valid = X_train_valid[indicies_train], X_train_valid[indicies_valid] \n",
    "    y_train, y_valid = y_train_valid[indicies_train], y_train_valid[indicies_valid]\n",
    "\n",
    "\n",
    "    # Preprocessing the dataset\n",
    "    x_train,y_train = data_prep(X_train,y_train,2,2,True)\n",
    "    x_valid,y_valid = data_prep(X_valid,y_valid,2,2,True)\n",
    "    X_test_prep,y_test_prep = data_prep(X_test,y_test,2,2,True)\n",
    "\n",
    "\n",
    "\n",
    "    # print('Shape of training set:',x_train.shape)\n",
    "    # print('Shape of validation set:',x_valid.shape)\n",
    "    # print('Shape of training labels:',y_train.shape)\n",
    "    # print('Shape of validation labels:',y_valid.shape)\n",
    "    # print('Shape of testing set:',X_test_prep.shape)\n",
    "    # print('Shape of testing labels:',y_test_prep.shape)\n",
    "\n",
    "\n",
    "    # Converting the labels to categorical variables for multiclass classification\n",
    "    y_train = to_categorical(y_train, 4)\n",
    "    y_valid = to_categorical(y_valid, 4)\n",
    "    y_test = to_categorical(y_test_prep, 4)\n",
    "    # print('Shape of training labels after categorical conversion:',y_train.shape)\n",
    "    # print('Shape of validation labels after categorical conversion:',y_valid.shape)\n",
    "    # print('Shape of test labels after categorical conversion:',y_test.shape)\n",
    "\n",
    "    # Adding width of the segment to be 1\n",
    "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "    x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)\n",
    "    x_test = X_test_prep.reshape(X_test_prep.shape[0], X_test_prep.shape[1], X_test_prep.shape[2], 1)\n",
    "    # print('Shape of training set after adding width info:',x_train.shape)\n",
    "    # print('Shape of validation set after adding width info:',x_valid.shape)\n",
    "    # print('Shape of test set after adding width info:',x_test.shape)\n",
    "\n",
    "\n",
    "    # Reshaping the training and validation dataset\n",
    "    x_train = np.swapaxes(x_train, 1,3)\n",
    "    x_train = np.swapaxes(x_train, 1,2)\n",
    "    x_valid = np.swapaxes(x_valid, 1,3)\n",
    "    x_valid = np.swapaxes(x_valid, 1,2)\n",
    "    x_test = np.swapaxes(x_test, 1,3)\n",
    "    x_test = np.swapaxes(x_test, 1,2)\n",
    "    # print('Shape of training set after dimension reshaping:',x_train.shape)\n",
    "    # print('Shape of validation set after dimension reshaping:',x_valid.shape)\n",
    "    # print('Shape of test set after dimension reshaping:',x_test.shape)\n",
    "\n",
    "    return (x_train, y_train, x_valid, y_valid, x_test, y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_lstm_model():    \n",
    "    # Building the CNN model using sequential class\n",
    "    cnn_lstm_model = Sequential()\n",
    "\n",
    "    # Conv. block 1\n",
    "    cnn_lstm_model.add(Conv2D(filters=20, kernel_size=(5,1), padding='same', activation='elu', input_shape=(250,1,22)))\n",
    "    cnn_lstm_model.add(MaxPooling2D(pool_size=(3,1), padding='same')) \n",
    "    cnn_lstm_model.add(BatchNormalization())\n",
    "    cnn_lstm_model.add(Dropout(0.5))\n",
    "\n",
    "    # Conv. block 2\n",
    "    cnn_lstm_model.add(Conv2D(filters=20, kernel_size=(15,1), padding='same', activation='elu'))\n",
    "    cnn_lstm_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "    cnn_lstm_model.add(BatchNormalization())\n",
    "    cnn_lstm_model.add(Dropout(0.5))\n",
    "\n",
    "    # Conv. block 3\n",
    "    cnn_lstm_model.add(Conv2D(filters=10, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "    cnn_lstm_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "    cnn_lstm_model.add(BatchNormalization())\n",
    "    cnn_lstm_model.add(Dropout(0.5))\n",
    "\n",
    "    # Add LSTM layers\n",
    "    cnn_lstm_model.add(Permute((2, 3, 1)))\n",
    "    cnn_lstm_model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "    cnn_lstm_model.add(LSTM(250, return_sequences=True))\n",
    "    cnn_lstm_model.add(Dropout(0.5))\n",
    "    cnn_lstm_model.add(LSTM(100, return_sequences=True))\n",
    "    cnn_lstm_model.add(Dropout(0.5))\n",
    "    cnn_lstm_model.add(LSTM(50))\n",
    "\n",
    "    # Output layer with Softmax activation\n",
    "    cnn_lstm_model.add(Flatten()) # Flattens the input\n",
    "    cnn_lstm_model.add(Dense(4, activation='softmax')) # Output FC layer with softmax activation\n",
    "\n",
    "\n",
    "    return cnn_lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model():    \n",
    "    # Building the CNN model using sequential class\n",
    "    basic_cnn = Sequential()\n",
    "\n",
    "    # Conv. block 1\n",
    "    basic_cnn.add(Conv2D(filters=10, kernel_size=(5,1), padding='same', activation='elu', input_shape=(250,1,22)))\n",
    "    basic_cnn.add(MaxPooling2D(pool_size=(3,1), padding='same')) \n",
    "    basic_cnn.add(BatchNormalization())\n",
    "    basic_cnn.add(Dropout(0.5))\n",
    "\n",
    "    # Conv. block 2\n",
    "    basic_cnn.add(Conv2D(filters=10, kernel_size=(15,1), padding='same', activation='elu'))\n",
    "    basic_cnn.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "    basic_cnn.add(BatchNormalization())\n",
    "    basic_cnn.add(Dropout(0.5))\n",
    "\n",
    "    # # Conv. block 2\n",
    "    # basic_cnn.add(Conv2D(filters=10, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "    # basic_cnn.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "    # basic_cnn.add(BatchNormalization())\n",
    "    # basic_cnn.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "    # Output layer with Softmax activation\n",
    "    basic_cnn.add(Flatten()) # Flattens the input\n",
    "    basic_cnn.add(Dense(4, activation='softmax')) # Output FC layer with softmax activation\n",
    "\n",
    "\n",
    "    return basic_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_generator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64 * 125 * 1, use_bias=False, input_shape=(100,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Reshape((125, 1, 64)))\n",
    "    assert model.output_shape == (None, 125, 1, 64)  # Note: None is the batch size\n",
    "\n",
    "    model.add(Conv2DTranspose(32, (3, 3), strides=(2, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 250, 1, 32)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Conv2DTranspose(16, (3, 3), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 250, 1, 16)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "    model.add(Conv2DTranspose(1, (3, 3), strides=(1, 1), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 250, 1, 1)\n",
    "\n",
    "    model.add(Reshape((250, 1, 1)))\n",
    "    assert model.output_shape == (None, 250, 1, 1)\n",
    "\n",
    "    model.add(Conv2DTranspose(22, (3, 3), strides=(1, 1), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 250, 1, 22)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_discriminator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (3, 3), strides=(1, 1), padding='same',\n",
    "                                     input_shape=[250, 1, 22]))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), strides=(2, 1), padding='same'))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), strides=(2, 1), padding='same'))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    return model\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "noise_dim = 100\n",
    "num_classes = 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the generator and discriminator models\n",
    "generators = [make_generator_model() for _ in range(4)]\n",
    "discriminators = [make_discriminator_model() for _ in range(4)]\n",
    "\n",
    "# Define the optimizer for the generator and discriminator\n",
    "generator_optimizer = keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = keras.optimizers.Adam(1e-4)\n",
    "\n",
    "def train_step(images, class_label):\n",
    "    noise = tf.random.normal([batch_size, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generators[class_label](noise, training=True)\n",
    "\n",
    "\n",
    "        real_output = discriminators[class_label](images, training=True)\n",
    "        fake_output = discriminators[class_label](generated_images, training=True)\n",
    "\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generators[class_label].trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminators[class_label].trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generators[class_label].trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminators[class_label].trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(dataset, labels, epochs):\n",
    "    dataset_0 = dataset[np.where(labels[:,0] == 1)[0]]\n",
    "    dataset_1 = dataset[np.where(labels[:,1] == 1)[0]]\n",
    "    dataset_2 = dataset[np.where(labels[:,2] == 1)[0]]\n",
    "    dataset_3 = dataset[np.where(labels[:,3] == 1)[0]]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch: {epoch}, Label: 0')\n",
    "        train_step(dataset_0,0)\n",
    "        print(f'Epoch: {epoch}, Label: 1')\n",
    "        train_step(dataset_1,1)\n",
    "        print(f'Epoch: {epoch}, Label: 2')\n",
    "        train_step(dataset_2,2)\n",
    "        print(f'Epoch: {epoch}, Label: 3')\n",
    "        train_step(dataset_3,3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep_gan(X,y,sub_sample,average,noise, generators, trim_ratio=0.5):\n",
    "    \n",
    "    total_X = None\n",
    "    total_y = None\n",
    "    \n",
    "    # Trimming the data (sample,22,1000) -> (sample,22,500)\n",
    "    X = X[:,:, 0:(int(X.shape[2] * trim_ratio))]\n",
    "    print('Shape of X after trimming:',X.shape)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Maxpooling the data (sample,22,1000) -> (sample,22,500/sub_sample)\n",
    "    X_max = np.max(X.reshape(X.shape[0], X.shape[1], -1, sub_sample), axis=3)\n",
    "    \n",
    "    \n",
    "    total_X = X_max\n",
    "    total_y = y\n",
    "    # print('Shape of X after maxpooling:',total_X.shape)\n",
    "    \n",
    "    # Averaging + noise \n",
    "    X_average = np.mean(X.reshape(X.shape[0], X.shape[1], -1, average), axis=3)\n",
    "    X_average = X_average + np.random.normal(0.0, 0.5, X_average.shape)\n",
    "    \n",
    "    total_X = np.vstack((total_X, X_average))\n",
    "    total_y = np.hstack((total_y, y))\n",
    "    # print('Shape of X after averaging+noise and concatenating:',total_X.shape)\n",
    "    \n",
    "    # Subsampling\n",
    "    \n",
    "    for i in range(sub_sample):\n",
    "        \n",
    "        X_subsample = X[:, :, i::sub_sample] + (np.random.normal(0.0, 0.5, X[:, :,i::sub_sample].shape) if noise else 0.0)\n",
    "        total_X = np.vstack((total_X, X_subsample))\n",
    "        total_y = np.hstack((total_y, y))\n",
    "        \n",
    "    \n",
    "    # GAN\n",
    "    # get generated samples from conditional gan\n",
    "    trimmed_off_data = 1000 - int(X.shape[2] * trim_ratio)\n",
    "    \n",
    "    noise = tf.random.normal([batch_size, noise_dim])\n",
    "\n",
    "    generated_eeg = generators[0](noise, training=False)\n",
    "    generated_samples = generated_eeg.shape[0]\n",
    "    generated_eeg = np.swapaxes(generated_eeg, 2, 3).reshape(-1, 22, 250)\n",
    "    total_X = np.vstack((total_X, generated_eeg))\n",
    "    total_y = np.hstack((total_y, np.full(shape=(generated_samples,), fill_value=0)))\n",
    "\n",
    "    generated_eeg = generators[1](noise, training=False)\n",
    "    generated_samples = generated_eeg.shape[0]\n",
    "    generated_eeg = np.swapaxes(generated_eeg, 2, 3).reshape(-1, 22, 250)\n",
    "    total_X = np.vstack((total_X, generated_eeg))\n",
    "    total_y = np.hstack((total_y, np.full(shape=(generated_samples,), fill_value=1)))\n",
    "\n",
    "    generated_eeg = generators[2](noise, training=False)\n",
    "    generated_samples = generated_eeg.shape[0]\n",
    "    generated_eeg = np.swapaxes(generated_eeg, 2, 3).reshape(-1, 22, 250)\n",
    "    total_X = np.vstack((total_X, generated_eeg))\n",
    "    total_y = np.hstack((total_y, np.full(shape=(generated_samples,), fill_value=2)))\n",
    "\n",
    "    generated_eeg = generators[3](noise, training=False)\n",
    "    generated_samples = generated_eeg.shape[0]\n",
    "    generated_eeg = np.swapaxes(generated_eeg, 2, 3).reshape(-1, 22, 250)\n",
    "    total_X = np.vstack((total_X, generated_eeg))\n",
    "    total_y = np.hstack((total_y, np.full(shape=(generated_samples,), fill_value=3)))\n",
    "\n",
    "\n",
    "    \n",
    "    print('Shape of X after GAN:',total_X.shape)\n",
    "    \n",
    "    return total_X,total_y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_gan():\n",
    "    X_test = np.load(\"X_test.npy\")\n",
    "    y_test = np.load(\"y_test.npy\")\n",
    "    person_train_valid = np.load(\"person_train_valid.npy\")\n",
    "    X_train_valid = np.load(\"X_train_valid.npy\")\n",
    "    y_train_valid = np.load(\"y_train_valid.npy\")\n",
    "    person_test = np.load(\"person_test.npy\")\n",
    "\n",
    "    ## Adjusting the labels so that \n",
    "\n",
    "    # Cue onset left - 0\n",
    "    # Cue onset right - 1\n",
    "    # Cue onset foot - 2\n",
    "    # Cue onset tongue - 3\n",
    "\n",
    "    y_train_valid -= 769\n",
    "    y_test -= 769\n",
    "    \n",
    "\n",
    "    # print(f'X_test Shape for Subject {subject}: {subject_X_test.shape}')\n",
    "    # print(f'y_test Shape for Subject {subject}: {suject_y_test.shape}')\n",
    "    # print(f'X_train_valid Shape for Subject {subject}: {suject_X_train_valid.shape}')\n",
    "    # print(f'y_train_valid Shape for Subject {subject}: {suject_y_train_valid.shape}')\n",
    "\n",
    "    # shuffle with 5 fold\n",
    "    indicies_valid = np.random.choice(X_train_valid.shape[0], X_train_valid.shape[0] // 5, replace=False)\n",
    "    indicies_train = np.array(list(set(range(X_train_valid.shape[0])).difference(set(indicies_valid))))\n",
    "\n",
    "    # Creating the training and validation sets using the generated indices\n",
    "    X_train, X_valid = X_train_valid[indicies_train], X_train_valid[indicies_valid] \n",
    "    y_train, y_valid = y_train_valid[indicies_train], y_train_valid[indicies_valid]\n",
    "\n",
    "\n",
    "    # Preprocessing the dataset\n",
    "    x_train,y_train = data_prep_gan(X_train,y_train,2,2,True, generators)\n",
    "    x_valid,y_valid = data_prep_gan(X_valid,y_valid,2,2,True, generators)\n",
    "    X_test_prep,y_test_prep = data_prep_gan(X_test,y_test,2,2,True, generators)\n",
    "\n",
    "\n",
    "\n",
    "    # print('Shape of training set:',x_train.shape)\n",
    "    # print('Shape of validation set:',x_valid.shape)\n",
    "    # print('Shape of training labels:',y_train.shape)\n",
    "    # print('Shape of validation labels:',y_valid.shape)\n",
    "    # print('Shape of testing set:',X_test_prep.shape)\n",
    "    # print('Shape of testing labels:',y_test_prep.shape)\n",
    "\n",
    "\n",
    "    # Converting the labels to categorical variables for multiclass classification\n",
    "    y_train = to_categorical(y_train, 4)\n",
    "    y_valid = to_categorical(y_valid, 4)\n",
    "    y_test = to_categorical(y_test_prep, 4)\n",
    "    # print('Shape of training labels after categorical conversion:',y_train.shape)\n",
    "    # print('Shape of validation labels after categorical conversion:',y_valid.shape)\n",
    "    # print('Shape of test labels after categorical conversion:',y_test.shape)\n",
    "\n",
    "    # Adding width of the segment to be 1\n",
    "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "    x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)\n",
    "    x_test = X_test_prep.reshape(X_test_prep.shape[0], X_test_prep.shape[1], X_test_prep.shape[2], 1)\n",
    "    # print('Shape of training set after adding width info:',x_train.shape)\n",
    "    # print('Shape of validation set after adding width info:',x_valid.shape)\n",
    "    # print('Shape of test set after adding width info:',x_test.shape)\n",
    "\n",
    "\n",
    "    # Reshaping the training and validation dataset\n",
    "    x_train = np.swapaxes(x_train, 1,3)\n",
    "    x_train = np.swapaxes(x_train, 1,2)\n",
    "    x_valid = np.swapaxes(x_valid, 1,3)\n",
    "    x_valid = np.swapaxes(x_valid, 1,2)\n",
    "    x_test = np.swapaxes(x_test, 1,3)\n",
    "    x_test = np.swapaxes(x_test, 1,2)\n",
    "    # print('Shape of training set after dimension reshaping:',x_train.shape)\n",
    "    # print('Shape of validation set after dimension reshaping:',x_valid.shape)\n",
    "    # print('Shape of test set after dimension reshaping:',x_test.shape)\n",
    "\n",
    "    return (x_train, y_train, x_valid, y_valid, x_test, y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X after trimming: (1692, 22, 500)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x_train, y_train, x_valid, y_valid, x_test, y_test \u001b[39m=\u001b[39m preprocess()\n\u001b[1;32m      2\u001b[0m train_gan(x_train, y_train, epochs)\n",
      "Cell \u001b[0;32mIn[104], line 35\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m y_train, y_valid \u001b[39m=\u001b[39m y_train_valid[indicies_train], y_train_valid[indicies_valid]\n\u001b[1;32m     34\u001b[0m \u001b[39m# Preprocessing the dataset\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m x_train,y_train \u001b[39m=\u001b[39m data_prep_gan(X_train,y_train,\u001b[39m2\u001b[39;49m,\u001b[39m2\u001b[39;49m,\u001b[39mTrue\u001b[39;49;00m, generators)\n\u001b[1;32m     36\u001b[0m x_valid,y_valid \u001b[39m=\u001b[39m data_prep_gan(X_valid,y_valid,\u001b[39m2\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39mTrue\u001b[39;00m, generators)\n\u001b[1;32m     37\u001b[0m X_test_prep,y_test_prep \u001b[39m=\u001b[39m data_prep_gan(X_test,y_test,\u001b[39m2\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39mTrue\u001b[39;00m, generators)\n",
      "Cell \u001b[0;32mIn[114], line 32\u001b[0m, in \u001b[0;36mdata_prep_gan\u001b[0;34m(X, y, sub_sample, average, noise, generators, trim_ratio)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39m# print('Shape of X after averaging+noise and concatenating:',total_X.shape)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[39m# Subsampling\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(sub_sample):\n\u001b[0;32m---> 32\u001b[0m     X_subsample \u001b[39m=\u001b[39m X[:, :, i::sub_sample] \u001b[39m+\u001b[39m (np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mnormal(\u001b[39m0.0\u001b[39;49m, \u001b[39m0.5\u001b[39;49m, X[:, :,i::sub_sample]\u001b[39m.\u001b[39;49mshape) \u001b[39mif\u001b[39;00m noise \u001b[39melse\u001b[39;00m \u001b[39m0.0\u001b[39m)\n\u001b[1;32m     33\u001b[0m     total_X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack((total_X, X_subsample))\n\u001b[1;32m     34\u001b[0m     total_y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mhstack((total_y, y))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_valid, y_valid, x_test, y_test = preprocess()\n",
    "train_gan(x_train, y_train, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X after trimming: (1692, 22, 500)\n",
      "Shape of X after GAN: (7024, 22, 250)\n",
      "Shape of X after trimming: (423, 22, 500)\n",
      "Shape of X after GAN: (1948, 22, 250)\n",
      "Shape of X after trimming: (443, 22, 500)\n",
      "Shape of X after GAN: (2028, 22, 250)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_valid, y_valid, x_test, y_test = preprocess_gan()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X after trimming: (1692, 22, 500)\n",
      "Shape of X after GAN: (7024, 22, 250)\n",
      "Shape of X after trimming: (423, 22, 500)\n",
      "Shape of X after GAN: (1948, 22, 250)\n",
      "Shape of X after trimming: (443, 22, 500)\n",
      "Shape of X after GAN: (2028, 22, 250)\n",
      "Model: \"sequential_91\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_141 (Conv2D)         (None, 250, 1, 10)        1110      \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 84, 1, 10)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_138 (Ba  (None, 84, 1, 10)        40        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_143 (Dropout)       (None, 84, 1, 10)         0         \n",
      "                                                                 \n",
      " conv2d_142 (Conv2D)         (None, 84, 1, 10)         1510      \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPoolin  (None, 28, 1, 10)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_139 (Ba  (None, 28, 1, 10)        40        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_144 (Dropout)       (None, 28, 1, 10)         0         \n",
      "                                                                 \n",
      " flatten_49 (Flatten)        (None, 280)               0         \n",
      "                                                                 \n",
      " dense_91 (Dense)            (None, 4)                 1124      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,824\n",
      "Trainable params: 3,784\n",
      "Non-trainable params: 40\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 1.5127 - accuracy: 0.3690 - val_loss: 1.2761 - val_accuracy: 0.4168\n",
      "Epoch 2/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.2278 - accuracy: 0.4465 - val_loss: 1.2433 - val_accuracy: 0.4174\n",
      "Epoch 3/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.1559 - accuracy: 0.4895 - val_loss: 1.1849 - val_accuracy: 0.4646\n",
      "Epoch 4/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.1180 - accuracy: 0.5188 - val_loss: 1.1595 - val_accuracy: 0.4754\n",
      "Epoch 5/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.0755 - accuracy: 0.5429 - val_loss: 1.1491 - val_accuracy: 0.4959\n",
      "Epoch 6/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.0483 - accuracy: 0.5551 - val_loss: 1.1185 - val_accuracy: 0.5169\n",
      "Epoch 7/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 1.0233 - accuracy: 0.5730 - val_loss: 1.0861 - val_accuracy: 0.5390\n",
      "Epoch 8/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9971 - accuracy: 0.5913 - val_loss: 1.0620 - val_accuracy: 0.5539\n",
      "Epoch 9/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.9978 - accuracy: 0.5903 - val_loss: 1.0666 - val_accuracy: 0.5488\n",
      "Epoch 10/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9612 - accuracy: 0.5995 - val_loss: 1.0345 - val_accuracy: 0.5637\n",
      "Epoch 11/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9566 - accuracy: 0.6032 - val_loss: 1.0076 - val_accuracy: 0.5708\n",
      "Epoch 12/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.9439 - accuracy: 0.6180 - val_loss: 1.0211 - val_accuracy: 0.5785\n",
      "Epoch 13/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.9277 - accuracy: 0.6143 - val_loss: 1.0298 - val_accuracy: 0.5652\n",
      "Epoch 14/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9332 - accuracy: 0.6113 - val_loss: 0.9921 - val_accuracy: 0.5821\n",
      "Epoch 15/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9154 - accuracy: 0.6254 - val_loss: 0.9972 - val_accuracy: 0.5919\n",
      "Epoch 16/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9026 - accuracy: 0.6230 - val_loss: 0.9905 - val_accuracy: 0.5719\n",
      "Epoch 17/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8962 - accuracy: 0.6367 - val_loss: 0.9892 - val_accuracy: 0.5965\n",
      "Epoch 18/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8886 - accuracy: 0.6315 - val_loss: 0.9720 - val_accuracy: 0.5862\n",
      "Epoch 19/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8728 - accuracy: 0.6438 - val_loss: 0.9658 - val_accuracy: 0.5893\n",
      "Epoch 20/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8613 - accuracy: 0.6492 - val_loss: 0.9599 - val_accuracy: 0.6088\n",
      "Epoch 21/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.8656 - accuracy: 0.6449 - val_loss: 0.9412 - val_accuracy: 0.6068\n",
      "Epoch 22/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.8643 - accuracy: 0.6513 - val_loss: 0.9372 - val_accuracy: 0.6160\n",
      "Epoch 23/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8742 - accuracy: 0.6415 - val_loss: 0.9602 - val_accuracy: 0.6063\n",
      "Epoch 24/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8530 - accuracy: 0.6503 - val_loss: 0.9440 - val_accuracy: 0.6027\n",
      "Epoch 25/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8469 - accuracy: 0.6610 - val_loss: 0.9281 - val_accuracy: 0.6247\n",
      "Epoch 26/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.8436 - accuracy: 0.6587 - val_loss: 0.9472 - val_accuracy: 0.6057\n",
      "Epoch 27/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8401 - accuracy: 0.6604 - val_loss: 0.9324 - val_accuracy: 0.6227\n",
      "Epoch 28/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.8304 - accuracy: 0.6683 - val_loss: 0.9464 - val_accuracy: 0.5996\n",
      "Epoch 29/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8216 - accuracy: 0.6659 - val_loss: 0.9431 - val_accuracy: 0.6104\n",
      "Epoch 30/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.8287 - accuracy: 0.6663 - val_loss: 0.9229 - val_accuracy: 0.6109\n",
      "Epoch 31/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8254 - accuracy: 0.6679 - val_loss: 0.9409 - val_accuracy: 0.5980\n",
      "Epoch 32/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8365 - accuracy: 0.6563 - val_loss: 0.9271 - val_accuracy: 0.6083\n",
      "Epoch 33/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8291 - accuracy: 0.6569 - val_loss: 0.9477 - val_accuracy: 0.5970\n",
      "Epoch 34/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8291 - accuracy: 0.6676 - val_loss: 0.9210 - val_accuracy: 0.6196\n",
      "Epoch 35/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8087 - accuracy: 0.6753 - val_loss: 0.9211 - val_accuracy: 0.6078\n",
      "Epoch 36/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8229 - accuracy: 0.6723 - val_loss: 0.9256 - val_accuracy: 0.6150\n",
      "Epoch 37/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8246 - accuracy: 0.6660 - val_loss: 0.9186 - val_accuracy: 0.6258\n",
      "Epoch 38/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8091 - accuracy: 0.6703 - val_loss: 0.9266 - val_accuracy: 0.6104\n",
      "Epoch 39/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8124 - accuracy: 0.6684 - val_loss: 0.9077 - val_accuracy: 0.6319\n",
      "Epoch 40/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8121 - accuracy: 0.6670 - val_loss: 0.9198 - val_accuracy: 0.6093\n",
      "Epoch 41/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7937 - accuracy: 0.6811 - val_loss: 0.9174 - val_accuracy: 0.6170\n",
      "Epoch 42/100\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8050 - accuracy: 0.6808 - val_loss: 0.9179 - val_accuracy: 0.6232\n",
      "Epoch 43/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8091 - accuracy: 0.6730 - val_loss: 0.9171 - val_accuracy: 0.6181\n",
      "Epoch 44/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8108 - accuracy: 0.6711 - val_loss: 0.9068 - val_accuracy: 0.6299\n",
      "Epoch 45/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7960 - accuracy: 0.6791 - val_loss: 0.9140 - val_accuracy: 0.6299\n",
      "Epoch 46/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7811 - accuracy: 0.6838 - val_loss: 0.8992 - val_accuracy: 0.6355\n",
      "Epoch 47/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7869 - accuracy: 0.6849 - val_loss: 0.8942 - val_accuracy: 0.6366\n",
      "Epoch 48/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.8019 - accuracy: 0.6787 - val_loss: 0.9155 - val_accuracy: 0.6186\n",
      "Epoch 49/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7807 - accuracy: 0.6854 - val_loss: 0.8964 - val_accuracy: 0.6350\n",
      "Epoch 50/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7875 - accuracy: 0.6802 - val_loss: 0.8900 - val_accuracy: 0.6160\n",
      "Epoch 51/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7945 - accuracy: 0.6805 - val_loss: 0.9009 - val_accuracy: 0.6340\n",
      "Epoch 52/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.7885 - accuracy: 0.6827 - val_loss: 0.9208 - val_accuracy: 0.6289\n",
      "Epoch 53/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.8070 - accuracy: 0.6708 - val_loss: 0.8963 - val_accuracy: 0.6458\n",
      "Epoch 54/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.7836 - accuracy: 0.6872 - val_loss: 0.8903 - val_accuracy: 0.6273\n",
      "Epoch 55/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7748 - accuracy: 0.6894 - val_loss: 0.9143 - val_accuracy: 0.6330\n",
      "Epoch 56/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.7774 - accuracy: 0.6839 - val_loss: 0.8956 - val_accuracy: 0.6242\n",
      "Epoch 57/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7712 - accuracy: 0.6862 - val_loss: 0.8947 - val_accuracy: 0.6227\n",
      "Epoch 58/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7753 - accuracy: 0.6913 - val_loss: 0.8936 - val_accuracy: 0.6283\n",
      "Epoch 59/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7757 - accuracy: 0.6902 - val_loss: 0.9155 - val_accuracy: 0.6237\n",
      "Epoch 60/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7696 - accuracy: 0.6885 - val_loss: 0.8901 - val_accuracy: 0.6371\n",
      "Epoch 61/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7859 - accuracy: 0.6775 - val_loss: 0.9035 - val_accuracy: 0.6324\n",
      "Epoch 62/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7757 - accuracy: 0.6862 - val_loss: 0.8917 - val_accuracy: 0.6294\n",
      "Epoch 63/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7852 - accuracy: 0.6834 - val_loss: 0.8949 - val_accuracy: 0.6381\n",
      "Epoch 64/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.7768 - accuracy: 0.6845 - val_loss: 0.8883 - val_accuracy: 0.6314\n",
      "Epoch 65/100\n",
      "110/110 [==============================] - 1s 10ms/step - loss: 0.7657 - accuracy: 0.6898 - val_loss: 0.8881 - val_accuracy: 0.6324\n",
      "Epoch 66/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.7794 - accuracy: 0.6926 - val_loss: 0.9191 - val_accuracy: 0.6165\n",
      "Epoch 67/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7781 - accuracy: 0.6881 - val_loss: 0.8871 - val_accuracy: 0.6314\n",
      "Epoch 68/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7804 - accuracy: 0.6852 - val_loss: 0.8845 - val_accuracy: 0.6484\n",
      "Epoch 69/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7740 - accuracy: 0.6855 - val_loss: 0.8900 - val_accuracy: 0.6319\n",
      "Epoch 70/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7569 - accuracy: 0.7027 - val_loss: 0.8780 - val_accuracy: 0.6330\n",
      "Epoch 71/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7693 - accuracy: 0.6906 - val_loss: 0.8862 - val_accuracy: 0.6432\n",
      "Epoch 72/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7748 - accuracy: 0.6832 - val_loss: 0.8851 - val_accuracy: 0.6407\n",
      "Epoch 73/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7557 - accuracy: 0.6975 - val_loss: 0.8727 - val_accuracy: 0.6396\n",
      "Epoch 74/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.7746 - accuracy: 0.6844 - val_loss: 0.8803 - val_accuracy: 0.6407\n",
      "Epoch 75/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7630 - accuracy: 0.6945 - val_loss: 0.8865 - val_accuracy: 0.6299\n",
      "Epoch 76/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.7593 - accuracy: 0.6958 - val_loss: 0.8870 - val_accuracy: 0.6340\n",
      "Epoch 77/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7653 - accuracy: 0.6908 - val_loss: 0.8953 - val_accuracy: 0.6314\n",
      "Epoch 78/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7652 - accuracy: 0.6894 - val_loss: 0.8668 - val_accuracy: 0.6422\n",
      "Epoch 79/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7679 - accuracy: 0.6936 - val_loss: 0.9079 - val_accuracy: 0.6140\n",
      "Epoch 80/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7754 - accuracy: 0.6892 - val_loss: 0.8931 - val_accuracy: 0.6299\n",
      "Epoch 81/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7653 - accuracy: 0.6892 - val_loss: 0.8885 - val_accuracy: 0.6360\n",
      "Epoch 82/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.7645 - accuracy: 0.6922 - val_loss: 0.8794 - val_accuracy: 0.6417\n",
      "Epoch 83/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7555 - accuracy: 0.6925 - val_loss: 0.8740 - val_accuracy: 0.6412\n",
      "Epoch 84/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7720 - accuracy: 0.6802 - val_loss: 0.8844 - val_accuracy: 0.6371\n",
      "Epoch 85/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7624 - accuracy: 0.6915 - val_loss: 0.8889 - val_accuracy: 0.6314\n",
      "Epoch 86/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7496 - accuracy: 0.6938 - val_loss: 0.8805 - val_accuracy: 0.6371\n",
      "Epoch 87/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7603 - accuracy: 0.7003 - val_loss: 0.8748 - val_accuracy: 0.6407\n",
      "Epoch 88/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7616 - accuracy: 0.6915 - val_loss: 0.8918 - val_accuracy: 0.6360\n",
      "Epoch 89/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7501 - accuracy: 0.6942 - val_loss: 0.8797 - val_accuracy: 0.6432\n",
      "Epoch 90/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7612 - accuracy: 0.6986 - val_loss: 0.8898 - val_accuracy: 0.6360\n",
      "Epoch 91/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.7559 - accuracy: 0.6959 - val_loss: 0.8676 - val_accuracy: 0.6473\n",
      "Epoch 92/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7419 - accuracy: 0.7024 - val_loss: 0.8902 - val_accuracy: 0.6330\n",
      "Epoch 93/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7544 - accuracy: 0.6952 - val_loss: 0.8896 - val_accuracy: 0.6437\n",
      "Epoch 94/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7669 - accuracy: 0.6889 - val_loss: 0.8808 - val_accuracy: 0.6330\n",
      "Epoch 95/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7652 - accuracy: 0.6856 - val_loss: 0.8801 - val_accuracy: 0.6304\n",
      "Epoch 96/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7558 - accuracy: 0.6921 - val_loss: 0.8875 - val_accuracy: 0.6324\n",
      "Epoch 97/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7482 - accuracy: 0.6982 - val_loss: 0.8821 - val_accuracy: 0.6366\n",
      "Epoch 98/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7610 - accuracy: 0.6996 - val_loss: 0.8726 - val_accuracy: 0.6376\n",
      "Epoch 99/100\n",
      "110/110 [==============================] - 1s 9ms/step - loss: 0.7624 - accuracy: 0.6935 - val_loss: 0.8791 - val_accuracy: 0.6350\n",
      "Epoch 100/100\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.7382 - accuracy: 0.7074 - val_loss: 0.8686 - val_accuracy: 0.6381\n"
     ]
    }
   ],
   "source": [
    "# Printing the model summary\n",
    "x_train, y_train, x_valid, y_valid, x_test, y_test = preprocess_gan()\n",
    "model = cnn_model()\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model_results = model.fit(x_train,\n",
    "            y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(x_valid, y_valid), verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 2ms/step - loss: 0.8447 - accuracy: 0.6400\n",
      "Test accuracy of the GAN-CNN model: 0.6400394439697266\n"
     ]
    }
   ],
   "source": [
    "model_name = 'GAN-CNN'\n",
    "model_score = model.evaluate(x_test, y_test, verbose=True)\n",
    "print(f'Test accuracy of the {model_name} model:',model_score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee147",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
