{"cells":[{"cell_type":"markdown","metadata":{"id":"j9trg6idGKRl"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"xMisdpDcGKRp"},"outputs":[],"source":["import tensorflow as tf                                \n","from tensorflow import keras             \n","import numpy as np                       \n","from sklearn.model_selection import train_test_split   \n","import numpy as np\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Flatten,Dropout\n","from keras.layers import Conv2D,BatchNormalization,MaxPooling2D,Reshape, LSTM, ConvLSTM2D, Permute, TimeDistributed\n","from keras.utils import to_categorical\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.layers import Dense, Reshape, Conv2DTranspose, LeakyReLU, BatchNormalization, Input, concatenate\n","from tensorflow.keras.models import Model\n","import os"]},{"cell_type":"markdown","metadata":{"id":"bGa7AcyoGKR3"},"source":["# Default Project"]},{"cell_type":"markdown","metadata":{"id":"hMDSSA3fGKR3"},"source":["## 1. Optimize the classification accuracy for subject 1. Does it help to train across all subjects?"]},{"cell_type":"markdown","metadata":{"id":"bD3to0v6GKR3"},"source":["#### Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0CabJRqokrSB"},"outputs":[],"source":["def data_prep(X,y,sub_sample,average,noise):\n","    \n","    total_X = None\n","    total_y = None\n","    \n","    # Trimming the data (sample,22,1000) -> (sample,22,500)\n","    X = X[:,:,0:500]\n","    # print('Shape of X after trimming:',X.shape)\n","    \n","    # Maxpooling the data (sample,22,1000) -> (sample,22,500/sub_sample)\n","    X_max = np.max(X.reshape(X.shape[0], X.shape[1], -1, sub_sample), axis=3)\n","    \n","    \n","    total_X = X_max\n","    total_y = y\n","    # print('Shape of X after maxpooling:',total_X.shape)\n","    \n","    # Averaging + noise \n","    X_average = np.mean(X.reshape(X.shape[0], X.shape[1], -1, average),axis=3)\n","    X_average = X_average + np.random.normal(0.0, 0.5, X_average.shape)\n","    \n","    total_X = np.vstack((total_X, X_average))\n","    total_y = np.hstack((total_y, y))\n","    # print('Shape of X after averaging+noise and concatenating:',total_X.shape)\n","    \n","    # Subsampling\n","    \n","    for i in range(sub_sample):\n","        \n","        X_subsample = X[:, :, i::sub_sample] + \\\n","                            (np.random.normal(0.0, 0.5, X[:, :,i::sub_sample].shape) if noise else 0.0)\n","            \n","        total_X = np.vstack((total_X, X_subsample))\n","        total_y = np.hstack((total_y, y))\n","        \n","    \n","    # print('Shape of X after subsampling and concatenating:',total_X.shape)\n","    return total_X,total_y\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f69cLbtZGKR4","outputId":"b520199e-c0e4-43db-d07a-cdac0b259104"},"outputs":[{"name":"stdout","output_type":"stream","text":["X_test Shape for Subject 0: (50, 22, 1000)\n","y_test Shape for Subject 0: (50,)\n","X_train_valid Shape for Subject 0: (237, 22, 1000)\n","y_train_valid Shape for Subject 0: (237,)\n"]}],"source":["X_test = np.load(\"X_test.npy\")\n","y_test = np.load(\"y_test.npy\")\n","person_train_valid = np.load(\"person_train_valid.npy\")\n","X_train_valid = np.load(\"X_train_valid.npy\")\n","y_train_valid = np.load(\"y_train_valid.npy\")\n","person_test = np.load(\"person_test.npy\")\n","\n","## Adjusting the labels so that \n","\n","# Cue onset left - 0\n","# Cue onset right - 1\n","# Cue onset foot - 2\n","# Cue onset tongue - 3\n","\n","y_train_valid -= 769\n","y_test -= 769\n","\n","subject = 0\n","subject_test_idx = np.where(person_test==subject)[0]\n","subject_valid_idx = np.where(person_train_valid==subject)[0]\n","\n","\n","subject_X_test = X_test[subject_test_idx]\n","suject_y_test = y_test[subject_test_idx]\n","suject_X_train_valid = X_train_valid[subject_valid_idx]\n","suject_y_train_valid = y_train_valid[subject_valid_idx]\n","\n","print(f'X_test Shape for Subject {subject}: {subject_X_test.shape}')\n","print(f'y_test Shape for Subject {subject}: {suject_y_test.shape}')\n","print(f'X_train_valid Shape for Subject {subject}: {suject_X_train_valid.shape}')\n","print(f'y_train_valid Shape for Subject {subject}: {suject_y_train_valid.shape}')\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iP8GfInMGKR4","outputId":"81275080-1a9f-4b6f-fa33-2027aa1919cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of training set: (760, 22, 250)\n","Shape of validation set: (188, 22, 250)\n","Shape of training labels: (760,)\n","Shape of validation labels: (188,)\n","Shape of testing set: (200, 22, 250)\n","Shape of testing labels: (200,)\n","Shape of training labels after categorical conversion: (760, 4)\n","Shape of validation labels after categorical conversion: (188, 4)\n","Shape of test labels after categorical conversion: (200, 4)\n","Shape of training set after adding width info: (760, 22, 250, 1)\n","Shape of validation set after adding width info: (188, 22, 250, 1)\n","Shape of test set after adding width info: (200, 22, 250, 1)\n","Shape of training set after dimension reshaping: (760, 250, 1, 22)\n","Shape of validation set after dimension reshaping: (188, 250, 1, 22)\n","Shape of test set after dimension reshaping: (200, 250, 1, 22)\n"]}],"source":["# shuffle with 5 fold\n","indicies_valid = np.random.choice(suject_X_train_valid.shape[0], suject_X_train_valid.shape[0] // 5, replace=False)\n","indicies_train = np.array(list(set(range(suject_X_train_valid.shape[0])).difference(set(indicies_valid))))\n","\n","# Creating the training and validation sets using the generated indices\n","X_train, X_valid = suject_X_train_valid[indicies_train], suject_X_train_valid[indicies_valid] \n","y_train, y_valid = suject_y_train_valid[indicies_train], suject_y_train_valid[indicies_valid]\n","\n","\n","# Preprocessing the dataset\n","x_train,y_train = data_prep(X_train,y_train,2,2,True)\n","x_valid,y_valid = data_prep(X_valid,y_valid,2,2,True)\n","X_test_prep,y_test_prep = data_prep(subject_X_test,suject_y_test,2,2,True)\n","\n","\n","\n","print('Shape of training set:',x_train.shape)\n","print('Shape of validation set:',x_valid.shape)\n","print('Shape of training labels:',y_train.shape)\n","print('Shape of validation labels:',y_valid.shape)\n","print('Shape of testing set:',X_test_prep.shape)\n","print('Shape of testing labels:',y_test_prep.shape)\n","\n","\n","# Converting the labels to categorical variables for multiclass classification\n","y_train = to_categorical(y_train, 4)\n","y_valid = to_categorical(y_valid, 4)\n","y_test = to_categorical(y_test_prep, 4)\n","print('Shape of training labels after categorical conversion:',y_train.shape)\n","print('Shape of validation labels after categorical conversion:',y_valid.shape)\n","print('Shape of test labels after categorical conversion:',y_test.shape)\n","\n","# Adding width of the segment to be 1\n","x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n","x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)\n","x_test = X_test_prep.reshape(X_test_prep.shape[0], X_test_prep.shape[1], X_test_prep.shape[2], 1)\n","print('Shape of training set after adding width info:',x_train.shape)\n","print('Shape of validation set after adding width info:',x_valid.shape)\n","print('Shape of test set after adding width info:',x_test.shape)\n","\n","\n","# Reshaping the training and validation dataset\n","x_train = np.swapaxes(x_train, 1,3)\n","x_train = np.swapaxes(x_train, 1,2)\n","x_valid = np.swapaxes(x_valid, 1,3)\n","x_valid = np.swapaxes(x_valid, 1,2)\n","x_test = np.swapaxes(x_test, 1,3)\n","x_test = np.swapaxes(x_test, 1,2)\n","print('Shape of training set after dimension reshaping:',x_train.shape)\n","print('Shape of validation set after dimension reshaping:',x_valid.shape)\n","print('Shape of test set after dimension reshaping:',x_test.shape)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aYd42N1tGKR4"},"source":["#### Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z2FGJl9hGKR4","outputId":"dc8826c9-2968-43cd-e17e-636676dfc7b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_2 (Conv2D)           (None, 250, 1, 10)        1110      \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 84, 1, 10)        0         \n"," 2D)                                                             \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 84, 1, 10)        40        \n"," hNormalization)                                                 \n","                                                                 \n"," dropout_2 (Dropout)         (None, 84, 1, 10)         0         \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 84, 1, 10)         1510      \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 28, 1, 10)        0         \n"," 2D)                                                             \n","                                                                 \n"," batch_normalization_3 (Batc  (None, 28, 1, 10)        40        \n"," hNormalization)                                                 \n","                                                                 \n"," dropout_3 (Dropout)         (None, 28, 1, 10)         0         \n","                                                                 \n"," flatten_1 (Flatten)         (None, 280)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 4)                 1124      \n","                                                                 \n","=================================================================\n","Total params: 3,824\n","Trainable params: 3,784\n","Non-trainable params: 40\n","_________________________________________________________________\n"]}],"source":["\n","# Building the CNN model using sequential class\n","cnn_subject_model = Sequential()\n","\n","# Conv. block 1\n","cnn_subject_model.add(Conv2D(filters=10, kernel_size=(5,1), padding='same', activation='elu', input_shape=(250,1,22)))\n","cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same')) \n","cnn_subject_model.add(BatchNormalization())\n","cnn_subject_model.add(Dropout(0.5))\n","\n","# Conv. block 2\n","cnn_subject_model.add(Conv2D(filters=10, kernel_size=(15,1), padding='same', activation='elu'))\n","cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n","cnn_subject_model.add(BatchNormalization())\n","cnn_subject_model.add(Dropout(0.5))\n","\n","# Output layer with Softmax activation\n","cnn_subject_model.add(Flatten()) # Flattens the input\n","cnn_subject_model.add(Dense(4, activation='softmax')) # Output FC layer with softmax activation\n","\n","\n","# Printing the model summary\n","cnn_subject_model.summary()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ioTUpE9GGKR5"},"source":["#### Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"px5d_FleGKR5"},"outputs":[],"source":["learning_rate = 1e-3\n","epochs = 100\n","cnn_subject_model_optimizer = keras.optimizers.Adam(learning_rate=learning_rate)"]},{"cell_type":"markdown","metadata":{"id":"u95L4saOGKR5"},"source":["#### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rft3Ig-mGKR5","outputId":"2ae1e88f-8ee5-496c-8328-275c0d07c8f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","12/12 [==============================] - 1s 22ms/step - loss: 2.1012 - accuracy: 0.3053 - val_loss: 5.7088 - val_accuracy: 0.2872\n","Epoch 2/100\n","12/12 [==============================] - 0s 12ms/step - loss: 1.9649 - accuracy: 0.3382 - val_loss: 3.7922 - val_accuracy: 0.3032\n","Epoch 3/100\n","12/12 [==============================] - 0s 10ms/step - loss: 1.8282 - accuracy: 0.3487 - val_loss: 2.8641 - val_accuracy: 0.3351\n","Epoch 4/100\n","12/12 [==============================] - 0s 11ms/step - loss: 1.6714 - accuracy: 0.3776 - val_loss: 2.2715 - val_accuracy: 0.3617\n","Epoch 5/100\n","12/12 [==============================] - 0s 12ms/step - loss: 1.4687 - accuracy: 0.4250 - val_loss: 1.9499 - val_accuracy: 0.3617\n","Epoch 6/100\n","12/12 [==============================] - 0s 10ms/step - loss: 1.3964 - accuracy: 0.4605 - val_loss: 1.7824 - val_accuracy: 0.3777\n","Epoch 7/100\n","12/12 [==============================] - 0s 11ms/step - loss: 1.3930 - accuracy: 0.4289 - val_loss: 1.5789 - val_accuracy: 0.4096\n","Epoch 8/100\n","12/12 [==============================] - 0s 11ms/step - loss: 1.3047 - accuracy: 0.4605 - val_loss: 1.4476 - val_accuracy: 0.3989\n","Epoch 9/100\n","12/12 [==============================] - 0s 16ms/step - loss: 1.3049 - accuracy: 0.4737 - val_loss: 1.3760 - val_accuracy: 0.4202\n","Epoch 10/100\n","12/12 [==============================] - 0s 11ms/step - loss: 1.1704 - accuracy: 0.5145 - val_loss: 1.2565 - val_accuracy: 0.4628\n","Epoch 11/100\n","12/12 [==============================] - 0s 11ms/step - loss: 1.1198 - accuracy: 0.5592 - val_loss: 1.2099 - val_accuracy: 0.4840\n","Epoch 12/100\n","12/12 [==============================] - 0s 13ms/step - loss: 1.0451 - accuracy: 0.5855 - val_loss: 1.0791 - val_accuracy: 0.5319\n","Epoch 13/100\n","12/12 [==============================] - 0s 13ms/step - loss: 0.9862 - accuracy: 0.5961 - val_loss: 1.1048 - val_accuracy: 0.5319\n","Epoch 14/100\n","12/12 [==============================] - 0s 14ms/step - loss: 0.9742 - accuracy: 0.5934 - val_loss: 1.0575 - val_accuracy: 0.5372\n","Epoch 15/100\n","12/12 [==============================] - 0s 15ms/step - loss: 0.8880 - accuracy: 0.6184 - val_loss: 1.0628 - val_accuracy: 0.5266\n","Epoch 16/100\n","12/12 [==============================] - 0s 11ms/step - loss: 0.8278 - accuracy: 0.6605 - val_loss: 1.0038 - val_accuracy: 0.5372\n","Epoch 17/100\n","12/12 [==============================] - 0s 11ms/step - loss: 0.7284 - accuracy: 0.7118 - val_loss: 0.9959 - val_accuracy: 0.5319\n","Epoch 18/100\n","12/12 [==============================] - 0s 14ms/step - loss: 0.7345 - accuracy: 0.7105 - val_loss: 1.0001 - val_accuracy: 0.5213\n","Epoch 19/100\n","12/12 [==============================] - 0s 12ms/step - loss: 0.7737 - accuracy: 0.6855 - val_loss: 0.9828 - val_accuracy: 0.5372\n","Epoch 20/100\n","12/12 [==============================] - 0s 16ms/step - loss: 0.6912 - accuracy: 0.7263 - val_loss: 0.9340 - val_accuracy: 0.5532\n","Epoch 21/100\n","12/12 [==============================] - 0s 18ms/step - loss: 0.6637 - accuracy: 0.7224 - val_loss: 0.9516 - val_accuracy: 0.5372\n","Epoch 22/100\n","12/12 [==============================] - 0s 15ms/step - loss: 0.6354 - accuracy: 0.7447 - val_loss: 1.0503 - val_accuracy: 0.4947\n","Epoch 23/100\n","12/12 [==============================] - 0s 17ms/step - loss: 0.6436 - accuracy: 0.7316 - val_loss: 0.9874 - val_accuracy: 0.5160\n","Epoch 24/100\n","12/12 [==============================] - 0s 12ms/step - loss: 0.5955 - accuracy: 0.7645 - val_loss: 0.9956 - val_accuracy: 0.5372\n","Epoch 25/100\n","12/12 [==============================] - 0s 12ms/step - loss: 0.5333 - accuracy: 0.8013 - val_loss: 1.0025 - val_accuracy: 0.5532\n","Epoch 26/100\n","12/12 [==============================] - 0s 11ms/step - loss: 0.5458 - accuracy: 0.7763 - val_loss: 1.0020 - val_accuracy: 0.5372\n","Epoch 27/100\n","12/12 [==============================] - 0s 14ms/step - loss: 0.5414 - accuracy: 0.7816 - val_loss: 0.9298 - val_accuracy: 0.5691\n","Epoch 28/100\n","12/12 [==============================] - 0s 11ms/step - loss: 0.4807 - accuracy: 0.8105 - val_loss: 1.0018 - val_accuracy: 0.5426\n","Epoch 29/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.5474 - accuracy: 0.7803 - val_loss: 0.9618 - val_accuracy: 0.5319\n","Epoch 30/100\n","12/12 [==============================] - 0s 11ms/step - loss: 0.4988 - accuracy: 0.7868 - val_loss: 0.9345 - val_accuracy: 0.5691\n","Epoch 31/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.4907 - accuracy: 0.8053 - val_loss: 0.9778 - val_accuracy: 0.5691\n","Epoch 32/100\n","12/12 [==============================] - 0s 10ms/step - loss: 0.4757 - accuracy: 0.8079 - val_loss: 0.9079 - val_accuracy: 0.5638\n","Epoch 33/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.4891 - accuracy: 0.8066 - val_loss: 0.9738 - val_accuracy: 0.5213\n","Epoch 34/100\n","12/12 [==============================] - 0s 10ms/step - loss: 0.4445 - accuracy: 0.8105 - val_loss: 0.9683 - val_accuracy: 0.5479\n","Epoch 35/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.4079 - accuracy: 0.8408 - val_loss: 1.0106 - val_accuracy: 0.5426\n","Epoch 36/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.4324 - accuracy: 0.8289 - val_loss: 0.9883 - val_accuracy: 0.5053\n","Epoch 37/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.4407 - accuracy: 0.8197 - val_loss: 0.9179 - val_accuracy: 0.5638\n","Epoch 38/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.4291 - accuracy: 0.8289 - val_loss: 0.9539 - val_accuracy: 0.5638\n","Epoch 39/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.4251 - accuracy: 0.8421 - val_loss: 0.9771 - val_accuracy: 0.5691\n","Epoch 40/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.4160 - accuracy: 0.8316 - val_loss: 0.9489 - val_accuracy: 0.5638\n","Epoch 41/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.3840 - accuracy: 0.8513 - val_loss: 0.9815 - val_accuracy: 0.5426\n","Epoch 42/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.3730 - accuracy: 0.8447 - val_loss: 0.9814 - val_accuracy: 0.5426\n","Epoch 43/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.3730 - accuracy: 0.8526 - val_loss: 0.9783 - val_accuracy: 0.5372\n","Epoch 44/100\n","12/12 [==============================] - 0s 14ms/step - loss: 0.3916 - accuracy: 0.8487 - val_loss: 1.0029 - val_accuracy: 0.5319\n","Epoch 45/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.3698 - accuracy: 0.8618 - val_loss: 0.9144 - val_accuracy: 0.5745\n","Epoch 46/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.3400 - accuracy: 0.8684 - val_loss: 0.9672 - val_accuracy: 0.5532\n","Epoch 47/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.3060 - accuracy: 0.8855 - val_loss: 0.8971 - val_accuracy: 0.5798\n","Epoch 48/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.3512 - accuracy: 0.8566 - val_loss: 0.9194 - val_accuracy: 0.5691\n","Epoch 49/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.3044 - accuracy: 0.8908 - val_loss: 0.9404 - val_accuracy: 0.6064\n","Epoch 50/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.3223 - accuracy: 0.8711 - val_loss: 0.9399 - val_accuracy: 0.6117\n","Epoch 51/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.2903 - accuracy: 0.8934 - val_loss: 0.9482 - val_accuracy: 0.6064\n","Epoch 52/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.2994 - accuracy: 0.8895 - val_loss: 0.9018 - val_accuracy: 0.5957\n","Epoch 53/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.2974 - accuracy: 0.8816 - val_loss: 0.8813 - val_accuracy: 0.6064\n","Epoch 54/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.2565 - accuracy: 0.9092 - val_loss: 0.9130 - val_accuracy: 0.6383\n","Epoch 55/100\n","12/12 [==============================] - 0s 11ms/step - loss: 0.2820 - accuracy: 0.8987 - val_loss: 0.9026 - val_accuracy: 0.6064\n","Epoch 56/100\n","12/12 [==============================] - 0s 12ms/step - loss: 0.2745 - accuracy: 0.9026 - val_loss: 0.9165 - val_accuracy: 0.6170\n","Epoch 57/100\n","12/12 [==============================] - 0s 11ms/step - loss: 0.2963 - accuracy: 0.8908 - val_loss: 0.9558 - val_accuracy: 0.6011\n","Epoch 58/100\n","12/12 [==============================] - 0s 10ms/step - loss: 0.2721 - accuracy: 0.8987 - val_loss: 0.9590 - val_accuracy: 0.5691\n","Epoch 59/100\n","12/12 [==============================] - 0s 10ms/step - loss: 0.2682 - accuracy: 0.8947 - val_loss: 0.9143 - val_accuracy: 0.5957\n","Epoch 60/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.2572 - accuracy: 0.9053 - val_loss: 0.9258 - val_accuracy: 0.6330\n","Epoch 61/100\n","12/12 [==============================] - 0s 10ms/step - loss: 0.2560 - accuracy: 0.9092 - val_loss: 0.9168 - val_accuracy: 0.5904\n","Epoch 62/100\n","12/12 [==============================] - 0s 11ms/step - loss: 0.2648 - accuracy: 0.9039 - val_loss: 0.9354 - val_accuracy: 0.5851\n","Epoch 63/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.2749 - accuracy: 0.9066 - val_loss: 0.8840 - val_accuracy: 0.6170\n","Epoch 64/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.2697 - accuracy: 0.9039 - val_loss: 0.9100 - val_accuracy: 0.6489\n","Epoch 65/100\n","12/12 [==============================] - 0s 10ms/step - loss: 0.2542 - accuracy: 0.9026 - val_loss: 0.9014 - val_accuracy: 0.6436\n","Epoch 66/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.2573 - accuracy: 0.9079 - val_loss: 0.8626 - val_accuracy: 0.6755\n","Epoch 67/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.2222 - accuracy: 0.9132 - val_loss: 0.8541 - val_accuracy: 0.6436\n","Epoch 68/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.2727 - accuracy: 0.8974 - val_loss: 0.8883 - val_accuracy: 0.6436\n","Epoch 69/100\n","12/12 [==============================] - 0s 12ms/step - loss: 0.2332 - accuracy: 0.9053 - val_loss: 0.9132 - val_accuracy: 0.6170\n","Epoch 70/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.2310 - accuracy: 0.9184 - val_loss: 0.8895 - val_accuracy: 0.6436\n","Epoch 71/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.2249 - accuracy: 0.9184 - val_loss: 0.8914 - val_accuracy: 0.6755\n","Epoch 72/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.2329 - accuracy: 0.9211 - val_loss: 0.8780 - val_accuracy: 0.6543\n","Epoch 73/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.2056 - accuracy: 0.9211 - val_loss: 0.8364 - val_accuracy: 0.6968\n","Epoch 74/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.2068 - accuracy: 0.9316 - val_loss: 0.8183 - val_accuracy: 0.7074\n","Epoch 75/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.2239 - accuracy: 0.9079 - val_loss: 0.8324 - val_accuracy: 0.7021\n","Epoch 76/100\n","12/12 [==============================] - 0s 10ms/step - loss: 0.2088 - accuracy: 0.9145 - val_loss: 0.8138 - val_accuracy: 0.6862\n","Epoch 77/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.1975 - accuracy: 0.9342 - val_loss: 0.8237 - val_accuracy: 0.6649\n","Epoch 78/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.2025 - accuracy: 0.9276 - val_loss: 0.8555 - val_accuracy: 0.6755\n","Epoch 79/100\n","12/12 [==============================] - 0s 10ms/step - loss: 0.2278 - accuracy: 0.9197 - val_loss: 0.8834 - val_accuracy: 0.6649\n","Epoch 80/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.1764 - accuracy: 0.9329 - val_loss: 0.8489 - val_accuracy: 0.6809\n","Epoch 81/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.2081 - accuracy: 0.9263 - val_loss: 0.8624 - val_accuracy: 0.6755\n","Epoch 82/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.2025 - accuracy: 0.9118 - val_loss: 0.9757 - val_accuracy: 0.6596\n","Epoch 83/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.2269 - accuracy: 0.9132 - val_loss: 0.8385 - val_accuracy: 0.6649\n","Epoch 84/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.1962 - accuracy: 0.9395 - val_loss: 0.8623 - val_accuracy: 0.6968\n","Epoch 85/100\n","12/12 [==============================] - 0s 10ms/step - loss: 0.2021 - accuracy: 0.9224 - val_loss: 0.8659 - val_accuracy: 0.6649\n","Epoch 86/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.1750 - accuracy: 0.9500 - val_loss: 0.8369 - val_accuracy: 0.7021\n","Epoch 87/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.1808 - accuracy: 0.9487 - val_loss: 0.8444 - val_accuracy: 0.6543\n","Epoch 88/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.1701 - accuracy: 0.9461 - val_loss: 0.8902 - val_accuracy: 0.6702\n","Epoch 89/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.2067 - accuracy: 0.9250 - val_loss: 0.9400 - val_accuracy: 0.6543\n","Epoch 90/100\n","12/12 [==============================] - 0s 11ms/step - loss: 0.1514 - accuracy: 0.9487 - val_loss: 0.8938 - val_accuracy: 0.6383\n","Epoch 91/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.1761 - accuracy: 0.9368 - val_loss: 0.9316 - val_accuracy: 0.6596\n","Epoch 92/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.1947 - accuracy: 0.9237 - val_loss: 0.8303 - val_accuracy: 0.7021\n","Epoch 93/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.1899 - accuracy: 0.9355 - val_loss: 0.9308 - val_accuracy: 0.6170\n","Epoch 94/100\n","12/12 [==============================] - 0s 10ms/step - loss: 0.2055 - accuracy: 0.9276 - val_loss: 0.8537 - val_accuracy: 0.6862\n","Epoch 95/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.1604 - accuracy: 0.9461 - val_loss: 0.8450 - val_accuracy: 0.6702\n","Epoch 96/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.1650 - accuracy: 0.9316 - val_loss: 0.8823 - val_accuracy: 0.7021\n","Epoch 97/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.1790 - accuracy: 0.9434 - val_loss: 0.8604 - val_accuracy: 0.6702\n","Epoch 98/100\n","12/12 [==============================] - 0s 10ms/step - loss: 0.1850 - accuracy: 0.9355 - val_loss: 0.8636 - val_accuracy: 0.6489\n","Epoch 99/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.1484 - accuracy: 0.9500 - val_loss: 0.8912 - val_accuracy: 0.6649\n","Epoch 100/100\n","12/12 [==============================] - 0s 9ms/step - loss: 0.1326 - accuracy: 0.9526 - val_loss: 0.8272 - val_accuracy: 0.6330\n"]}],"source":["cnn_subject_model.compile(loss='categorical_crossentropy',\n","                 optimizer=cnn_subject_model_optimizer,\n","                 metrics=['accuracy'])\n","\n","cnn_subject_model_results = cnn_subject_model.fit(x_train,\n","             y_train,\n","             batch_size=64,\n","             epochs=epochs,\n","             validation_data=(x_valid, y_valid), verbose=True\n","             )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kFaoUrLdkrSG","outputId":"e1a7b023-c5f0-4c3f-8b6f-40d9cf8d7c98"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# summarize history for accuracy\n","plt.plot(cnn_subject_model_results.history['accuracy'])\n","plt.plot(cnn_subject_model_results.history['val_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()\n","# summarize history for loss\n","plt.plot(cnn_subject_model_results.history['loss'])\n","plt.plot(cnn_subject_model_results.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"3yxvYcfjGKR6"},"source":["#### Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QwZMjkCEGKR6","outputId":"210e45e6-21ee-4c74-c934-6c82829af9a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test accuracy of the CNN model for subject 0: 0.75\n"]}],"source":["cnn_subject_model_score = cnn_subject_model.evaluate(x_test, y_test, verbose=0)\n","print(f'Test accuracy of the CNN model for subject {subject}:',cnn_subject_model_score[1])"]},{"cell_type":"markdown","metadata":{"id":"dTD25eBSGKR6"},"source":["## 1.2 Now Training across all subjects"]},{"cell_type":"markdown","metadata":{"id":"krzQennDGKR6"},"source":["#### Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sEOEH8rRGKR6","outputId":"16e8b40c-2321-462e-ca7d-b20c35f8a3c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["X_test Shape for Subject 0: (50, 22, 1000)\n","y_test Shape for Subject 0: (50,)\n","X_train_valid Shape for Subject 0: (237, 22, 1000)\n","y_train_valid Shape for Subject 0: (237,)\n","Shape of training set: (6768, 22, 250)\n","Shape of validation set: (1692, 22, 250)\n","Shape of training labels: (6768,)\n","Shape of validation labels: (1692,)\n","Shape of testing set: (200, 22, 250)\n","Shape of testing labels: (200,)\n","Shape of training labels after categorical conversion: (6768, 4)\n","Shape of validation labels after categorical conversion: (1692, 4)\n","Shape of test labels after categorical conversion: (200, 4)\n","Shape of training set after adding width info: (6768, 22, 250, 1)\n","Shape of validation set after adding width info: (1692, 22, 250, 1)\n","Shape of test set after adding width info: (200, 22, 250, 1)\n","Shape of training set after dimension reshaping: (6768, 250, 1, 22)\n","Shape of validation set after dimension reshaping: (1692, 250, 1, 22)\n","Shape of test set after dimension reshaping: (200, 250, 1, 22)\n"]}],"source":["X_test = np.load(\"X_test.npy\")\n","y_test = np.load(\"y_test.npy\")\n","person_train_valid = np.load(\"person_train_valid.npy\")\n","X_train_valid = np.load(\"X_train_valid.npy\")\n","y_train_valid = np.load(\"y_train_valid.npy\")\n","person_test = np.load(\"person_test.npy\")\n","\n","## Adjusting the labels so that \n","\n","# Cue onset left - 0\n","# Cue onset right - 1\n","# Cue onset foot - 2\n","# Cue onset tongue - 3\n","\n","y_train_valid -= 769\n","y_test -= 769\n","\n","subject = 0\n","subject_test_idx = np.where(person_test==subject)[0]\n","subject_valid_idx = np.where(person_train_valid==subject)[0]\n","\n","\n","subject_X_test = X_test[subject_test_idx]\n","suject_y_test = y_test[subject_test_idx]\n","suject_X_train_valid = X_train_valid[subject_valid_idx]\n","suject_y_train_valid = y_train_valid[subject_valid_idx]\n","\n","print(f'X_test Shape for Subject {subject}: {subject_X_test.shape}')\n","print(f'y_test Shape for Subject {subject}: {suject_y_test.shape}')\n","print(f'X_train_valid Shape for Subject {subject}: {suject_X_train_valid.shape}')\n","print(f'y_train_valid Shape for Subject {subject}: {suject_y_train_valid.shape}')\n","\n","# shuffle with 5 fold\n","indicies_valid = np.random.choice(X_train_valid.shape[0], X_train_valid.shape[0] // 5, replace=False)\n","indicies_train = np.array(list(set(range(X_train_valid.shape[0])).difference(set(indicies_valid))))\n","\n","# Creating the training and validation sets using the generated indices\n","X_train, X_valid = X_train_valid[indicies_train], X_train_valid[indicies_valid] \n","y_train, y_valid = y_train_valid[indicies_train], y_train_valid[indicies_valid]\n","\n","\n","# Preprocessing the dataset\n","x_train,y_train = data_prep(X_train,y_train,2,2,True)\n","x_valid,y_valid = data_prep(X_valid,y_valid,2,2,True)\n","X_test_prep,y_test_prep = data_prep(subject_X_test,suject_y_test,2,2,True)\n","\n","\n","\n","print('Shape of training set:',x_train.shape)\n","print('Shape of validation set:',x_valid.shape)\n","print('Shape of training labels:',y_train.shape)\n","print('Shape of validation labels:',y_valid.shape)\n","print('Shape of testing set:',X_test_prep.shape)\n","print('Shape of testing labels:',y_test_prep.shape)\n","\n","\n","# Converting the labels to categorical variables for multiclass classification\n","y_train = to_categorical(y_train, 4)\n","y_valid = to_categorical(y_valid, 4)\n","y_test = to_categorical(y_test_prep, 4)\n","print('Shape of training labels after categorical conversion:',y_train.shape)\n","print('Shape of validation labels after categorical conversion:',y_valid.shape)\n","print('Shape of test labels after categorical conversion:',y_test.shape)\n","\n","# Adding width of the segment to be 1\n","x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n","x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)\n","x_test = X_test_prep.reshape(X_test_prep.shape[0], X_test_prep.shape[1], X_test_prep.shape[2], 1)\n","print('Shape of training set after adding width info:',x_train.shape)\n","print('Shape of validation set after adding width info:',x_valid.shape)\n","print('Shape of test set after adding width info:',x_test.shape)\n","\n","\n","# Reshaping the training and validation dataset\n","x_train = np.swapaxes(x_train, 1,3)\n","x_train = np.swapaxes(x_train, 1,2)\n","x_valid = np.swapaxes(x_valid, 1,3)\n","x_valid = np.swapaxes(x_valid, 1,2)\n","x_test = np.swapaxes(x_test, 1,3)\n","x_test = np.swapaxes(x_test, 1,2)\n","print('Shape of training set after dimension reshaping:',x_train.shape)\n","print('Shape of validation set after dimension reshaping:',x_valid.shape)\n","print('Shape of test set after dimension reshaping:',x_test.shape)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DCKBSGgSGKR7"},"source":["#### Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tVI4PzObGKR7","outputId":"c7fc9e19-a2b6-42c2-c128-759b80698730"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_4 (Conv2D)           (None, 250, 1, 10)        1110      \n","                                                                 \n"," max_pooling2d_4 (MaxPooling  (None, 84, 1, 10)        0         \n"," 2D)                                                             \n","                                                                 \n"," batch_normalization_4 (Batc  (None, 84, 1, 10)        40        \n"," hNormalization)                                                 \n","                                                                 \n"," dropout_4 (Dropout)         (None, 84, 1, 10)         0         \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 84, 1, 10)         1510      \n","                                                                 \n"," max_pooling2d_5 (MaxPooling  (None, 28, 1, 10)        0         \n"," 2D)                                                             \n","                                                                 \n"," batch_normalization_5 (Batc  (None, 28, 1, 10)        40        \n"," hNormalization)                                                 \n","                                                                 \n"," dropout_5 (Dropout)         (None, 28, 1, 10)         0         \n","                                                                 \n"," flatten_2 (Flatten)         (None, 280)               0         \n","                                                                 \n"," dense_2 (Dense)             (None, 4)                 1124      \n","                                                                 \n","=================================================================\n","Total params: 3,824\n","Trainable params: 3,784\n","Non-trainable params: 40\n","_________________________________________________________________\n"]}],"source":["\n","# Building the CNN model using sequential class\n","cnn_subject_model = Sequential()\n","\n","# Conv. block 1\n","cnn_subject_model.add(Conv2D(filters=10, kernel_size=(5,1), padding='same', activation='elu', input_shape=(250,1,22)))\n","cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same')) \n","cnn_subject_model.add(BatchNormalization())\n","cnn_subject_model.add(Dropout(0.5))\n","\n","# Conv. block 2\n","cnn_subject_model.add(Conv2D(filters=10, kernel_size=(15,1), padding='same', activation='elu'))\n","cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n","cnn_subject_model.add(BatchNormalization())\n","cnn_subject_model.add(Dropout(0.5))\n","\n","\n","# Output layer with Softmax activation\n","cnn_subject_model.add(Flatten()) # Flattens the input\n","cnn_subject_model.add(Dense(4, activation='softmax')) # Output FC layer with softmax activation\n","\n","\n","# Printing the model summary\n","cnn_subject_model.summary()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4MtxVA-PGKR7"},"source":["#### Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vUCxzbm6GKR8"},"outputs":[],"source":["learning_rate = 1e-3\n","epochs = 100\n","cnn_subject_model_optimizer = keras.optimizers.Adam(learning_rate=learning_rate)"]},{"cell_type":"markdown","metadata":{"id":"erljC2VLGKR8"},"source":["#### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"56QXs7HFGKR8","outputId":"4102f43e-d78f-4145-dee2-029d6d46a793"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","106/106 [==============================] - 1s 9ms/step - loss: 2.0054 - accuracy: 0.2858 - val_loss: 1.4232 - val_accuracy: 0.3700\n","Epoch 2/100\n"," 32/106 [========>.....................] - ETA: 0s - loss: 1.6796 - accuracy: 0.3188"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m cnn_subject_model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      2\u001b[0m                  optimizer\u001b[39m=\u001b[39mcnn_subject_model_optimizer,\n\u001b[1;32m      3\u001b[0m                  metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m cnn_subject_model_results \u001b[39m=\u001b[39m cnn_subject_model\u001b[39m.\u001b[39;49mfit(x_train,\n\u001b[1;32m      6\u001b[0m              y_train,\n\u001b[1;32m      7\u001b[0m              batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m              epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m      9\u001b[0m              validation_data\u001b[39m=\u001b[39;49m(x_valid, y_valid), verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","File \u001b[0;32m~/opt/anaconda3/envs/ee147/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[0;32m~/opt/anaconda3/envs/ee147/lib/python3.10/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n","File \u001b[0;32m~/opt/anaconda3/envs/ee147/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[0;32m~/opt/anaconda3/envs/ee147/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m~/opt/anaconda3/envs/ee147/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n","File \u001b[0;32m~/opt/anaconda3/envs/ee147/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n","File \u001b[0;32m~/opt/anaconda3/envs/ee147/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n","File \u001b[0;32m~/opt/anaconda3/envs/ee147/lib/python3.10/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n","File \u001b[0;32m~/opt/anaconda3/envs/ee147/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["cnn_subject_model.compile(loss='categorical_crossentropy',\n","                 optimizer=cnn_subject_model_optimizer,\n","                 metrics=['accuracy'])\n","\n","cnn_subject_model_results = cnn_subject_model.fit(x_train,\n","             y_train,\n","             batch_size=64,\n","             epochs=epochs,\n","             validation_data=(x_valid, y_valid), verbose=True)\n"]},{"cell_type":"markdown","metadata":{"id":"-acbUx1PGKR8"},"source":["#### Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iiZDDFgUGKR9","outputId":"21e605ec-33b8-4a7c-ecbf-1690941fe6ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test accuracy of the CNN model for subject 0: 0.6449999809265137\n"]}],"source":["cnn_subject_model_score = cnn_subject_model.evaluate(x_test, y_test, verbose=False)\n","print(f'Test accuracy of the CNN model for subject {subject}:',cnn_subject_model_score[1])\n"]},{"cell_type":"markdown","metadata":{"id":"APp4GfXiGKR9"},"source":["## 2. Optimize the classification accuracy across all subjects. How does the classifier do? Do you notice any interesting trends?"]},{"cell_type":"markdown","metadata":{"id":"xWKBf2NdGKR9"},"source":["#### Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g6_qnCAcGKR9"},"outputs":[],"source":["def preprocess_subjects(subject):\n","    X_test = np.load(\"X_test.npy\")\n","    y_test = np.load(\"y_test.npy\")\n","    person_train_valid = np.load(\"person_train_valid.npy\")\n","    X_train_valid = np.load(\"X_train_valid.npy\")\n","    y_train_valid = np.load(\"y_train_valid.npy\")\n","    person_test = np.load(\"person_test.npy\")\n","\n","    ## Adjusting the labels so that \n","\n","    # Cue onset left - 0\n","    # Cue onset right - 1\n","    # Cue onset foot - 2\n","    # Cue onset tongue - 3\n","\n","    y_train_valid -= 769\n","    y_test -= 769\n","    \n","\n","\n","    subject_test_idx = np.where(person_test==subject)[0]\n","    subject_valid_idx = np.where(person_train_valid==subject)[0]\n","\n","\n","    subject_X_test = X_test[subject_test_idx]\n","    suject_y_test = y_test[subject_test_idx]\n","    suject_X_train_valid = X_train_valid[subject_valid_idx]\n","    suject_y_train_valid = y_train_valid[subject_valid_idx]\n","\n","\n","    # shuffle with 5 fold\n","    indicies_valid = np.random.choice(X_train_valid.shape[0], X_train_valid.shape[0] // 5, replace=False)\n","    indicies_train = np.array(list(set(range(X_train_valid.shape[0])).difference(set(indicies_valid))))\n","\n","    # Creating the training and validation sets using the generated indices\n","    X_train, X_valid = X_train_valid[indicies_train], X_train_valid[indicies_valid] \n","    y_train, y_valid = y_train_valid[indicies_train], y_train_valid[indicies_valid]\n","\n","\n","    # Preprocessing the dataset\n","    x_train,y_train = data_prep(X_train,y_train,2,2,True)\n","    x_valid,y_valid = data_prep(X_valid,y_valid,2,2,True)\n","    X_test_prep,y_test_prep = data_prep(subject_X_test,suject_y_test,2,2,True)\n","\n","\n","\n","    # Converting the labels to categorical variables for multiclass classification\n","    y_train = to_categorical(y_train, 4)\n","    y_valid = to_categorical(y_valid, 4)\n","    y_test = to_categorical(y_test_prep, 4)\n","\n","\n","    # Adding width of the segment to be 1\n","    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n","    x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)\n","    x_test = X_test_prep.reshape(X_test_prep.shape[0], X_test_prep.shape[1], X_test_prep.shape[2], 1)\n","\n","\n","    # Reshaping the training and validation dataset\n","    x_train = np.swapaxes(x_train, 1,3)\n","    x_train = np.swapaxes(x_train, 1,2)\n","    x_valid = np.swapaxes(x_valid, 1,3)\n","    x_valid = np.swapaxes(x_valid, 1,2)\n","    x_test = np.swapaxes(x_test, 1,3)\n","    x_test = np.swapaxes(x_test, 1,2)\n","\n","    return (x_train, y_train, x_valid, y_valid, x_test, y_test)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xJC849YUGKR-"},"source":["#### Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZIF9FVOqGKR-","outputId":"45f315b0-4de0-4eee-944b-54ad025caa09"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","106/106 [==============================] - 3s 26ms/step - loss: 1.8722 - accuracy: 0.2914 - val_loss: 1.4435 - val_accuracy: 0.3322\n","Epoch 2/100\n","106/106 [==============================] - 2s 23ms/step - loss: 1.5134 - accuracy: 0.3468 - val_loss: 1.2604 - val_accuracy: 0.3936\n","Epoch 3/100\n","106/106 [==============================] - 2s 23ms/step - loss: 1.3594 - accuracy: 0.3840 - val_loss: 1.2390 - val_accuracy: 0.3995\n","Epoch 4/100\n","106/106 [==============================] - 2s 23ms/step - loss: 1.2570 - accuracy: 0.4289 - val_loss: 1.1870 - val_accuracy: 0.4687\n","Epoch 5/100\n","106/106 [==============================] - 2s 23ms/step - loss: 1.2129 - accuracy: 0.4542 - val_loss: 1.1776 - val_accuracy: 0.4929\n","Epoch 6/100\n","106/106 [==============================] - 2s 23ms/step - loss: 1.1811 - accuracy: 0.4690 - val_loss: 1.1546 - val_accuracy: 0.5118\n","Epoch 7/100\n","106/106 [==============================] - 2s 22ms/step - loss: 1.1562 - accuracy: 0.4866 - val_loss: 1.1436 - val_accuracy: 0.5378\n","Epoch 8/100\n","106/106 [==============================] - 2s 23ms/step - loss: 1.1365 - accuracy: 0.5069 - val_loss: 1.1213 - val_accuracy: 0.5520\n","Epoch 9/100\n","106/106 [==============================] - 2s 22ms/step - loss: 1.1140 - accuracy: 0.5134 - val_loss: 1.1082 - val_accuracy: 0.5544\n","Epoch 10/100\n","106/106 [==============================] - 2s 23ms/step - loss: 1.0987 - accuracy: 0.5247 - val_loss: 1.0994 - val_accuracy: 0.5691\n","Epoch 11/100\n","106/106 [==============================] - 2s 22ms/step - loss: 1.0882 - accuracy: 0.5337 - val_loss: 1.0765 - val_accuracy: 0.5804\n","Epoch 12/100\n","106/106 [==============================] - 2s 23ms/step - loss: 1.0551 - accuracy: 0.5536 - val_loss: 1.0490 - val_accuracy: 0.5969\n","Epoch 13/100\n","106/106 [==============================] - 2s 23ms/step - loss: 1.0336 - accuracy: 0.5624 - val_loss: 1.0084 - val_accuracy: 0.6040\n","Epoch 14/100\n","106/106 [==============================] - 2s 23ms/step - loss: 1.0279 - accuracy: 0.5708 - val_loss: 0.9843 - val_accuracy: 0.6070\n","Epoch 15/100\n","106/106 [==============================] - 2s 23ms/step - loss: 1.0032 - accuracy: 0.5876 - val_loss: 0.9934 - val_accuracy: 0.6058\n","Epoch 16/100\n","106/106 [==============================] - 3s 24ms/step - loss: 0.9861 - accuracy: 0.5907 - val_loss: 0.9357 - val_accuracy: 0.6330\n","Epoch 17/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.9746 - accuracy: 0.5968 - val_loss: 0.9256 - val_accuracy: 0.6383\n","Epoch 18/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.9495 - accuracy: 0.6036 - val_loss: 0.9168 - val_accuracy: 0.6306\n","Epoch 19/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.9459 - accuracy: 0.6068 - val_loss: 0.9168 - val_accuracy: 0.6212\n","Epoch 20/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.9365 - accuracy: 0.6201 - val_loss: 0.8872 - val_accuracy: 0.6548\n","Epoch 21/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.9219 - accuracy: 0.6185 - val_loss: 0.8918 - val_accuracy: 0.6472\n","Epoch 22/100\n","106/106 [==============================] - 3s 24ms/step - loss: 0.9061 - accuracy: 0.6316 - val_loss: 0.9025 - val_accuracy: 0.6407\n","Epoch 23/100\n","106/106 [==============================] - 3s 24ms/step - loss: 0.8961 - accuracy: 0.6299 - val_loss: 0.9061 - val_accuracy: 0.6288\n","Epoch 24/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.8974 - accuracy: 0.6281 - val_loss: 0.8744 - val_accuracy: 0.6590\n","Epoch 25/100\n","106/106 [==============================] - 3s 27ms/step - loss: 0.8746 - accuracy: 0.6488 - val_loss: 0.8701 - val_accuracy: 0.6578\n","Epoch 26/100\n","106/106 [==============================] - 3s 25ms/step - loss: 0.8840 - accuracy: 0.6383 - val_loss: 0.8624 - val_accuracy: 0.6714\n","Epoch 27/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.8676 - accuracy: 0.6520 - val_loss: 0.8637 - val_accuracy: 0.6667\n","Epoch 28/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.8738 - accuracy: 0.6452 - val_loss: 0.8487 - val_accuracy: 0.6738\n","Epoch 29/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.8650 - accuracy: 0.6520 - val_loss: 0.8579 - val_accuracy: 0.6803\n","Epoch 30/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.8513 - accuracy: 0.6578 - val_loss: 0.8704 - val_accuracy: 0.6531\n","Epoch 31/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.8329 - accuracy: 0.6634 - val_loss: 0.8397 - val_accuracy: 0.6761\n","Epoch 32/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.8426 - accuracy: 0.6608 - val_loss: 0.8369 - val_accuracy: 0.6714\n","Epoch 33/100\n","106/106 [==============================] - 3s 24ms/step - loss: 0.8231 - accuracy: 0.6698 - val_loss: 0.8345 - val_accuracy: 0.6785\n","Epoch 34/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.8134 - accuracy: 0.6717 - val_loss: 0.8424 - val_accuracy: 0.6720\n","Epoch 35/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.8190 - accuracy: 0.6748 - val_loss: 0.8289 - val_accuracy: 0.6927\n","Epoch 36/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.8098 - accuracy: 0.6776 - val_loss: 0.8198 - val_accuracy: 0.6826\n","Epoch 37/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.8074 - accuracy: 0.6773 - val_loss: 0.8408 - val_accuracy: 0.6637\n","Epoch 38/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.8009 - accuracy: 0.6779 - val_loss: 0.8426 - val_accuracy: 0.6631\n","Epoch 39/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.8023 - accuracy: 0.6809 - val_loss: 0.8319 - val_accuracy: 0.6832\n","Epoch 40/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.8043 - accuracy: 0.6741 - val_loss: 0.8148 - val_accuracy: 0.6956\n","Epoch 41/100\n","106/106 [==============================] - 2s 20ms/step - loss: 0.7922 - accuracy: 0.6887 - val_loss: 0.8299 - val_accuracy: 0.6809\n","Epoch 42/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7845 - accuracy: 0.6791 - val_loss: 0.8024 - val_accuracy: 0.6992\n","Epoch 43/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7882 - accuracy: 0.6850 - val_loss: 0.8214 - val_accuracy: 0.6832\n","Epoch 44/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7877 - accuracy: 0.6884 - val_loss: 0.8515 - val_accuracy: 0.6566\n","Epoch 45/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7699 - accuracy: 0.6950 - val_loss: 0.8334 - val_accuracy: 0.6820\n","Epoch 46/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7727 - accuracy: 0.6884 - val_loss: 0.8113 - val_accuracy: 0.6944\n","Epoch 47/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7543 - accuracy: 0.7001 - val_loss: 0.8008 - val_accuracy: 0.6980\n","Epoch 48/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7724 - accuracy: 0.6905 - val_loss: 0.7975 - val_accuracy: 0.6974\n","Epoch 49/100\n","106/106 [==============================] - 2s 14ms/step - loss: 0.7779 - accuracy: 0.6947 - val_loss: 0.7979 - val_accuracy: 0.7045\n","Epoch 50/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7698 - accuracy: 0.7002 - val_loss: 0.8088 - val_accuracy: 0.6927\n","Epoch 51/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7557 - accuracy: 0.6968 - val_loss: 0.8275 - val_accuracy: 0.6832\n","Epoch 52/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7375 - accuracy: 0.7043 - val_loss: 0.7972 - val_accuracy: 0.7057\n","Epoch 53/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7540 - accuracy: 0.7083 - val_loss: 0.8061 - val_accuracy: 0.6933\n","Epoch 54/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7540 - accuracy: 0.6970 - val_loss: 0.7891 - val_accuracy: 0.7110\n","Epoch 55/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7494 - accuracy: 0.6989 - val_loss: 0.8163 - val_accuracy: 0.6838\n","Epoch 56/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7509 - accuracy: 0.7021 - val_loss: 0.7793 - val_accuracy: 0.7169\n","Epoch 57/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7581 - accuracy: 0.6996 - val_loss: 0.7764 - val_accuracy: 0.7039\n","Epoch 58/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7443 - accuracy: 0.7054 - val_loss: 0.8050 - val_accuracy: 0.7039\n","Epoch 59/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7287 - accuracy: 0.7057 - val_loss: 0.7760 - val_accuracy: 0.7163\n","Epoch 60/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7424 - accuracy: 0.6962 - val_loss: 0.7841 - val_accuracy: 0.6944\n","Epoch 61/100\n","106/106 [==============================] - 2s 15ms/step - loss: 0.7361 - accuracy: 0.7100 - val_loss: 0.7929 - val_accuracy: 0.6879\n","Epoch 62/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7298 - accuracy: 0.7094 - val_loss: 0.7853 - val_accuracy: 0.7222\n","Epoch 63/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7345 - accuracy: 0.7024 - val_loss: 0.7922 - val_accuracy: 0.7092\n","Epoch 64/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7327 - accuracy: 0.7092 - val_loss: 0.7846 - val_accuracy: 0.7021\n","Epoch 65/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7225 - accuracy: 0.7138 - val_loss: 0.7976 - val_accuracy: 0.6921\n","Epoch 66/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7229 - accuracy: 0.7137 - val_loss: 0.7787 - val_accuracy: 0.7169\n","Epoch 67/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7144 - accuracy: 0.7147 - val_loss: 0.7991 - val_accuracy: 0.7045\n","Epoch 68/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7203 - accuracy: 0.7135 - val_loss: 0.7901 - val_accuracy: 0.7015\n","Epoch 69/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7221 - accuracy: 0.7185 - val_loss: 0.7767 - val_accuracy: 0.7116\n","Epoch 70/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7309 - accuracy: 0.7032 - val_loss: 0.7870 - val_accuracy: 0.7021\n","Epoch 71/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7157 - accuracy: 0.7188 - val_loss: 0.7809 - val_accuracy: 0.6950\n","Epoch 72/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7229 - accuracy: 0.7119 - val_loss: 0.8007 - val_accuracy: 0.6891\n","Epoch 73/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7092 - accuracy: 0.7129 - val_loss: 0.7958 - val_accuracy: 0.6933\n","Epoch 74/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7163 - accuracy: 0.7142 - val_loss: 0.7697 - val_accuracy: 0.7069\n","Epoch 75/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7144 - accuracy: 0.7125 - val_loss: 0.7856 - val_accuracy: 0.7128\n","Epoch 76/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7195 - accuracy: 0.7113 - val_loss: 0.7823 - val_accuracy: 0.7051\n","Epoch 77/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7066 - accuracy: 0.7190 - val_loss: 0.7811 - val_accuracy: 0.7021\n","Epoch 78/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7116 - accuracy: 0.7157 - val_loss: 0.7829 - val_accuracy: 0.7021\n","Epoch 79/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7033 - accuracy: 0.7247 - val_loss: 0.7802 - val_accuracy: 0.7015\n","Epoch 80/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7089 - accuracy: 0.7168 - val_loss: 0.7696 - val_accuracy: 0.7169\n","Epoch 81/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7148 - accuracy: 0.7215 - val_loss: 0.7959 - val_accuracy: 0.7051\n","Epoch 82/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7211 - accuracy: 0.7148 - val_loss: 0.7807 - val_accuracy: 0.7086\n","Epoch 83/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.6984 - accuracy: 0.7225 - val_loss: 0.7900 - val_accuracy: 0.7074\n","Epoch 84/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7036 - accuracy: 0.7194 - val_loss: 0.7775 - val_accuracy: 0.7175\n","Epoch 85/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7033 - accuracy: 0.7213 - val_loss: 0.7755 - val_accuracy: 0.7080\n","Epoch 86/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.6927 - accuracy: 0.7193 - val_loss: 0.7567 - val_accuracy: 0.7204\n","Epoch 87/100\n","106/106 [==============================] - 2s 15ms/step - loss: 0.6970 - accuracy: 0.7272 - val_loss: 0.7765 - val_accuracy: 0.7169\n","Epoch 88/100\n","106/106 [==============================] - 2s 15ms/step - loss: 0.7086 - accuracy: 0.7196 - val_loss: 0.7478 - val_accuracy: 0.7270\n","Epoch 89/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7042 - accuracy: 0.7206 - val_loss: 0.7644 - val_accuracy: 0.7323\n","Epoch 90/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.6919 - accuracy: 0.7309 - val_loss: 0.7870 - val_accuracy: 0.7122\n","Epoch 91/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.6894 - accuracy: 0.7302 - val_loss: 0.7637 - val_accuracy: 0.7175\n","Epoch 92/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.6875 - accuracy: 0.7253 - val_loss: 0.7626 - val_accuracy: 0.7134\n","Epoch 93/100\n","106/106 [==============================] - 2s 15ms/step - loss: 0.7071 - accuracy: 0.7289 - val_loss: 0.7661 - val_accuracy: 0.7199\n","Epoch 94/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.6801 - accuracy: 0.7292 - val_loss: 0.7773 - val_accuracy: 0.7051\n","Epoch 95/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7080 - accuracy: 0.7221 - val_loss: 0.7735 - val_accuracy: 0.7163\n","Epoch 96/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.6975 - accuracy: 0.7196 - val_loss: 0.7566 - val_accuracy: 0.7222\n","Epoch 97/100\n","106/106 [==============================] - 2s 15ms/step - loss: 0.6958 - accuracy: 0.7228 - val_loss: 0.7537 - val_accuracy: 0.7193\n","Epoch 98/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.6900 - accuracy: 0.7280 - val_loss: 0.7544 - val_accuracy: 0.7092\n","Epoch 99/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.6737 - accuracy: 0.7374 - val_loss: 0.7379 - val_accuracy: 0.7388\n","Epoch 100/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.6692 - accuracy: 0.7376 - val_loss: 0.7615 - val_accuracy: 0.7139\n"]}],"source":["learning_rate = 1e-3\n","epochs = 100\n","cnn_subject_model_optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n","\n","x_train, y_train, x_valid, y_valid, _, _ = preprocess_subjects(subject=2)\n","\n","# Building the CNN model using sequential class\n","cnn_subject_model = Sequential()\n","\n","# Conv. block 1\n","cnn_subject_model.add(Conv2D(filters=20, kernel_size=(5,1), padding='same', activation='elu', input_shape=(250,1,22)))\n","cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same')) \n","cnn_subject_model.add(BatchNormalization())\n","cnn_subject_model.add(Dropout(0.5))\n","\n","# Conv. block 2\n","cnn_subject_model.add(Conv2D(filters=20, kernel_size=(15,1), padding='same', activation='elu'))\n","cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n","cnn_subject_model.add(BatchNormalization())\n","cnn_subject_model.add(Dropout(0.5))\n","\n","# Conv. block 3\n","cnn_subject_model.add(Conv2D(filters=10, kernel_size=(10,1), padding='same', activation='elu'))\n","cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n","cnn_subject_model.add(BatchNormalization())\n","cnn_subject_model.add(Dropout(0.5))\n","\n","\n","# Output layer with Softmax activation\n","cnn_subject_model.add(Flatten()) # Flattens the input\n","cnn_subject_model.add(Dense(4, activation='softmax')) # Output FC layer with softmax activation"]},{"cell_type":"markdown","metadata":{"id":"tb4dCbGpGKR_"},"source":["#### Training\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zMrbBgE5GKR_"},"outputs":[],"source":["# Printing the model summary\n","# cnn_subject_model.summary()\n","cnn_subject_model.compile(loss='categorical_crossentropy',\n","                optimizer=cnn_subject_model_optimizer,\n","                metrics=['accuracy'])\n","\n","cnn_subject_model_results = cnn_subject_model.fit(x_train,\n","            y_train,\n","            batch_size=64,\n","            epochs=epochs,\n","            validation_data=(x_valid, y_valid), verbose=True)"]},{"cell_type":"markdown","metadata":{"id":"sCmBn-4FGKR_"},"source":["#### Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ANo1mkOGKR_","outputId":"ba5ce5df-9c11-4285-bd81-2d8c251eedf1"},"outputs":[{"name":"stdout","output_type":"stream","text":["7/7 [==============================] - 0s 3ms/step - loss: 0.8490 - accuracy: 0.6400\n","Test accuracy of the CNN model for subject 0: 0.6399999856948853\n","7/7 [==============================] - 0s 3ms/step - loss: 1.0048 - accuracy: 0.5600\n","Test accuracy of the CNN model for subject 1: 0.5600000023841858\n","7/7 [==============================] - 0s 3ms/step - loss: 0.5531 - accuracy: 0.7800\n","Test accuracy of the CNN model for subject 2: 0.7799999713897705\n","7/7 [==============================] - 0s 3ms/step - loss: 0.7179 - accuracy: 0.6800\n","Test accuracy of the CNN model for subject 3: 0.6800000071525574\n","6/6 [==============================] - 0s 4ms/step - loss: 0.6179 - accuracy: 0.8138\n","Test accuracy of the CNN model for subject 4: 0.813829779624939\n","7/7 [==============================] - 0s 3ms/step - loss: 0.7460 - accuracy: 0.7245\n","Test accuracy of the CNN model for subject 5: 0.7244898080825806\n","7/7 [==============================] - 0s 3ms/step - loss: 0.8005 - accuracy: 0.6750\n","Test accuracy of the CNN model for subject 6: 0.675000011920929\n","7/7 [==============================] - 0s 3ms/step - loss: 0.6560 - accuracy: 0.7300\n","Test accuracy of the CNN model for subject 7: 0.7300000190734863\n","6/6 [==============================] - 0s 4ms/step - loss: 0.6320 - accuracy: 0.7660\n","Test accuracy of the CNN model for subject 8: 0.7659574747085571\n"]}],"source":["subjects = 9\n","for subject in range(subjects):\n","    # tf.keras.backend.clear_session()\n","    _, _, _, _, x_test, y_test = preprocess_subjects(subject=subject)\n","    cnn_subject_model_score = cnn_subject_model.evaluate(x_test, y_test, verbose=True)\n","    print(f'Test accuracy of the CNN model for subject {subject}:',cnn_subject_model_score[1])\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fsTQ2joUGKSA"},"source":["## 3. Evaluate the classification accuracy as a function of time (e.g., does it increase as you have data over longer periods of time? how much time is required to get a reasonable classification accuracy?)"]},{"cell_type":"markdown","metadata":{"id":"fuBtpN55GKSA"},"source":["#### Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2iytyE4xkrSN"},"outputs":[],"source":["def data_prep_modular(X,y,sub_sample,average,noise, trim_ratio=0.5):\n","    \n","    total_X = None\n","    total_y = None\n","    \n","    # Trimming the data (sample,22,1000) -> (sample,22,500)\n","    X = X[:,:, 0:(int(X.shape[2] * trim_ratio))]\n","    print('Shape of X after trimming:',X.shape)\n","    \n","    # Maxpooling the data (sample,22,1000) -> (sample,22,500/sub_sample)\n","    X_max = np.max(X.reshape(X.shape[0], X.shape[1], -1, sub_sample), axis=3)\n","    \n","    \n","    total_X = X_max\n","    total_y = y\n","    # print('Shape of X after maxpooling:',total_X.shape)\n","    \n","    # Averaging + noise \n","    X_average = np.mean(X.reshape(X.shape[0], X.shape[1], -1, average), axis=3)\n","    X_average = X_average + np.random.normal(0.0, 0.5, X_average.shape)\n","    \n","    total_X = np.vstack((total_X, X_average))\n","    total_y = np.hstack((total_y, y))\n","    # print('Shape of X after averaging+noise and concatenating:',total_X.shape)\n","    \n","    # Subsampling\n","    \n","    for i in range(sub_sample):\n","        \n","        X_subsample = X[:, :, i::sub_sample] + (np.random.normal(0.0, 0.5, X[:, :,i::sub_sample].shape) if noise else 0.0)\n","        total_X = np.vstack((total_X, X_subsample))\n","        total_y = np.hstack((total_y, y))\n","        \n","    \n","    # print('Shape of X after subsampling and concatenating:',total_X.shape)\n","    return total_X,total_y\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wZl8T00ZkrSO"},"outputs":[],"source":["\n","def preprocess_time(trim_ratio):\n","    X_test = np.load(\"X_test.npy\")\n","    y_test = np.load(\"y_test.npy\")\n","    person_train_valid = np.load(\"person_train_valid.npy\")\n","    X_train_valid = np.load(\"X_train_valid.npy\")\n","    y_train_valid = np.load(\"y_train_valid.npy\")\n","    person_test = np.load(\"person_test.npy\")\n","\n","    ## Adjusting the labels so that \n","\n","    # Cue onset left - 0\n","    # Cue onset right - 1\n","    # Cue onset foot - 2\n","    # Cue onset tongue - 3\n","\n","    y_train_valid -= 769\n","    y_test -= 769\n","\n","\n","    # shuffle with 5 fold\n","    indicies_valid = np.random.choice(X_train_valid.shape[0], X_train_valid.shape[0] // 5, replace=False)\n","    indicies_train = np.array(list(set(range(X_train_valid.shape[0])).difference(set(indicies_valid))))\n","\n","    # Creating the training and validation sets using the generated indices\n","    X_train, X_valid = X_train_valid[indicies_train], X_train_valid[indicies_valid] \n","    y_train, y_valid = y_train_valid[indicies_train], y_train_valid[indicies_valid]\n","\n","\n","\n","    # Preprocessing the dataset\n","    x_train,y_train = data_prep_modular(X_train,y_train,2,2,True, trim_ratio=trim_ratio)\n","    x_valid,y_valid = data_prep_modular(X_valid,y_valid,2,2,True, trim_ratio=trim_ratio)\n","    X_test_prep,y_test_prep = data_prep_modular(X_test,y_test,2,2,True, trim_ratio=trim_ratio)\n","\n","    print('Shape of training set:',x_train.shape)\n","    print('Shape of validation set:',x_valid.shape)\n","    print('Shape of training labels:',y_train.shape)\n","    print('Shape of validation labels:',y_valid.shape)\n","    print('Shape of testing set:',X_test_prep.shape)\n","    print('Shape of testing labels:',y_test_prep.shape)\n","\n","    # Converting the labels to categorical variables for multiclass classification\n","    y_train = to_categorical(y_train, 4)\n","    y_valid = to_categorical(y_valid, 4)\n","    y_test = to_categorical(y_test_prep, 4)\n","    print('Shape of training labels after categorical conversion:',y_train.shape)\n","    print('Shape of validation labels after categorical conversion:',y_valid.shape)\n","    print('Shape of test labels after categorical conversion:',y_test.shape)\n","\n","    # Adding width of the segment to be 1\n","    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n","    x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)\n","    x_test = X_test_prep.reshape(X_test_prep.shape[0], X_test_prep.shape[1], X_test_prep.shape[2], 1)\n","    print('Shape of training set after adding width info:',x_train.shape)\n","    print('Shape of validation set after adding width info:',x_valid.shape)\n","    print('Shape of test set after adding width info:',x_test.shape)\n","\n","    # Reshaping the training and validation dataset\n","    x_train = np.swapaxes(x_train, 1,3)\n","    x_train = np.swapaxes(x_train, 1,2)\n","    x_valid = np.swapaxes(x_valid, 1,3)\n","    x_valid = np.swapaxes(x_valid, 1,2)\n","    x_test = np.swapaxes(x_test, 1,3)\n","    x_test = np.swapaxes(x_test, 1,2)\n","    print('Shape of training set after dimension reshaping:',x_train.shape)\n","    print('Shape of validation set after dimension reshaping:',x_valid.shape)\n","    print('Shape of test set after dimension reshaping:',x_test.shape)\n","\n","\n","    return (x_train, y_train, x_valid, y_valid, x_test, y_test)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EGpl_bA8GKSB"},"source":["#### Model / Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5otg3iW9GKSB"},"outputs":[],"source":["def cnn_model(trim_ratio):\n","    learning_rate = 1e-3\n","    epochs = 100\n","    cnn_subject_model_optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n","\n","    x_train, y_train, x_valid, y_valid, x_test, y_test = preprocess_time(trim_ratio=trim_ratio)\n","\n","    # Building the CNN model using sequential class\n","    cnn = Sequential()\n","\n","    # Conv. block 1\n","    cnn.add(Conv2D(filters=20, kernel_size=(5,1), padding='same', activation='elu', input_shape=(x_train.shape[1],1,22)))\n","    cnn.add(MaxPooling2D(pool_size=(3,1), padding='same')) \n","    cnn.add(BatchNormalization())\n","    cnn.add(Dropout(0.5))\n","\n","    # Conv. block 2\n","    cnn.add(Conv2D(filters=20, kernel_size=(15,1), padding='same', activation='elu'))\n","    cnn.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n","    cnn.add(BatchNormalization())\n","    cnn.add(Dropout(0.5))\n","\n","    # Conv. block 3\n","    cnn.add(Conv2D(filters=10, kernel_size=(10,1), padding='same', activation='elu'))\n","    cnn.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n","    cnn.add(BatchNormalization())\n","    cnn.add(Dropout(0.5))\n","\n","\n","    # Output layer with Softmax activation\n","    cnn.add(Flatten()) # Flattens the input\n","    cnn.add(Dense(4, activation='softmax')) # Output FC layer with softmax activation\n","\n","    cnn.compile(loss='categorical_crossentropy',\n","                    optimizer=cnn_subject_model_optimizer,\n","                    metrics=['accuracy'])\n","    \n","    cnn_training_results = cnn.fit(x_train,\n","            y_train,\n","            batch_size=64,\n","            epochs=epochs,\n","            validation_data=(x_valid, y_valid), verbose=True)\n","    \n","    cnn_test_score = cnn.evaluate(x_test, y_test, verbose=True)\n","    print(f'Test Accuracy: {cnn_test_score[1]}')\n","    return cnn, cnn_training_results, cnn_test_score\n","    "]},{"cell_type":"markdown","metadata":{"id":"oqGCBWToGKSC"},"source":["#### Training /Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t1woeXr_GKSC","outputId":"2b708d2d-12c8-4327-f095-f6e781505edb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of X after trimming: (1692, 22, 100)\n","Shape of X after trimming: (423, 22, 100)\n","Shape of X after trimming: (443, 22, 100)\n","Shape of training set: (6768, 22, 50)\n","Shape of validation set: (1692, 22, 50)\n","Shape of training labels: (6768,)\n","Shape of validation labels: (1692,)\n","Shape of testing set: (1772, 22, 50)\n","Shape of testing labels: (1772,)\n","Shape of training labels after categorical conversion: (6768, 4)\n","Shape of validation labels after categorical conversion: (1692, 4)\n","Shape of test labels after categorical conversion: (1772, 4)\n","Shape of training set after adding width info: (6768, 22, 50, 1)\n","Shape of validation set after adding width info: (1692, 22, 50, 1)\n","Shape of test set after adding width info: (1772, 22, 50, 1)\n","Shape of training set after dimension reshaping: (6768, 50, 1, 22)\n","Shape of validation set after dimension reshaping: (1692, 50, 1, 22)\n","Shape of test set after dimension reshaping: (1772, 50, 1, 22)\n","Epoch 1/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.9433 - accuracy: 0.2574 - val_loss: 1.4690 - val_accuracy: 0.3056\n","Epoch 2/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.6423 - accuracy: 0.2766 - val_loss: 1.3885 - val_accuracy: 0.3150\n","Epoch 3/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.5308 - accuracy: 0.2757 - val_loss: 1.3760 - val_accuracy: 0.3327\n","Epoch 4/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.4444 - accuracy: 0.2986 - val_loss: 1.3654 - val_accuracy: 0.3440\n","Epoch 5/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.4062 - accuracy: 0.3082 - val_loss: 1.3544 - val_accuracy: 0.3647\n","Epoch 6/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.3823 - accuracy: 0.3143 - val_loss: 1.3446 - val_accuracy: 0.3989\n","Epoch 7/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.3652 - accuracy: 0.3304 - val_loss: 1.3384 - val_accuracy: 0.3972\n","Epoch 8/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.3471 - accuracy: 0.3462 - val_loss: 1.3312 - val_accuracy: 0.3913\n","Epoch 9/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.3383 - accuracy: 0.3528 - val_loss: 1.3297 - val_accuracy: 0.3830\n","Epoch 10/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.3235 - accuracy: 0.3568 - val_loss: 1.3136 - val_accuracy: 0.4108\n","Epoch 11/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.3141 - accuracy: 0.3726 - val_loss: 1.3038 - val_accuracy: 0.4184\n","Epoch 12/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.3097 - accuracy: 0.3800 - val_loss: 1.2887 - val_accuracy: 0.4462\n","Epoch 13/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.2931 - accuracy: 0.3976 - val_loss: 1.3132 - val_accuracy: 0.3995\n","Epoch 14/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.2824 - accuracy: 0.4128 - val_loss: 1.2799 - val_accuracy: 0.4391\n","Epoch 15/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.2804 - accuracy: 0.4085 - val_loss: 1.2855 - val_accuracy: 0.4279\n","Epoch 16/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.2636 - accuracy: 0.4181 - val_loss: 1.2526 - val_accuracy: 0.4527\n","Epoch 17/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.2596 - accuracy: 0.4285 - val_loss: 1.2511 - val_accuracy: 0.4462\n","Epoch 18/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.2445 - accuracy: 0.4365 - val_loss: 1.2346 - val_accuracy: 0.4781\n","Epoch 19/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.2395 - accuracy: 0.4428 - val_loss: 1.3006 - val_accuracy: 0.3712\n","Epoch 20/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.2268 - accuracy: 0.4410 - val_loss: 1.2290 - val_accuracy: 0.4569\n","Epoch 21/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.2085 - accuracy: 0.4594 - val_loss: 1.2036 - val_accuracy: 0.5030\n","Epoch 22/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.1996 - accuracy: 0.4663 - val_loss: 1.1992 - val_accuracy: 0.4734\n","Epoch 23/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.1884 - accuracy: 0.4743 - val_loss: 1.2167 - val_accuracy: 0.4545\n","Epoch 24/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.1828 - accuracy: 0.4843 - val_loss: 1.1892 - val_accuracy: 0.4775\n","Epoch 25/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.1801 - accuracy: 0.4768 - val_loss: 1.1932 - val_accuracy: 0.4574\n","Epoch 26/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.1804 - accuracy: 0.4752 - val_loss: 1.1771 - val_accuracy: 0.4811\n","Epoch 27/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.1748 - accuracy: 0.4814 - val_loss: 1.2221 - val_accuracy: 0.4161\n","Epoch 28/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.1787 - accuracy: 0.4786 - val_loss: 1.1751 - val_accuracy: 0.4870\n","Epoch 29/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.1575 - accuracy: 0.4929 - val_loss: 1.1676 - val_accuracy: 0.4787\n","Epoch 30/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.1509 - accuracy: 0.4957 - val_loss: 1.1633 - val_accuracy: 0.4935\n","Epoch 31/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.1459 - accuracy: 0.4991 - val_loss: 1.1620 - val_accuracy: 0.4811\n","Epoch 32/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.1296 - accuracy: 0.5087 - val_loss: 1.1711 - val_accuracy: 0.4823\n","Epoch 33/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.1389 - accuracy: 0.5069 - val_loss: 1.1472 - val_accuracy: 0.4852\n","Epoch 34/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.1289 - accuracy: 0.5016 - val_loss: 1.1683 - val_accuracy: 0.4746\n","Epoch 35/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.1215 - accuracy: 0.5129 - val_loss: 1.1470 - val_accuracy: 0.4870\n","Epoch 36/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.1122 - accuracy: 0.5194 - val_loss: 1.1463 - val_accuracy: 0.4953\n","Epoch 37/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.1200 - accuracy: 0.5117 - val_loss: 1.1474 - val_accuracy: 0.4840\n","Epoch 38/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.1066 - accuracy: 0.5270 - val_loss: 1.1317 - val_accuracy: 0.5006\n","Epoch 39/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0975 - accuracy: 0.5263 - val_loss: 1.1683 - val_accuracy: 0.4704\n","Epoch 40/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.1025 - accuracy: 0.5291 - val_loss: 1.1390 - val_accuracy: 0.4823\n","Epoch 41/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.1035 - accuracy: 0.5300 - val_loss: 1.1184 - val_accuracy: 0.5083\n","Epoch 42/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.1009 - accuracy: 0.5232 - val_loss: 1.1209 - val_accuracy: 0.5100\n","Epoch 43/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.0917 - accuracy: 0.5313 - val_loss: 1.1379 - val_accuracy: 0.4870\n","Epoch 44/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.0845 - accuracy: 0.5328 - val_loss: 1.1255 - val_accuracy: 0.5171\n","Epoch 45/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.0835 - accuracy: 0.5341 - val_loss: 1.1206 - val_accuracy: 0.5118\n","Epoch 46/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.0727 - accuracy: 0.5452 - val_loss: 1.1265 - val_accuracy: 0.4976\n","Epoch 47/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0782 - accuracy: 0.5384 - val_loss: 1.0979 - val_accuracy: 0.5272\n","Epoch 48/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0777 - accuracy: 0.5424 - val_loss: 1.1129 - val_accuracy: 0.5118\n","Epoch 49/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0728 - accuracy: 0.5493 - val_loss: 1.1163 - val_accuracy: 0.5041\n","Epoch 50/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0645 - accuracy: 0.5526 - val_loss: 1.1221 - val_accuracy: 0.5112\n","Epoch 51/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0663 - accuracy: 0.5437 - val_loss: 1.1267 - val_accuracy: 0.4929\n","Epoch 52/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0578 - accuracy: 0.5533 - val_loss: 1.1019 - val_accuracy: 0.5154\n","Epoch 53/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0697 - accuracy: 0.5455 - val_loss: 1.1052 - val_accuracy: 0.5059\n","Epoch 54/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0699 - accuracy: 0.5400 - val_loss: 1.1382 - val_accuracy: 0.4923\n","Epoch 55/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0567 - accuracy: 0.5547 - val_loss: 1.1171 - val_accuracy: 0.5053\n","Epoch 56/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0513 - accuracy: 0.5569 - val_loss: 1.1184 - val_accuracy: 0.5165\n","Epoch 57/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.0529 - accuracy: 0.5559 - val_loss: 1.1182 - val_accuracy: 0.5089\n","Epoch 58/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0501 - accuracy: 0.5517 - val_loss: 1.1654 - val_accuracy: 0.4710\n","Epoch 59/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0483 - accuracy: 0.5553 - val_loss: 1.1414 - val_accuracy: 0.4846\n","Epoch 60/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0490 - accuracy: 0.5513 - val_loss: 1.1208 - val_accuracy: 0.5041\n","Epoch 61/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0549 - accuracy: 0.5567 - val_loss: 1.1446 - val_accuracy: 0.4894\n","Epoch 62/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.0432 - accuracy: 0.5647 - val_loss: 1.1218 - val_accuracy: 0.5053\n","Epoch 63/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0363 - accuracy: 0.5683 - val_loss: 1.1235 - val_accuracy: 0.5000\n","Epoch 64/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0375 - accuracy: 0.5643 - val_loss: 1.1021 - val_accuracy: 0.5124\n","Epoch 65/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0365 - accuracy: 0.5590 - val_loss: 1.0886 - val_accuracy: 0.5189\n","Epoch 66/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0301 - accuracy: 0.5624 - val_loss: 1.0872 - val_accuracy: 0.5219\n","Epoch 67/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.0359 - accuracy: 0.5601 - val_loss: 1.1593 - val_accuracy: 0.4722\n","Epoch 68/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0314 - accuracy: 0.5647 - val_loss: 1.1261 - val_accuracy: 0.5177\n","Epoch 69/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0289 - accuracy: 0.5615 - val_loss: 1.0935 - val_accuracy: 0.5236\n","Epoch 70/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0151 - accuracy: 0.5759 - val_loss: 1.1061 - val_accuracy: 0.5195\n","Epoch 71/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0211 - accuracy: 0.5672 - val_loss: 1.1118 - val_accuracy: 0.5071\n","Epoch 72/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0341 - accuracy: 0.5624 - val_loss: 1.1565 - val_accuracy: 0.4835\n","Epoch 73/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.0225 - accuracy: 0.5706 - val_loss: 1.1128 - val_accuracy: 0.5024\n","Epoch 74/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0159 - accuracy: 0.5696 - val_loss: 1.1209 - val_accuracy: 0.5071\n","Epoch 75/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0269 - accuracy: 0.5730 - val_loss: 1.0910 - val_accuracy: 0.5254\n","Epoch 76/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0000 - accuracy: 0.5799 - val_loss: 1.1501 - val_accuracy: 0.4911\n","Epoch 77/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0220 - accuracy: 0.5659 - val_loss: 1.1109 - val_accuracy: 0.5100\n","Epoch 78/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.0176 - accuracy: 0.5715 - val_loss: 1.0832 - val_accuracy: 0.5225\n","Epoch 79/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0198 - accuracy: 0.5718 - val_loss: 1.0912 - val_accuracy: 0.5313\n","Epoch 80/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0082 - accuracy: 0.5792 - val_loss: 1.1300 - val_accuracy: 0.4888\n","Epoch 81/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0090 - accuracy: 0.5771 - val_loss: 1.1055 - val_accuracy: 0.5248\n","Epoch 82/100\n","106/106 [==============================] - 0s 5ms/step - loss: 0.9893 - accuracy: 0.5885 - val_loss: 1.1195 - val_accuracy: 0.5041\n","Epoch 83/100\n","106/106 [==============================] - 1s 5ms/step - loss: 1.0258 - accuracy: 0.5660 - val_loss: 1.1326 - val_accuracy: 0.4941\n","Epoch 84/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0067 - accuracy: 0.5758 - val_loss: 1.0884 - val_accuracy: 0.5307\n","Epoch 85/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0132 - accuracy: 0.5720 - val_loss: 1.1262 - val_accuracy: 0.5000\n","Epoch 86/100\n","106/106 [==============================] - 0s 5ms/step - loss: 0.9881 - accuracy: 0.5830 - val_loss: 1.1308 - val_accuracy: 0.5024\n","Epoch 87/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0103 - accuracy: 0.5736 - val_loss: 1.1196 - val_accuracy: 0.5024\n","Epoch 88/100\n","106/106 [==============================] - 1s 5ms/step - loss: 0.9998 - accuracy: 0.5807 - val_loss: 1.1046 - val_accuracy: 0.5000\n","Epoch 89/100\n","106/106 [==============================] - 0s 5ms/step - loss: 0.9987 - accuracy: 0.5804 - val_loss: 1.0867 - val_accuracy: 0.5171\n","Epoch 90/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0066 - accuracy: 0.5822 - val_loss: 1.1125 - val_accuracy: 0.5154\n","Epoch 91/100\n","106/106 [==============================] - 0s 5ms/step - loss: 1.0027 - accuracy: 0.5783 - val_loss: 1.0963 - val_accuracy: 0.5219\n","Epoch 92/100\n","106/106 [==============================] - 0s 5ms/step - loss: 0.9954 - accuracy: 0.5792 - val_loss: 1.0833 - val_accuracy: 0.5307\n","Epoch 93/100\n","106/106 [==============================] - 1s 5ms/step - loss: 0.9828 - accuracy: 0.5819 - val_loss: 1.0999 - val_accuracy: 0.5189\n","Epoch 94/100\n","106/106 [==============================] - 0s 5ms/step - loss: 0.9964 - accuracy: 0.5731 - val_loss: 1.0985 - val_accuracy: 0.5171\n","Epoch 95/100\n","106/106 [==============================] - 0s 5ms/step - loss: 0.9965 - accuracy: 0.5885 - val_loss: 1.1119 - val_accuracy: 0.5030\n","Epoch 96/100\n","106/106 [==============================] - 0s 5ms/step - loss: 0.9786 - accuracy: 0.5903 - val_loss: 1.1191 - val_accuracy: 0.5189\n","Epoch 97/100\n","106/106 [==============================] - 0s 5ms/step - loss: 0.9980 - accuracy: 0.5805 - val_loss: 1.1292 - val_accuracy: 0.4959\n","Epoch 98/100\n","106/106 [==============================] - 1s 5ms/step - loss: 0.9896 - accuracy: 0.5795 - val_loss: 1.0989 - val_accuracy: 0.5361\n","Epoch 99/100\n","106/106 [==============================] - 0s 5ms/step - loss: 0.9884 - accuracy: 0.5900 - val_loss: 1.0810 - val_accuracy: 0.5301\n","Epoch 100/100\n","106/106 [==============================] - 1s 5ms/step - loss: 0.9782 - accuracy: 0.5906 - val_loss: 1.0877 - val_accuracy: 0.5296\n","56/56 [==============================] - 0s 1ms/step - loss: 1.0414 - accuracy: 0.5440\n","Shape of X after trimming: (1692, 22, 200)\n","Shape of X after trimming: (423, 22, 200)\n","Shape of X after trimming: (443, 22, 200)\n","Shape of training set: (6768, 22, 100)\n","Shape of validation set: (1692, 22, 100)\n","Shape of training labels: (6768,)\n","Shape of validation labels: (1692,)\n","Shape of testing set: (1772, 22, 100)\n","Shape of testing labels: (1772,)\n","Shape of training labels after categorical conversion: (6768, 4)\n","Shape of validation labels after categorical conversion: (1692, 4)\n","Shape of test labels after categorical conversion: (1772, 4)\n","Shape of training set after adding width info: (6768, 22, 100, 1)\n","Shape of validation set after adding width info: (1692, 22, 100, 1)\n","Shape of test set after adding width info: (1772, 22, 100, 1)\n","Shape of training set after dimension reshaping: (6768, 100, 1, 22)\n","Shape of validation set after dimension reshaping: (1692, 100, 1, 22)\n","Shape of test set after dimension reshaping: (1772, 100, 1, 22)\n","Epoch 1/100\n","106/106 [==============================] - 1s 7ms/step - loss: 1.8739 - accuracy: 0.2807 - val_loss: 1.4497 - val_accuracy: 0.3322\n","Epoch 2/100\n","106/106 [==============================] - 1s 7ms/step - loss: 1.5863 - accuracy: 0.2961 - val_loss: 1.3445 - val_accuracy: 0.3410\n","Epoch 3/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.4638 - accuracy: 0.3112 - val_loss: 1.3284 - val_accuracy: 0.3676\n","Epoch 4/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.3829 - accuracy: 0.3423 - val_loss: 1.3283 - val_accuracy: 0.3682\n","Epoch 5/100\n","106/106 [==============================] - 1s 7ms/step - loss: 1.3341 - accuracy: 0.3611 - val_loss: 1.3030 - val_accuracy: 0.3995\n","Epoch 6/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.3112 - accuracy: 0.3849 - val_loss: 1.2855 - val_accuracy: 0.4119\n","Epoch 7/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.2895 - accuracy: 0.4025 - val_loss: 1.2678 - val_accuracy: 0.4385\n","Epoch 8/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.2603 - accuracy: 0.4193 - val_loss: 1.2644 - val_accuracy: 0.4409\n","Epoch 9/100\n","106/106 [==============================] - 1s 7ms/step - loss: 1.2572 - accuracy: 0.4306 - val_loss: 1.2860 - val_accuracy: 0.4178\n","Epoch 10/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.2371 - accuracy: 0.4480 - val_loss: 1.2303 - val_accuracy: 0.4527\n","Epoch 11/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.2153 - accuracy: 0.4594 - val_loss: 1.2246 - val_accuracy: 0.4397\n","Epoch 12/100\n","106/106 [==============================] - 1s 7ms/step - loss: 1.2075 - accuracy: 0.4576 - val_loss: 1.2036 - val_accuracy: 0.4569\n","Epoch 13/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.1920 - accuracy: 0.4733 - val_loss: 1.1934 - val_accuracy: 0.4687\n","Epoch 14/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.1847 - accuracy: 0.4780 - val_loss: 1.1726 - val_accuracy: 0.4970\n","Epoch 15/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.1720 - accuracy: 0.4768 - val_loss: 1.1639 - val_accuracy: 0.5035\n","Epoch 16/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.1599 - accuracy: 0.4898 - val_loss: 1.1467 - val_accuracy: 0.5030\n","Epoch 17/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.1443 - accuracy: 0.5095 - val_loss: 1.1822 - val_accuracy: 0.4586\n","Epoch 18/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.1308 - accuracy: 0.5132 - val_loss: 1.1482 - val_accuracy: 0.4876\n","Epoch 19/100\n","106/106 [==============================] - 1s 7ms/step - loss: 1.1252 - accuracy: 0.5140 - val_loss: 1.1132 - val_accuracy: 0.5431\n","Epoch 20/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.1286 - accuracy: 0.5173 - val_loss: 1.1081 - val_accuracy: 0.5301\n","Epoch 21/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.1043 - accuracy: 0.5270 - val_loss: 1.0936 - val_accuracy: 0.5307\n","Epoch 22/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.0910 - accuracy: 0.5414 - val_loss: 1.1091 - val_accuracy: 0.4982\n","Epoch 23/100\n","106/106 [==============================] - 1s 7ms/step - loss: 1.0925 - accuracy: 0.5369 - val_loss: 1.0848 - val_accuracy: 0.5225\n","Epoch 24/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.0805 - accuracy: 0.5443 - val_loss: 1.0810 - val_accuracy: 0.5307\n","Epoch 25/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.0724 - accuracy: 0.5483 - val_loss: 1.0877 - val_accuracy: 0.5183\n","Epoch 26/100\n","106/106 [==============================] - 1s 7ms/step - loss: 1.0652 - accuracy: 0.5542 - val_loss: 1.0898 - val_accuracy: 0.5095\n","Epoch 27/100\n","106/106 [==============================] - 1s 7ms/step - loss: 1.0521 - accuracy: 0.5536 - val_loss: 1.0719 - val_accuracy: 0.5402\n","Epoch 28/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.0469 - accuracy: 0.5671 - val_loss: 1.0899 - val_accuracy: 0.5077\n","Epoch 29/100\n","106/106 [==============================] - 1s 7ms/step - loss: 1.0442 - accuracy: 0.5606 - val_loss: 1.0606 - val_accuracy: 0.5337\n","Epoch 30/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.0364 - accuracy: 0.5709 - val_loss: 1.0643 - val_accuracy: 0.5461\n","Epoch 31/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.0322 - accuracy: 0.5697 - val_loss: 1.0729 - val_accuracy: 0.5366\n","Epoch 32/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.0301 - accuracy: 0.5788 - val_loss: 1.0512 - val_accuracy: 0.5278\n","Epoch 33/100\n","106/106 [==============================] - 1s 7ms/step - loss: 1.0258 - accuracy: 0.5748 - val_loss: 1.0426 - val_accuracy: 0.5313\n","Epoch 34/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.0061 - accuracy: 0.5770 - val_loss: 1.0644 - val_accuracy: 0.5219\n","Epoch 35/100\n","106/106 [==============================] - 1s 6ms/step - loss: 1.0007 - accuracy: 0.5900 - val_loss: 1.0370 - val_accuracy: 0.5561\n","Epoch 36/100\n","106/106 [==============================] - 1s 7ms/step - loss: 1.0028 - accuracy: 0.5850 - val_loss: 1.0198 - val_accuracy: 0.5626\n","Epoch 37/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.9978 - accuracy: 0.5817 - val_loss: 1.0413 - val_accuracy: 0.5550\n","Epoch 38/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.9871 - accuracy: 0.5960 - val_loss: 1.0333 - val_accuracy: 0.5455\n","Epoch 39/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.9777 - accuracy: 0.5953 - val_loss: 1.0353 - val_accuracy: 0.5626\n","Epoch 40/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.9754 - accuracy: 0.5996 - val_loss: 1.0375 - val_accuracy: 0.5544\n","Epoch 41/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.9807 - accuracy: 0.5919 - val_loss: 1.0347 - val_accuracy: 0.5609\n","Epoch 42/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.9821 - accuracy: 0.5830 - val_loss: 1.0303 - val_accuracy: 0.5691\n","Epoch 43/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.9745 - accuracy: 0.5978 - val_loss: 1.0076 - val_accuracy: 0.5579\n","Epoch 44/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.9768 - accuracy: 0.5997 - val_loss: 1.0099 - val_accuracy: 0.5479\n","Epoch 45/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.9820 - accuracy: 0.5937 - val_loss: 0.9978 - val_accuracy: 0.5615\n","Epoch 46/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.9649 - accuracy: 0.6012 - val_loss: 0.9972 - val_accuracy: 0.5745\n","Epoch 47/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.9614 - accuracy: 0.6028 - val_loss: 0.9994 - val_accuracy: 0.5875\n","Epoch 48/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.9329 - accuracy: 0.6200 - val_loss: 0.9997 - val_accuracy: 0.5751\n","Epoch 49/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.9579 - accuracy: 0.6071 - val_loss: 0.9804 - val_accuracy: 0.5869\n","Epoch 50/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.9495 - accuracy: 0.6116 - val_loss: 0.9885 - val_accuracy: 0.5804\n","Epoch 51/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.9569 - accuracy: 0.6045 - val_loss: 0.9670 - val_accuracy: 0.5816\n","Epoch 52/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.9424 - accuracy: 0.6210 - val_loss: 0.9707 - val_accuracy: 0.6070\n","Epoch 53/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.9495 - accuracy: 0.6104 - val_loss: 0.9779 - val_accuracy: 0.5940\n","Epoch 54/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.9451 - accuracy: 0.6104 - val_loss: 0.9770 - val_accuracy: 0.5827\n","Epoch 55/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.9370 - accuracy: 0.6136 - val_loss: 0.9720 - val_accuracy: 0.6028\n","Epoch 56/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.9424 - accuracy: 0.6113 - val_loss: 0.9997 - val_accuracy: 0.5786\n","Epoch 57/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.9350 - accuracy: 0.6200 - val_loss: 0.9597 - val_accuracy: 0.6070\n","Epoch 58/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.9352 - accuracy: 0.6175 - val_loss: 0.9853 - val_accuracy: 0.5922\n","Epoch 59/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.9216 - accuracy: 0.6163 - val_loss: 0.9991 - val_accuracy: 0.5686\n","Epoch 60/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.9219 - accuracy: 0.6192 - val_loss: 0.9866 - val_accuracy: 0.5934\n","Epoch 61/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.9307 - accuracy: 0.6152 - val_loss: 0.9734 - val_accuracy: 0.6058\n","Epoch 62/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.9169 - accuracy: 0.6226 - val_loss: 0.9790 - val_accuracy: 0.5969\n","Epoch 63/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.9312 - accuracy: 0.6215 - val_loss: 0.9816 - val_accuracy: 0.5804\n","Epoch 64/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.9197 - accuracy: 0.6288 - val_loss: 0.9570 - val_accuracy: 0.5963\n","Epoch 65/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.9147 - accuracy: 0.6260 - val_loss: 0.9659 - val_accuracy: 0.5981\n","Epoch 66/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.9170 - accuracy: 0.6293 - val_loss: 0.9557 - val_accuracy: 0.5946\n","Epoch 67/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.9209 - accuracy: 0.6184 - val_loss: 0.9643 - val_accuracy: 0.5963\n","Epoch 68/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.9349 - accuracy: 0.6132 - val_loss: 0.9858 - val_accuracy: 0.5904\n","Epoch 69/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.9151 - accuracy: 0.6246 - val_loss: 0.9567 - val_accuracy: 0.6076\n","Epoch 70/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.9055 - accuracy: 0.6240 - val_loss: 0.9712 - val_accuracy: 0.5987\n","Epoch 71/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.9168 - accuracy: 0.6278 - val_loss: 0.9523 - val_accuracy: 0.6052\n","Epoch 72/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.9160 - accuracy: 0.6232 - val_loss: 0.9571 - val_accuracy: 0.6082\n","Epoch 73/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.9034 - accuracy: 0.6302 - val_loss: 0.9609 - val_accuracy: 0.6117\n","Epoch 74/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.8916 - accuracy: 0.6386 - val_loss: 0.9559 - val_accuracy: 0.6046\n","Epoch 75/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.8969 - accuracy: 0.6331 - val_loss: 0.9602 - val_accuracy: 0.5881\n","Epoch 76/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.8914 - accuracy: 0.6376 - val_loss: 0.9694 - val_accuracy: 0.5892\n","Epoch 77/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.8887 - accuracy: 0.6389 - val_loss: 0.9348 - val_accuracy: 0.6099\n","Epoch 78/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.8928 - accuracy: 0.6325 - val_loss: 0.9690 - val_accuracy: 0.5987\n","Epoch 79/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.8928 - accuracy: 0.6415 - val_loss: 0.9542 - val_accuracy: 0.6017\n","Epoch 80/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.8968 - accuracy: 0.6325 - val_loss: 0.9606 - val_accuracy: 0.6082\n","Epoch 81/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.8978 - accuracy: 0.6325 - val_loss: 0.9528 - val_accuracy: 0.6070\n","Epoch 82/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.9020 - accuracy: 0.6305 - val_loss: 0.9623 - val_accuracy: 0.6034\n","Epoch 83/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.8971 - accuracy: 0.6361 - val_loss: 0.9426 - val_accuracy: 0.6105\n","Epoch 84/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.8929 - accuracy: 0.6411 - val_loss: 0.9290 - val_accuracy: 0.6271\n","Epoch 85/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.8799 - accuracy: 0.6463 - val_loss: 0.9668 - val_accuracy: 0.6099\n","Epoch 86/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.8864 - accuracy: 0.6413 - val_loss: 1.0036 - val_accuracy: 0.5922\n","Epoch 87/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.8926 - accuracy: 0.6331 - val_loss: 0.9482 - val_accuracy: 0.6147\n","Epoch 88/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.8783 - accuracy: 0.6382 - val_loss: 0.9556 - val_accuracy: 0.6087\n","Epoch 89/100\n","106/106 [==============================] - 1s 8ms/step - loss: 0.8799 - accuracy: 0.6435 - val_loss: 0.9443 - val_accuracy: 0.6052\n","Epoch 90/100\n","106/106 [==============================] - 1s 8ms/step - loss: 0.8889 - accuracy: 0.6325 - val_loss: 0.9266 - val_accuracy: 0.6277\n","Epoch 91/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.8908 - accuracy: 0.6356 - val_loss: 0.9402 - val_accuracy: 0.6188\n","Epoch 92/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.8879 - accuracy: 0.6485 - val_loss: 0.9700 - val_accuracy: 0.6105\n","Epoch 93/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.8767 - accuracy: 0.6478 - val_loss: 0.9456 - val_accuracy: 0.6182\n","Epoch 94/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.8818 - accuracy: 0.6424 - val_loss: 0.9326 - val_accuracy: 0.6265\n","Epoch 95/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.8702 - accuracy: 0.6438 - val_loss: 0.9348 - val_accuracy: 0.6111\n","Epoch 96/100\n","106/106 [==============================] - 1s 6ms/step - loss: 0.8821 - accuracy: 0.6424 - val_loss: 0.9371 - val_accuracy: 0.6259\n","Epoch 97/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.8610 - accuracy: 0.6506 - val_loss: 0.9544 - val_accuracy: 0.6087\n","Epoch 98/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.8787 - accuracy: 0.6451 - val_loss: 0.9671 - val_accuracy: 0.6170\n","Epoch 99/100\n","106/106 [==============================] - 1s 8ms/step - loss: 0.8778 - accuracy: 0.6417 - val_loss: 0.9491 - val_accuracy: 0.6117\n","Epoch 100/100\n","106/106 [==============================] - 1s 7ms/step - loss: 0.8708 - accuracy: 0.6469 - val_loss: 0.9327 - val_accuracy: 0.6259\n","56/56 [==============================] - 0s 2ms/step - loss: 0.9075 - accuracy: 0.6247\n","Shape of X after trimming: (1692, 22, 300)\n","Shape of X after trimming: (423, 22, 300)\n","Shape of X after trimming: (443, 22, 300)\n","Shape of training set: (6768, 22, 150)\n","Shape of validation set: (1692, 22, 150)\n","Shape of training labels: (6768,)\n","Shape of validation labels: (1692,)\n","Shape of testing set: (1772, 22, 150)\n","Shape of testing labels: (1772,)\n","Shape of training labels after categorical conversion: (6768, 4)\n","Shape of validation labels after categorical conversion: (1692, 4)\n","Shape of test labels after categorical conversion: (1772, 4)\n","Shape of training set after adding width info: (6768, 22, 150, 1)\n","Shape of validation set after adding width info: (1692, 22, 150, 1)\n","Shape of test set after adding width info: (1772, 22, 150, 1)\n","Shape of training set after dimension reshaping: (6768, 150, 1, 22)\n","Shape of validation set after dimension reshaping: (1692, 150, 1, 22)\n","Shape of test set after dimension reshaping: (1772, 150, 1, 22)\n","Epoch 1/100\n","106/106 [==============================] - 2s 11ms/step - loss: 1.9796 - accuracy: 0.2639 - val_loss: 1.4382 - val_accuracy: 0.3050\n","Epoch 2/100\n","106/106 [==============================] - 1s 10ms/step - loss: 1.6357 - accuracy: 0.2920 - val_loss: 1.3484 - val_accuracy: 0.3286\n","Epoch 3/100\n","106/106 [==============================] - 1s 10ms/step - loss: 1.4858 - accuracy: 0.3051 - val_loss: 1.3252 - val_accuracy: 0.3670\n","Epoch 4/100\n","106/106 [==============================] - 1s 10ms/step - loss: 1.3883 - accuracy: 0.3369 - val_loss: 1.3081 - val_accuracy: 0.3735\n","Epoch 5/100\n","106/106 [==============================] - 1s 9ms/step - loss: 1.3392 - accuracy: 0.3610 - val_loss: 1.2765 - val_accuracy: 0.4066\n","Epoch 6/100\n","106/106 [==============================] - 1s 9ms/step - loss: 1.2972 - accuracy: 0.3874 - val_loss: 1.2506 - val_accuracy: 0.4433\n","Epoch 7/100\n","106/106 [==============================] - 1s 9ms/step - loss: 1.2641 - accuracy: 0.4048 - val_loss: 1.2044 - val_accuracy: 0.4710\n","Epoch 8/100\n","106/106 [==============================] - 1s 9ms/step - loss: 1.2397 - accuracy: 0.4356 - val_loss: 1.1901 - val_accuracy: 0.4634\n","Epoch 9/100\n","106/106 [==============================] - 1s 9ms/step - loss: 1.2219 - accuracy: 0.4400 - val_loss: 1.1719 - val_accuracy: 0.5065\n","Epoch 10/100\n","106/106 [==============================] - 1s 9ms/step - loss: 1.1936 - accuracy: 0.4694 - val_loss: 1.1410 - val_accuracy: 0.5260\n","Epoch 11/100\n","106/106 [==============================] - 1s 9ms/step - loss: 1.1713 - accuracy: 0.4874 - val_loss: 1.1162 - val_accuracy: 0.5313\n","Epoch 12/100\n","106/106 [==============================] - 1s 9ms/step - loss: 1.1623 - accuracy: 0.4913 - val_loss: 1.0693 - val_accuracy: 0.5496\n","Epoch 13/100\n","106/106 [==============================] - 1s 9ms/step - loss: 1.1482 - accuracy: 0.5010 - val_loss: 1.0740 - val_accuracy: 0.5556\n","Epoch 14/100\n","106/106 [==============================] - 1s 9ms/step - loss: 1.1275 - accuracy: 0.5084 - val_loss: 1.0567 - val_accuracy: 0.5703\n","Epoch 15/100\n","106/106 [==============================] - 1s 9ms/step - loss: 1.1247 - accuracy: 0.5130 - val_loss: 1.0498 - val_accuracy: 0.5561\n","Epoch 16/100\n","106/106 [==============================] - 1s 9ms/step - loss: 1.1060 - accuracy: 0.5155 - val_loss: 1.0232 - val_accuracy: 0.5816\n","Epoch 17/100\n","106/106 [==============================] - 1s 9ms/step - loss: 1.0881 - accuracy: 0.5327 - val_loss: 1.0191 - val_accuracy: 0.5703\n","Epoch 18/100\n","106/106 [==============================] - 1s 9ms/step - loss: 1.0801 - accuracy: 0.5366 - val_loss: 0.9961 - val_accuracy: 0.6064\n","Epoch 19/100\n","106/106 [==============================] - 1s 9ms/step - loss: 1.0665 - accuracy: 0.5442 - val_loss: 0.9847 - val_accuracy: 0.5963\n","Epoch 20/100\n","106/106 [==============================] - 1s 9ms/step - loss: 1.0538 - accuracy: 0.5526 - val_loss: 0.9671 - val_accuracy: 0.6306\n","Epoch 21/100\n","106/106 [==============================] - 1s 9ms/step - loss: 1.0504 - accuracy: 0.5569 - val_loss: 0.9677 - val_accuracy: 0.6212\n","Epoch 22/100\n","106/106 [==============================] - 1s 9ms/step - loss: 1.0391 - accuracy: 0.5641 - val_loss: 0.9787 - val_accuracy: 0.6099\n","Epoch 23/100\n","106/106 [==============================] - 1s 9ms/step - loss: 1.0320 - accuracy: 0.5669 - val_loss: 0.9632 - val_accuracy: 0.6277\n","Epoch 24/100\n","106/106 [==============================] - 1s 9ms/step - loss: 1.0281 - accuracy: 0.5649 - val_loss: 0.9614 - val_accuracy: 0.6123\n","Epoch 25/100\n","106/106 [==============================] - 1s 9ms/step - loss: 1.0090 - accuracy: 0.5749 - val_loss: 0.9280 - val_accuracy: 0.6253\n","Epoch 26/100\n","106/106 [==============================] - 1s 9ms/step - loss: 1.0153 - accuracy: 0.5754 - val_loss: 0.9373 - val_accuracy: 0.6223\n","Epoch 27/100\n","106/106 [==============================] - 1s 9ms/step - loss: 1.0029 - accuracy: 0.5786 - val_loss: 0.9278 - val_accuracy: 0.6430\n","Epoch 28/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.9821 - accuracy: 0.5861 - val_loss: 0.9106 - val_accuracy: 0.6472\n","Epoch 29/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.9802 - accuracy: 0.5910 - val_loss: 0.9283 - val_accuracy: 0.6265\n","Epoch 30/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.9729 - accuracy: 0.5981 - val_loss: 0.9141 - val_accuracy: 0.6241\n","Epoch 31/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.9650 - accuracy: 0.6020 - val_loss: 0.9142 - val_accuracy: 0.6223\n","Epoch 32/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.9559 - accuracy: 0.6020 - val_loss: 0.8764 - val_accuracy: 0.6460\n","Epoch 33/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.9528 - accuracy: 0.6071 - val_loss: 0.8951 - val_accuracy: 0.6401\n","Epoch 34/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.9482 - accuracy: 0.6037 - val_loss: 0.8878 - val_accuracy: 0.6466\n","Epoch 35/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.9464 - accuracy: 0.6068 - val_loss: 0.9268 - val_accuracy: 0.6164\n","Epoch 36/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.9273 - accuracy: 0.6135 - val_loss: 0.8824 - val_accuracy: 0.6472\n","Epoch 37/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.9303 - accuracy: 0.6127 - val_loss: 0.8987 - val_accuracy: 0.6306\n","Epoch 38/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.9260 - accuracy: 0.6200 - val_loss: 0.8675 - val_accuracy: 0.6560\n","Epoch 39/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.9041 - accuracy: 0.6284 - val_loss: 0.8649 - val_accuracy: 0.6501\n","Epoch 40/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.9203 - accuracy: 0.6219 - val_loss: 0.8457 - val_accuracy: 0.6596\n","Epoch 41/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.9110 - accuracy: 0.6263 - val_loss: 0.8629 - val_accuracy: 0.6554\n","Epoch 42/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.8911 - accuracy: 0.6452 - val_loss: 0.8460 - val_accuracy: 0.6548\n","Epoch 43/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.9145 - accuracy: 0.6275 - val_loss: 0.8364 - val_accuracy: 0.6637\n","Epoch 44/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.8915 - accuracy: 0.6420 - val_loss: 0.8226 - val_accuracy: 0.6773\n","Epoch 45/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.8906 - accuracy: 0.6343 - val_loss: 0.8603 - val_accuracy: 0.6519\n","Epoch 46/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.8826 - accuracy: 0.6398 - val_loss: 0.8718 - val_accuracy: 0.6454\n","Epoch 47/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.8898 - accuracy: 0.6387 - val_loss: 0.8401 - val_accuracy: 0.6720\n","Epoch 48/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.8646 - accuracy: 0.6492 - val_loss: 0.8553 - val_accuracy: 0.6537\n","Epoch 49/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.8702 - accuracy: 0.6486 - val_loss: 0.8186 - val_accuracy: 0.6755\n","Epoch 50/100\n","106/106 [==============================] - 1s 10ms/step - loss: 0.8603 - accuracy: 0.6513 - val_loss: 0.8328 - val_accuracy: 0.6749\n","Epoch 51/100\n","106/106 [==============================] - 1s 10ms/step - loss: 0.8580 - accuracy: 0.6520 - val_loss: 0.8189 - val_accuracy: 0.6743\n","Epoch 52/100\n","106/106 [==============================] - 1s 10ms/step - loss: 0.8634 - accuracy: 0.6498 - val_loss: 0.8080 - val_accuracy: 0.6856\n","Epoch 53/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.8583 - accuracy: 0.6473 - val_loss: 0.8314 - val_accuracy: 0.6625\n","Epoch 54/100\n","106/106 [==============================] - 1s 10ms/step - loss: 0.8645 - accuracy: 0.6483 - val_loss: 0.8274 - val_accuracy: 0.6749\n","Epoch 55/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.8655 - accuracy: 0.6461 - val_loss: 0.8220 - val_accuracy: 0.6749\n","Epoch 56/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.8648 - accuracy: 0.6463 - val_loss: 0.8268 - val_accuracy: 0.6743\n","Epoch 57/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.8512 - accuracy: 0.6528 - val_loss: 0.8033 - val_accuracy: 0.6820\n","Epoch 58/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.8494 - accuracy: 0.6585 - val_loss: 0.8005 - val_accuracy: 0.6891\n","Epoch 59/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.8350 - accuracy: 0.6611 - val_loss: 0.8072 - val_accuracy: 0.6755\n","Epoch 60/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.8338 - accuracy: 0.6622 - val_loss: 0.8142 - val_accuracy: 0.6868\n","Epoch 61/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.8362 - accuracy: 0.6600 - val_loss: 0.7907 - val_accuracy: 0.6927\n","Epoch 62/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.8425 - accuracy: 0.6619 - val_loss: 0.8226 - val_accuracy: 0.6844\n","Epoch 63/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.8414 - accuracy: 0.6646 - val_loss: 0.7942 - val_accuracy: 0.6927\n","Epoch 64/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.8205 - accuracy: 0.6742 - val_loss: 0.8042 - val_accuracy: 0.6791\n","Epoch 65/100\n","106/106 [==============================] - 1s 10ms/step - loss: 0.8356 - accuracy: 0.6667 - val_loss: 0.7933 - val_accuracy: 0.6974\n","Epoch 66/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.8210 - accuracy: 0.6670 - val_loss: 0.7949 - val_accuracy: 0.6844\n","Epoch 67/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.8141 - accuracy: 0.6721 - val_loss: 0.7833 - val_accuracy: 0.6868\n","Epoch 68/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.8136 - accuracy: 0.6650 - val_loss: 0.7858 - val_accuracy: 0.6767\n","Epoch 69/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.8020 - accuracy: 0.6741 - val_loss: 0.7827 - val_accuracy: 0.6897\n","Epoch 70/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.8224 - accuracy: 0.6665 - val_loss: 0.7837 - val_accuracy: 0.6885\n","Epoch 71/100\n","106/106 [==============================] - 1s 10ms/step - loss: 0.8234 - accuracy: 0.6690 - val_loss: 0.7843 - val_accuracy: 0.6998\n","Epoch 72/100\n","106/106 [==============================] - 1s 12ms/step - loss: 0.8143 - accuracy: 0.6686 - val_loss: 0.8077 - val_accuracy: 0.6749\n","Epoch 73/100\n","106/106 [==============================] - 1s 10ms/step - loss: 0.8154 - accuracy: 0.6670 - val_loss: 0.7818 - val_accuracy: 0.6956\n","Epoch 74/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.8045 - accuracy: 0.6760 - val_loss: 0.7918 - val_accuracy: 0.6797\n","Epoch 75/100\n","106/106 [==============================] - 1s 10ms/step - loss: 0.8250 - accuracy: 0.6745 - val_loss: 0.8056 - val_accuracy: 0.6690\n","Epoch 76/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.8210 - accuracy: 0.6665 - val_loss: 0.7856 - val_accuracy: 0.6915\n","Epoch 77/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.7973 - accuracy: 0.6823 - val_loss: 0.7675 - val_accuracy: 0.6980\n","Epoch 78/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.8115 - accuracy: 0.6732 - val_loss: 0.7813 - val_accuracy: 0.6921\n","Epoch 79/100\n","106/106 [==============================] - 1s 10ms/step - loss: 0.8102 - accuracy: 0.6704 - val_loss: 0.7801 - val_accuracy: 0.6950\n","Epoch 80/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7978 - accuracy: 0.6751 - val_loss: 0.7685 - val_accuracy: 0.7027\n","Epoch 81/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.7960 - accuracy: 0.6834 - val_loss: 0.8011 - val_accuracy: 0.6708\n","Epoch 82/100\n","106/106 [==============================] - 1s 10ms/step - loss: 0.7863 - accuracy: 0.6866 - val_loss: 0.7809 - val_accuracy: 0.7045\n","Epoch 83/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.8109 - accuracy: 0.6712 - val_loss: 0.7683 - val_accuracy: 0.6927\n","Epoch 84/100\n","106/106 [==============================] - 1s 10ms/step - loss: 0.8000 - accuracy: 0.6809 - val_loss: 0.7728 - val_accuracy: 0.6998\n","Epoch 85/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.8140 - accuracy: 0.6767 - val_loss: 0.7617 - val_accuracy: 0.7074\n","Epoch 86/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.7919 - accuracy: 0.6866 - val_loss: 0.7630 - val_accuracy: 0.6998\n","Epoch 87/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.7941 - accuracy: 0.6775 - val_loss: 0.7826 - val_accuracy: 0.6962\n","Epoch 88/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.7999 - accuracy: 0.6789 - val_loss: 0.7774 - val_accuracy: 0.6921\n","Epoch 89/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7920 - accuracy: 0.6837 - val_loss: 0.7859 - val_accuracy: 0.6868\n","Epoch 90/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.7946 - accuracy: 0.6760 - val_loss: 0.7766 - val_accuracy: 0.7009\n","Epoch 91/100\n","106/106 [==============================] - 1s 10ms/step - loss: 0.7906 - accuracy: 0.6903 - val_loss: 0.7689 - val_accuracy: 0.6909\n","Epoch 92/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.7758 - accuracy: 0.6933 - val_loss: 0.7862 - val_accuracy: 0.6915\n","Epoch 93/100\n","106/106 [==============================] - 1s 10ms/step - loss: 0.7881 - accuracy: 0.6847 - val_loss: 0.7511 - val_accuracy: 0.7092\n","Epoch 94/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.7839 - accuracy: 0.6859 - val_loss: 0.7665 - val_accuracy: 0.7009\n","Epoch 95/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.7973 - accuracy: 0.6838 - val_loss: 0.7555 - val_accuracy: 0.7051\n","Epoch 96/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.7805 - accuracy: 0.6912 - val_loss: 0.7954 - val_accuracy: 0.6826\n","Epoch 97/100\n","106/106 [==============================] - 1s 10ms/step - loss: 0.7876 - accuracy: 0.6854 - val_loss: 0.7958 - val_accuracy: 0.6755\n","Epoch 98/100\n","106/106 [==============================] - 1s 9ms/step - loss: 0.7729 - accuracy: 0.6928 - val_loss: 0.7545 - val_accuracy: 0.7021\n","Epoch 99/100\n","106/106 [==============================] - 1s 10ms/step - loss: 0.7766 - accuracy: 0.6881 - val_loss: 0.7565 - val_accuracy: 0.7009\n","Epoch 100/100\n","106/106 [==============================] - 1s 10ms/step - loss: 0.7785 - accuracy: 0.6941 - val_loss: 0.7599 - val_accuracy: 0.6968\n","56/56 [==============================] - 0s 2ms/step - loss: 0.8021 - accuracy: 0.6721\n","Shape of X after trimming: (1692, 22, 400)\n","Shape of X after trimming: (423, 22, 400)\n","Shape of X after trimming: (443, 22, 400)\n","Shape of training set: (6768, 22, 200)\n","Shape of validation set: (1692, 22, 200)\n","Shape of training labels: (6768,)\n","Shape of validation labels: (1692,)\n","Shape of testing set: (1772, 22, 200)\n","Shape of testing labels: (1772,)\n","Shape of training labels after categorical conversion: (6768, 4)\n","Shape of validation labels after categorical conversion: (1692, 4)\n","Shape of test labels after categorical conversion: (1772, 4)\n","Shape of training set after adding width info: (6768, 22, 200, 1)\n","Shape of validation set after adding width info: (1692, 22, 200, 1)\n","Shape of test set after adding width info: (1772, 22, 200, 1)\n","Shape of training set after dimension reshaping: (6768, 200, 1, 22)\n","Shape of validation set after dimension reshaping: (1692, 200, 1, 22)\n","Shape of test set after dimension reshaping: (1772, 200, 1, 22)\n","Epoch 1/100\n","106/106 [==============================] - 2s 12ms/step - loss: 1.9566 - accuracy: 0.2834 - val_loss: 1.3897 - val_accuracy: 0.3481\n","Epoch 2/100\n","106/106 [==============================] - 1s 11ms/step - loss: 1.5718 - accuracy: 0.3146 - val_loss: 1.2945 - val_accuracy: 0.3913\n","Epoch 3/100\n","106/106 [==============================] - 1s 12ms/step - loss: 1.4128 - accuracy: 0.3503 - val_loss: 1.2696 - val_accuracy: 0.4066\n","Epoch 4/100\n","106/106 [==============================] - 1s 12ms/step - loss: 1.3222 - accuracy: 0.3842 - val_loss: 1.2215 - val_accuracy: 0.4917\n","Epoch 5/100\n","106/106 [==============================] - 1s 11ms/step - loss: 1.2634 - accuracy: 0.4116 - val_loss: 1.1949 - val_accuracy: 0.4728\n","Epoch 6/100\n","106/106 [==============================] - 1s 12ms/step - loss: 1.2183 - accuracy: 0.4421 - val_loss: 1.1823 - val_accuracy: 0.5018\n","Epoch 7/100\n","106/106 [==============================] - 2s 16ms/step - loss: 1.1945 - accuracy: 0.4523 - val_loss: 1.1804 - val_accuracy: 0.4781\n","Epoch 8/100\n","106/106 [==============================] - 1s 12ms/step - loss: 1.1849 - accuracy: 0.4666 - val_loss: 1.1664 - val_accuracy: 0.4965\n","Epoch 9/100\n","106/106 [==============================] - 1s 12ms/step - loss: 1.1776 - accuracy: 0.4713 - val_loss: 1.1642 - val_accuracy: 0.4900\n","Epoch 10/100\n","106/106 [==============================] - 1s 13ms/step - loss: 1.1570 - accuracy: 0.4805 - val_loss: 1.1627 - val_accuracy: 0.4840\n","Epoch 11/100\n","106/106 [==============================] - 1s 12ms/step - loss: 1.1326 - accuracy: 0.4991 - val_loss: 1.1148 - val_accuracy: 0.5414\n","Epoch 12/100\n","106/106 [==============================] - 1s 12ms/step - loss: 1.1112 - accuracy: 0.5146 - val_loss: 1.0991 - val_accuracy: 0.5437\n","Epoch 13/100\n","106/106 [==============================] - 1s 12ms/step - loss: 1.0908 - accuracy: 0.5241 - val_loss: 1.0845 - val_accuracy: 0.5496\n","Epoch 14/100\n","106/106 [==============================] - 1s 12ms/step - loss: 1.0838 - accuracy: 0.5375 - val_loss: 1.0630 - val_accuracy: 0.5739\n","Epoch 15/100\n","106/106 [==============================] - 1s 13ms/step - loss: 1.0590 - accuracy: 0.5495 - val_loss: 1.0759 - val_accuracy: 0.5449\n","Epoch 16/100\n","106/106 [==============================] - 1s 14ms/step - loss: 1.0405 - accuracy: 0.5612 - val_loss: 1.0480 - val_accuracy: 0.5585\n","Epoch 17/100\n","106/106 [==============================] - 1s 12ms/step - loss: 1.0262 - accuracy: 0.5674 - val_loss: 1.0165 - val_accuracy: 0.5857\n","Epoch 18/100\n","106/106 [==============================] - 2s 15ms/step - loss: 1.0209 - accuracy: 0.5697 - val_loss: 0.9908 - val_accuracy: 0.6087\n","Epoch 19/100\n","106/106 [==============================] - 2s 14ms/step - loss: 1.0093 - accuracy: 0.5811 - val_loss: 1.0174 - val_accuracy: 0.5898\n","Epoch 20/100\n","106/106 [==============================] - 1s 13ms/step - loss: 1.0020 - accuracy: 0.5848 - val_loss: 0.9790 - val_accuracy: 0.5993\n","Epoch 21/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.9813 - accuracy: 0.5903 - val_loss: 0.9455 - val_accuracy: 0.6377\n","Epoch 22/100\n","106/106 [==============================] - 1s 12ms/step - loss: 0.9472 - accuracy: 0.6070 - val_loss: 0.9365 - val_accuracy: 0.6200\n","Epoch 23/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.9684 - accuracy: 0.5977 - val_loss: 0.9434 - val_accuracy: 0.6200\n","Epoch 24/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.9499 - accuracy: 0.6113 - val_loss: 0.9389 - val_accuracy: 0.6306\n","Epoch 25/100\n","106/106 [==============================] - 1s 12ms/step - loss: 0.9265 - accuracy: 0.6160 - val_loss: 0.9155 - val_accuracy: 0.6478\n","Epoch 26/100\n","106/106 [==============================] - 1s 12ms/step - loss: 0.9120 - accuracy: 0.6209 - val_loss: 0.9070 - val_accuracy: 0.6507\n","Epoch 27/100\n","106/106 [==============================] - 1s 12ms/step - loss: 0.9222 - accuracy: 0.6268 - val_loss: 0.9167 - val_accuracy: 0.6483\n","Epoch 28/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.9020 - accuracy: 0.6297 - val_loss: 0.8993 - val_accuracy: 0.6472\n","Epoch 29/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.8857 - accuracy: 0.6370 - val_loss: 0.8966 - val_accuracy: 0.6365\n","Epoch 30/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.8901 - accuracy: 0.6376 - val_loss: 0.8840 - val_accuracy: 0.6560\n","Epoch 31/100\n","106/106 [==============================] - 1s 12ms/step - loss: 0.8909 - accuracy: 0.6398 - val_loss: 0.8884 - val_accuracy: 0.6489\n","Epoch 32/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.8848 - accuracy: 0.6365 - val_loss: 0.9042 - val_accuracy: 0.6277\n","Epoch 33/100\n","106/106 [==============================] - 1s 12ms/step - loss: 0.8704 - accuracy: 0.6470 - val_loss: 0.8824 - val_accuracy: 0.6501\n","Epoch 34/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.8682 - accuracy: 0.6523 - val_loss: 0.8828 - val_accuracy: 0.6596\n","Epoch 35/100\n","106/106 [==============================] - 1s 12ms/step - loss: 0.8529 - accuracy: 0.6590 - val_loss: 0.8715 - val_accuracy: 0.6584\n","Epoch 36/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.8568 - accuracy: 0.6526 - val_loss: 0.8552 - val_accuracy: 0.6667\n","Epoch 37/100\n","106/106 [==============================] - 1s 12ms/step - loss: 0.8563 - accuracy: 0.6534 - val_loss: 0.8801 - val_accuracy: 0.6637\n","Epoch 38/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.8358 - accuracy: 0.6609 - val_loss: 0.8676 - val_accuracy: 0.6478\n","Epoch 39/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.8334 - accuracy: 0.6613 - val_loss: 0.8681 - val_accuracy: 0.6572\n","Epoch 40/100\n","106/106 [==============================] - 1s 12ms/step - loss: 0.8395 - accuracy: 0.6615 - val_loss: 0.8852 - val_accuracy: 0.6489\n","Epoch 41/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.8339 - accuracy: 0.6646 - val_loss: 0.8383 - val_accuracy: 0.6779\n","Epoch 42/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.8252 - accuracy: 0.6662 - val_loss: 0.8558 - val_accuracy: 0.6678\n","Epoch 43/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.8196 - accuracy: 0.6646 - val_loss: 0.8967 - val_accuracy: 0.6513\n","Epoch 44/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.8187 - accuracy: 0.6681 - val_loss: 0.8746 - val_accuracy: 0.6554\n","Epoch 45/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.8031 - accuracy: 0.6764 - val_loss: 0.8496 - val_accuracy: 0.6608\n","Epoch 46/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.8122 - accuracy: 0.6677 - val_loss: 0.8459 - val_accuracy: 0.6643\n","Epoch 47/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.8038 - accuracy: 0.6795 - val_loss: 0.8949 - val_accuracy: 0.6300\n","Epoch 48/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.8028 - accuracy: 0.6749 - val_loss: 0.8520 - val_accuracy: 0.6690\n","Epoch 49/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7900 - accuracy: 0.6840 - val_loss: 0.8578 - val_accuracy: 0.6678\n","Epoch 50/100\n","106/106 [==============================] - 1s 12ms/step - loss: 0.8027 - accuracy: 0.6723 - val_loss: 0.8346 - val_accuracy: 0.6613\n","Epoch 51/100\n","106/106 [==============================] - 1s 12ms/step - loss: 0.7932 - accuracy: 0.6789 - val_loss: 0.8498 - val_accuracy: 0.6637\n","Epoch 52/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7853 - accuracy: 0.6896 - val_loss: 0.8333 - val_accuracy: 0.6613\n","Epoch 53/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7892 - accuracy: 0.6834 - val_loss: 0.8509 - val_accuracy: 0.6678\n","Epoch 54/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7957 - accuracy: 0.6801 - val_loss: 0.8197 - val_accuracy: 0.6773\n","Epoch 55/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7695 - accuracy: 0.6973 - val_loss: 0.8507 - val_accuracy: 0.6531\n","Epoch 56/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7789 - accuracy: 0.6860 - val_loss: 0.8184 - val_accuracy: 0.6850\n","Epoch 57/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7704 - accuracy: 0.6899 - val_loss: 0.8345 - val_accuracy: 0.6684\n","Epoch 58/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7740 - accuracy: 0.6894 - val_loss: 0.8320 - val_accuracy: 0.6708\n","Epoch 59/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7743 - accuracy: 0.6872 - val_loss: 0.8208 - val_accuracy: 0.6885\n","Epoch 60/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7720 - accuracy: 0.6905 - val_loss: 0.8292 - val_accuracy: 0.6755\n","Epoch 61/100\n","106/106 [==============================] - 1s 12ms/step - loss: 0.7619 - accuracy: 0.6990 - val_loss: 0.8484 - val_accuracy: 0.6755\n","Epoch 62/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7615 - accuracy: 0.6967 - val_loss: 0.8231 - val_accuracy: 0.6814\n","Epoch 63/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7685 - accuracy: 0.6893 - val_loss: 0.8355 - val_accuracy: 0.6696\n","Epoch 64/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7460 - accuracy: 0.7052 - val_loss: 0.8075 - val_accuracy: 0.6897\n","Epoch 65/100\n","106/106 [==============================] - 1s 12ms/step - loss: 0.7644 - accuracy: 0.6967 - val_loss: 0.8409 - val_accuracy: 0.6543\n","Epoch 66/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7491 - accuracy: 0.7009 - val_loss: 0.8176 - val_accuracy: 0.6844\n","Epoch 67/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7515 - accuracy: 0.7011 - val_loss: 0.8280 - val_accuracy: 0.6749\n","Epoch 68/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7549 - accuracy: 0.6983 - val_loss: 0.8079 - val_accuracy: 0.6968\n","Epoch 69/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7446 - accuracy: 0.6995 - val_loss: 0.8245 - val_accuracy: 0.6767\n","Epoch 70/100\n","106/106 [==============================] - 1s 12ms/step - loss: 0.7578 - accuracy: 0.6953 - val_loss: 0.8041 - val_accuracy: 0.6956\n","Epoch 71/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7531 - accuracy: 0.6937 - val_loss: 0.8078 - val_accuracy: 0.6844\n","Epoch 72/100\n","106/106 [==============================] - 1s 12ms/step - loss: 0.7324 - accuracy: 0.7074 - val_loss: 0.8336 - val_accuracy: 0.6608\n","Epoch 73/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7389 - accuracy: 0.7017 - val_loss: 0.8086 - val_accuracy: 0.6933\n","Epoch 74/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7472 - accuracy: 0.7052 - val_loss: 0.8418 - val_accuracy: 0.6690\n","Epoch 75/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7390 - accuracy: 0.6977 - val_loss: 0.8439 - val_accuracy: 0.6507\n","Epoch 76/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7364 - accuracy: 0.7083 - val_loss: 0.8241 - val_accuracy: 0.6897\n","Epoch 77/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7417 - accuracy: 0.7035 - val_loss: 0.8203 - val_accuracy: 0.6826\n","Epoch 78/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7478 - accuracy: 0.7024 - val_loss: 0.7979 - val_accuracy: 0.7015\n","Epoch 79/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7313 - accuracy: 0.7069 - val_loss: 0.8027 - val_accuracy: 0.7004\n","Epoch 80/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7324 - accuracy: 0.7101 - val_loss: 0.8289 - val_accuracy: 0.6738\n","Epoch 81/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7318 - accuracy: 0.7116 - val_loss: 0.8156 - val_accuracy: 0.6803\n","Epoch 82/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7216 - accuracy: 0.7134 - val_loss: 0.8113 - val_accuracy: 0.6891\n","Epoch 83/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7535 - accuracy: 0.7014 - val_loss: 0.8123 - val_accuracy: 0.6767\n","Epoch 84/100\n","106/106 [==============================] - 1s 12ms/step - loss: 0.7355 - accuracy: 0.7085 - val_loss: 0.8097 - val_accuracy: 0.6986\n","Epoch 85/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7232 - accuracy: 0.7141 - val_loss: 0.8133 - val_accuracy: 0.6814\n","Epoch 86/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7419 - accuracy: 0.7058 - val_loss: 0.8181 - val_accuracy: 0.6809\n","Epoch 87/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7325 - accuracy: 0.7070 - val_loss: 0.7953 - val_accuracy: 0.6992\n","Epoch 88/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7245 - accuracy: 0.7144 - val_loss: 0.8089 - val_accuracy: 0.6939\n","Epoch 89/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7309 - accuracy: 0.7057 - val_loss: 0.7988 - val_accuracy: 0.7057\n","Epoch 90/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7372 - accuracy: 0.7076 - val_loss: 0.8157 - val_accuracy: 0.6868\n","Epoch 91/100\n","106/106 [==============================] - 1s 12ms/step - loss: 0.7393 - accuracy: 0.6992 - val_loss: 0.8346 - val_accuracy: 0.6803\n","Epoch 92/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7330 - accuracy: 0.7061 - val_loss: 0.7989 - val_accuracy: 0.7057\n","Epoch 93/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7105 - accuracy: 0.7204 - val_loss: 0.8107 - val_accuracy: 0.6944\n","Epoch 94/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7124 - accuracy: 0.7219 - val_loss: 0.7910 - val_accuracy: 0.7015\n","Epoch 95/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7243 - accuracy: 0.7178 - val_loss: 0.8001 - val_accuracy: 0.6950\n","Epoch 96/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7077 - accuracy: 0.7207 - val_loss: 0.8021 - val_accuracy: 0.7045\n","Epoch 97/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7204 - accuracy: 0.7215 - val_loss: 0.7955 - val_accuracy: 0.7063\n","Epoch 98/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7142 - accuracy: 0.7199 - val_loss: 0.8111 - val_accuracy: 0.6885\n","Epoch 99/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7066 - accuracy: 0.7154 - val_loss: 0.8133 - val_accuracy: 0.6891\n","Epoch 100/100\n","106/106 [==============================] - 1s 11ms/step - loss: 0.7228 - accuracy: 0.7132 - val_loss: 0.7798 - val_accuracy: 0.7252\n","56/56 [==============================] - 0s 3ms/step - loss: 0.7654 - accuracy: 0.6823\n","Shape of X after trimming: (1692, 22, 500)\n","Shape of X after trimming: (423, 22, 500)\n","Shape of X after trimming: (443, 22, 500)\n","Shape of training set: (6768, 22, 250)\n","Shape of validation set: (1692, 22, 250)\n","Shape of training labels: (6768,)\n","Shape of validation labels: (1692,)\n","Shape of testing set: (1772, 22, 250)\n","Shape of testing labels: (1772,)\n","Shape of training labels after categorical conversion: (6768, 4)\n","Shape of validation labels after categorical conversion: (1692, 4)\n","Shape of test labels after categorical conversion: (1772, 4)\n","Shape of training set after adding width info: (6768, 22, 250, 1)\n","Shape of validation set after adding width info: (1692, 22, 250, 1)\n","Shape of test set after adding width info: (1772, 22, 250, 1)\n","Shape of training set after dimension reshaping: (6768, 250, 1, 22)\n","Shape of validation set after dimension reshaping: (1692, 250, 1, 22)\n","Shape of test set after dimension reshaping: (1772, 250, 1, 22)\n","Epoch 1/100\n","106/106 [==============================] - 2s 16ms/step - loss: 2.0150 - accuracy: 0.2668 - val_loss: 1.4260 - val_accuracy: 0.3617\n","Epoch 2/100\n","106/106 [==============================] - 1s 13ms/step - loss: 1.5742 - accuracy: 0.3267 - val_loss: 1.3119 - val_accuracy: 0.3623\n","Epoch 3/100\n","106/106 [==============================] - 1s 13ms/step - loss: 1.4027 - accuracy: 0.3624 - val_loss: 1.2532 - val_accuracy: 0.4155\n","Epoch 4/100\n","106/106 [==============================] - 1s 13ms/step - loss: 1.2863 - accuracy: 0.4133 - val_loss: 1.1956 - val_accuracy: 0.4450\n","Epoch 5/100\n","106/106 [==============================] - 1s 13ms/step - loss: 1.2291 - accuracy: 0.4549 - val_loss: 1.1436 - val_accuracy: 0.5030\n","Epoch 6/100\n","106/106 [==============================] - 1s 13ms/step - loss: 1.1891 - accuracy: 0.4778 - val_loss: 1.1082 - val_accuracy: 0.5343\n","Epoch 7/100\n","106/106 [==============================] - 1s 13ms/step - loss: 1.1540 - accuracy: 0.5001 - val_loss: 1.0899 - val_accuracy: 0.5508\n","Epoch 8/100\n","106/106 [==============================] - 1s 14ms/step - loss: 1.1346 - accuracy: 0.5145 - val_loss: 1.0681 - val_accuracy: 0.5798\n","Epoch 9/100\n","106/106 [==============================] - 1s 13ms/step - loss: 1.1085 - accuracy: 0.5290 - val_loss: 1.0602 - val_accuracy: 0.5715\n","Epoch 10/100\n","106/106 [==============================] - 1s 13ms/step - loss: 1.0865 - accuracy: 0.5363 - val_loss: 1.0536 - val_accuracy: 0.5703\n","Epoch 11/100\n","106/106 [==============================] - 1s 14ms/step - loss: 1.0645 - accuracy: 0.5539 - val_loss: 1.0379 - val_accuracy: 0.5798\n","Epoch 12/100\n","106/106 [==============================] - 2s 15ms/step - loss: 1.0593 - accuracy: 0.5516 - val_loss: 1.0048 - val_accuracy: 0.6164\n","Epoch 13/100\n","106/106 [==============================] - 1s 14ms/step - loss: 1.0440 - accuracy: 0.5693 - val_loss: 1.0166 - val_accuracy: 0.6070\n","Epoch 14/100\n","106/106 [==============================] - 1s 13ms/step - loss: 1.0214 - accuracy: 0.5728 - val_loss: 0.9981 - val_accuracy: 0.5999\n","Epoch 15/100\n","106/106 [==============================] - 1s 14ms/step - loss: 1.0175 - accuracy: 0.5757 - val_loss: 1.0249 - val_accuracy: 0.6028\n","Epoch 16/100\n","106/106 [==============================] - 1s 14ms/step - loss: 1.0002 - accuracy: 0.5891 - val_loss: 0.9931 - val_accuracy: 0.6164\n","Epoch 17/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.9912 - accuracy: 0.5954 - val_loss: 0.9759 - val_accuracy: 0.6182\n","Epoch 18/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.9737 - accuracy: 0.6034 - val_loss: 0.9495 - val_accuracy: 0.6259\n","Epoch 19/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.9515 - accuracy: 0.6145 - val_loss: 0.9321 - val_accuracy: 0.6371\n","Epoch 20/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.9513 - accuracy: 0.6065 - val_loss: 0.9128 - val_accuracy: 0.6602\n","Epoch 21/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.9391 - accuracy: 0.6135 - val_loss: 0.9204 - val_accuracy: 0.6584\n","Epoch 22/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.9285 - accuracy: 0.6198 - val_loss: 0.9665 - val_accuracy: 0.6288\n","Epoch 23/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.9197 - accuracy: 0.6294 - val_loss: 0.9051 - val_accuracy: 0.6472\n","Epoch 24/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.9154 - accuracy: 0.6254 - val_loss: 0.8959 - val_accuracy: 0.6643\n","Epoch 25/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.8999 - accuracy: 0.6334 - val_loss: 0.8662 - val_accuracy: 0.6749\n","Epoch 26/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.8923 - accuracy: 0.6340 - val_loss: 0.8710 - val_accuracy: 0.6862\n","Epoch 27/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.8828 - accuracy: 0.6426 - val_loss: 0.8712 - val_accuracy: 0.6755\n","Epoch 28/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.8655 - accuracy: 0.6467 - val_loss: 0.8494 - val_accuracy: 0.6903\n","Epoch 29/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.8788 - accuracy: 0.6423 - val_loss: 0.8228 - val_accuracy: 0.6897\n","Epoch 30/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.8480 - accuracy: 0.6616 - val_loss: 0.8492 - val_accuracy: 0.6755\n","Epoch 31/100\n","106/106 [==============================] - 2s 14ms/step - loss: 0.8525 - accuracy: 0.6519 - val_loss: 0.8386 - val_accuracy: 0.6903\n","Epoch 32/100\n","106/106 [==============================] - 2s 15ms/step - loss: 0.8436 - accuracy: 0.6621 - val_loss: 0.8321 - val_accuracy: 0.6879\n","Epoch 33/100\n","106/106 [==============================] - 2s 14ms/step - loss: 0.8519 - accuracy: 0.6612 - val_loss: 0.8159 - val_accuracy: 0.6885\n","Epoch 34/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.8400 - accuracy: 0.6606 - val_loss: 0.8092 - val_accuracy: 0.7027\n","Epoch 35/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.8346 - accuracy: 0.6690 - val_loss: 0.8175 - val_accuracy: 0.6944\n","Epoch 36/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.8218 - accuracy: 0.6656 - val_loss: 0.8062 - val_accuracy: 0.6933\n","Epoch 37/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.8141 - accuracy: 0.6637 - val_loss: 0.7929 - val_accuracy: 0.7021\n","Epoch 38/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.8237 - accuracy: 0.6699 - val_loss: 0.7904 - val_accuracy: 0.6785\n","Epoch 39/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.8086 - accuracy: 0.6748 - val_loss: 0.7833 - val_accuracy: 0.7092\n","Epoch 40/100\n","106/106 [==============================] - 2s 14ms/step - loss: 0.8036 - accuracy: 0.6772 - val_loss: 0.7731 - val_accuracy: 0.7169\n","Epoch 41/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7962 - accuracy: 0.6791 - val_loss: 0.7770 - val_accuracy: 0.7128\n","Epoch 42/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7999 - accuracy: 0.6856 - val_loss: 0.7740 - val_accuracy: 0.7092\n","Epoch 43/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7895 - accuracy: 0.6881 - val_loss: 0.7984 - val_accuracy: 0.6909\n","Epoch 44/100\n","106/106 [==============================] - 2s 15ms/step - loss: 0.7882 - accuracy: 0.6819 - val_loss: 0.7757 - val_accuracy: 0.7116\n","Epoch 45/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7993 - accuracy: 0.6785 - val_loss: 0.7784 - val_accuracy: 0.6986\n","Epoch 46/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7785 - accuracy: 0.6853 - val_loss: 0.7631 - val_accuracy: 0.7210\n","Epoch 47/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7891 - accuracy: 0.6816 - val_loss: 0.7499 - val_accuracy: 0.7157\n","Epoch 48/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7804 - accuracy: 0.6865 - val_loss: 0.8044 - val_accuracy: 0.6944\n","Epoch 49/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7743 - accuracy: 0.6872 - val_loss: 0.7490 - val_accuracy: 0.7193\n","Epoch 50/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7785 - accuracy: 0.6859 - val_loss: 0.7698 - val_accuracy: 0.7045\n","Epoch 51/100\n","106/106 [==============================] - 2s 14ms/step - loss: 0.7514 - accuracy: 0.7007 - val_loss: 0.7432 - val_accuracy: 0.7287\n","Epoch 52/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7699 - accuracy: 0.6953 - val_loss: 0.7682 - val_accuracy: 0.7116\n","Epoch 53/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7770 - accuracy: 0.6900 - val_loss: 0.7452 - val_accuracy: 0.7210\n","Epoch 54/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7657 - accuracy: 0.6928 - val_loss: 0.7605 - val_accuracy: 0.7069\n","Epoch 55/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7696 - accuracy: 0.6913 - val_loss: 0.7752 - val_accuracy: 0.7039\n","Epoch 56/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7593 - accuracy: 0.6992 - val_loss: 0.7619 - val_accuracy: 0.7074\n","Epoch 57/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7695 - accuracy: 0.6937 - val_loss: 0.7452 - val_accuracy: 0.7151\n","Epoch 58/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7432 - accuracy: 0.7049 - val_loss: 0.7477 - val_accuracy: 0.7293\n","Epoch 59/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7657 - accuracy: 0.6968 - val_loss: 0.7518 - val_accuracy: 0.7015\n","Epoch 60/100\n","106/106 [==============================] - 2s 14ms/step - loss: 0.7529 - accuracy: 0.6989 - val_loss: 0.7515 - val_accuracy: 0.7021\n","Epoch 61/100\n","106/106 [==============================] - 2s 14ms/step - loss: 0.7646 - accuracy: 0.6930 - val_loss: 0.7472 - val_accuracy: 0.7104\n","Epoch 62/100\n","106/106 [==============================] - 2s 15ms/step - loss: 0.7623 - accuracy: 0.6921 - val_loss: 0.7456 - val_accuracy: 0.7122\n","Epoch 63/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7353 - accuracy: 0.7163 - val_loss: 0.7444 - val_accuracy: 0.7299\n","Epoch 64/100\n","106/106 [==============================] - 2s 15ms/step - loss: 0.7393 - accuracy: 0.7035 - val_loss: 0.7790 - val_accuracy: 0.7092\n","Epoch 65/100\n","106/106 [==============================] - 2s 15ms/step - loss: 0.7453 - accuracy: 0.6981 - val_loss: 0.7453 - val_accuracy: 0.7092\n","Epoch 66/100\n","106/106 [==============================] - 2s 15ms/step - loss: 0.7393 - accuracy: 0.7082 - val_loss: 0.7415 - val_accuracy: 0.7204\n","Epoch 67/100\n","106/106 [==============================] - 2s 14ms/step - loss: 0.7396 - accuracy: 0.7070 - val_loss: 0.7158 - val_accuracy: 0.7293\n","Epoch 68/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.7283 - accuracy: 0.7114 - val_loss: 0.7460 - val_accuracy: 0.7240\n","Epoch 69/100\n","106/106 [==============================] - 2s 15ms/step - loss: 0.7508 - accuracy: 0.6989 - val_loss: 0.7321 - val_accuracy: 0.7157\n","Epoch 70/100\n","106/106 [==============================] - 2s 15ms/step - loss: 0.7200 - accuracy: 0.7178 - val_loss: 0.7426 - val_accuracy: 0.7086\n","Epoch 71/100\n","106/106 [==============================] - 2s 15ms/step - loss: 0.7202 - accuracy: 0.7134 - val_loss: 0.7388 - val_accuracy: 0.7139\n","Epoch 72/100\n","106/106 [==============================] - 2s 14ms/step - loss: 0.7394 - accuracy: 0.7039 - val_loss: 0.7418 - val_accuracy: 0.7134\n","Epoch 73/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.7293 - accuracy: 0.7145 - val_loss: 0.7353 - val_accuracy: 0.7175\n","Epoch 74/100\n","106/106 [==============================] - 2s 15ms/step - loss: 0.7295 - accuracy: 0.7142 - val_loss: 0.7365 - val_accuracy: 0.7358\n","Epoch 75/100\n","106/106 [==============================] - 2s 15ms/step - loss: 0.7270 - accuracy: 0.7101 - val_loss: 0.7563 - val_accuracy: 0.7110\n","Epoch 76/100\n","106/106 [==============================] - 2s 15ms/step - loss: 0.7371 - accuracy: 0.7148 - val_loss: 0.7358 - val_accuracy: 0.7110\n","Epoch 77/100\n","106/106 [==============================] - 2s 15ms/step - loss: 0.7178 - accuracy: 0.7160 - val_loss: 0.7304 - val_accuracy: 0.7240\n","Epoch 78/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.7257 - accuracy: 0.7148 - val_loss: 0.7505 - val_accuracy: 0.7074\n","Epoch 79/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.7180 - accuracy: 0.7123 - val_loss: 0.7380 - val_accuracy: 0.7181\n","Epoch 80/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.7256 - accuracy: 0.7092 - val_loss: 0.7312 - val_accuracy: 0.7193\n","Epoch 81/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.7162 - accuracy: 0.7184 - val_loss: 0.7198 - val_accuracy: 0.7234\n","Epoch 82/100\n","106/106 [==============================] - 2s 15ms/step - loss: 0.7147 - accuracy: 0.7179 - val_loss: 0.7186 - val_accuracy: 0.7193\n","Epoch 83/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7268 - accuracy: 0.7119 - val_loss: 0.7289 - val_accuracy: 0.7169\n","Epoch 84/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7137 - accuracy: 0.7162 - val_loss: 0.7444 - val_accuracy: 0.7175\n","Epoch 85/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7133 - accuracy: 0.7134 - val_loss: 0.7238 - val_accuracy: 0.7293\n","Epoch 86/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7145 - accuracy: 0.7137 - val_loss: 0.7193 - val_accuracy: 0.7364\n","Epoch 87/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7294 - accuracy: 0.7113 - val_loss: 0.7105 - val_accuracy: 0.7258\n","Epoch 88/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7210 - accuracy: 0.7168 - val_loss: 0.7132 - val_accuracy: 0.7264\n","Epoch 89/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7059 - accuracy: 0.7179 - val_loss: 0.7208 - val_accuracy: 0.7323\n","Epoch 90/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7230 - accuracy: 0.7144 - val_loss: 0.6998 - val_accuracy: 0.7388\n","Epoch 91/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7167 - accuracy: 0.7153 - val_loss: 0.7156 - val_accuracy: 0.7258\n","Epoch 92/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7188 - accuracy: 0.7135 - val_loss: 0.7248 - val_accuracy: 0.7110\n","Epoch 93/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7028 - accuracy: 0.7224 - val_loss: 0.7083 - val_accuracy: 0.7358\n","Epoch 94/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7278 - accuracy: 0.7171 - val_loss: 0.7096 - val_accuracy: 0.7346\n","Epoch 95/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7036 - accuracy: 0.7219 - val_loss: 0.7097 - val_accuracy: 0.7264\n","Epoch 96/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7106 - accuracy: 0.7225 - val_loss: 0.6973 - val_accuracy: 0.7441\n","Epoch 97/100\n","106/106 [==============================] - 1s 13ms/step - loss: 0.7015 - accuracy: 0.7240 - val_loss: 0.7286 - val_accuracy: 0.7305\n","Epoch 98/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.7029 - accuracy: 0.7219 - val_loss: 0.7343 - val_accuracy: 0.7199\n","Epoch 99/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.6921 - accuracy: 0.7284 - val_loss: 0.7036 - val_accuracy: 0.7264\n","Epoch 100/100\n","106/106 [==============================] - 1s 14ms/step - loss: 0.7055 - accuracy: 0.7163 - val_loss: 0.7109 - val_accuracy: 0.7335\n","56/56 [==============================] - 0s 3ms/step - loss: 0.7512 - accuracy: 0.7026\n","Shape of X after trimming: (1692, 22, 600)\n","Shape of X after trimming: (423, 22, 600)\n","Shape of X after trimming: (443, 22, 600)\n","Shape of training set: (6768, 22, 300)\n","Shape of validation set: (1692, 22, 300)\n","Shape of training labels: (6768,)\n","Shape of validation labels: (1692,)\n","Shape of testing set: (1772, 22, 300)\n","Shape of testing labels: (1772,)\n","Shape of training labels after categorical conversion: (6768, 4)\n","Shape of validation labels after categorical conversion: (1692, 4)\n","Shape of test labels after categorical conversion: (1772, 4)\n","Shape of training set after adding width info: (6768, 22, 300, 1)\n","Shape of validation set after adding width info: (1692, 22, 300, 1)\n","Shape of test set after adding width info: (1772, 22, 300, 1)\n","Shape of training set after dimension reshaping: (6768, 300, 1, 22)\n","Shape of validation set after dimension reshaping: (1692, 300, 1, 22)\n","Shape of test set after dimension reshaping: (1772, 300, 1, 22)\n","Epoch 1/100\n","106/106 [==============================] - 2s 18ms/step - loss: 1.9584 - accuracy: 0.2764 - val_loss: 1.4596 - val_accuracy: 0.3505\n","Epoch 2/100\n","106/106 [==============================] - 2s 16ms/step - loss: 1.5703 - accuracy: 0.3110 - val_loss: 1.3024 - val_accuracy: 0.3806\n","Epoch 3/100\n","106/106 [==============================] - 2s 17ms/step - loss: 1.4262 - accuracy: 0.3364 - val_loss: 1.2777 - val_accuracy: 0.4143\n","Epoch 4/100\n","106/106 [==============================] - 2s 18ms/step - loss: 1.3484 - accuracy: 0.3593 - val_loss: 1.2619 - val_accuracy: 0.4184\n","Epoch 5/100\n","106/106 [==============================] - 2s 18ms/step - loss: 1.2828 - accuracy: 0.4032 - val_loss: 1.2302 - val_accuracy: 0.4663\n","Epoch 6/100\n","106/106 [==============================] - 2s 17ms/step - loss: 1.2357 - accuracy: 0.4282 - val_loss: 1.1972 - val_accuracy: 0.4699\n","Epoch 7/100\n","106/106 [==============================] - 2s 17ms/step - loss: 1.1987 - accuracy: 0.4614 - val_loss: 1.1614 - val_accuracy: 0.5100\n","Epoch 8/100\n","106/106 [==============================] - 2s 17ms/step - loss: 1.1708 - accuracy: 0.4914 - val_loss: 1.1521 - val_accuracy: 0.5195\n","Epoch 9/100\n","106/106 [==============================] - 2s 18ms/step - loss: 1.1490 - accuracy: 0.4902 - val_loss: 1.1188 - val_accuracy: 0.5319\n","Epoch 10/100\n","106/106 [==============================] - 2s 17ms/step - loss: 1.1320 - accuracy: 0.5118 - val_loss: 1.1064 - val_accuracy: 0.5112\n","Epoch 11/100\n","106/106 [==============================] - 2s 17ms/step - loss: 1.1004 - accuracy: 0.5276 - val_loss: 1.0680 - val_accuracy: 0.5437\n","Epoch 12/100\n","106/106 [==============================] - 2s 18ms/step - loss: 1.0722 - accuracy: 0.5516 - val_loss: 1.0180 - val_accuracy: 0.6011\n","Epoch 13/100\n","106/106 [==============================] - 2s 16ms/step - loss: 1.0526 - accuracy: 0.5588 - val_loss: 0.9864 - val_accuracy: 0.6164\n","Epoch 14/100\n","106/106 [==============================] - 2s 16ms/step - loss: 1.0271 - accuracy: 0.5671 - val_loss: 0.9942 - val_accuracy: 0.5887\n","Epoch 15/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.9984 - accuracy: 0.5884 - val_loss: 0.9638 - val_accuracy: 0.6052\n","Epoch 16/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.9933 - accuracy: 0.5949 - val_loss: 0.9221 - val_accuracy: 0.6389\n","Epoch 17/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.9787 - accuracy: 0.5915 - val_loss: 0.9058 - val_accuracy: 0.6176\n","Epoch 18/100\n","106/106 [==============================] - 2s 21ms/step - loss: 0.9509 - accuracy: 0.6028 - val_loss: 0.8890 - val_accuracy: 0.6348\n","Epoch 19/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.9309 - accuracy: 0.6192 - val_loss: 0.8530 - val_accuracy: 0.6797\n","Epoch 20/100\n","106/106 [==============================] - 2s 21ms/step - loss: 0.9294 - accuracy: 0.6144 - val_loss: 0.8743 - val_accuracy: 0.6466\n","Epoch 21/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.9192 - accuracy: 0.6262 - val_loss: 0.8678 - val_accuracy: 0.6543\n","Epoch 22/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.9123 - accuracy: 0.6305 - val_loss: 0.8357 - val_accuracy: 0.6590\n","Epoch 23/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.8940 - accuracy: 0.6399 - val_loss: 0.8436 - val_accuracy: 0.6643\n","Epoch 24/100\n","106/106 [==============================] - 2s 18ms/step - loss: 0.8793 - accuracy: 0.6407 - val_loss: 0.8340 - val_accuracy: 0.6608\n","Epoch 25/100\n","106/106 [==============================] - 2s 21ms/step - loss: 0.8723 - accuracy: 0.6414 - val_loss: 0.8116 - val_accuracy: 0.6809\n","Epoch 26/100\n","106/106 [==============================] - 2s 18ms/step - loss: 0.8691 - accuracy: 0.6467 - val_loss: 0.8561 - val_accuracy: 0.6507\n","Epoch 27/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.8519 - accuracy: 0.6571 - val_loss: 0.8094 - val_accuracy: 0.6874\n","Epoch 28/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.8586 - accuracy: 0.6492 - val_loss: 0.7875 - val_accuracy: 0.6950\n","Epoch 29/100\n","106/106 [==============================] - 2s 18ms/step - loss: 0.8430 - accuracy: 0.6571 - val_loss: 0.7904 - val_accuracy: 0.6968\n","Epoch 30/100\n","106/106 [==============================] - 2s 18ms/step - loss: 0.8213 - accuracy: 0.6726 - val_loss: 0.7920 - val_accuracy: 0.6856\n","Epoch 31/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.8454 - accuracy: 0.6563 - val_loss: 0.8122 - val_accuracy: 0.6738\n","Epoch 32/100\n","106/106 [==============================] - 2s 18ms/step - loss: 0.8264 - accuracy: 0.6624 - val_loss: 0.7978 - val_accuracy: 0.6708\n","Epoch 33/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.8139 - accuracy: 0.6693 - val_loss: 0.8050 - val_accuracy: 0.6809\n","Epoch 34/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.8187 - accuracy: 0.6667 - val_loss: 0.7827 - val_accuracy: 0.7009\n","Epoch 35/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.7992 - accuracy: 0.6844 - val_loss: 0.7824 - val_accuracy: 0.6927\n","Epoch 36/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.8034 - accuracy: 0.6814 - val_loss: 0.7958 - val_accuracy: 0.6838\n","Epoch 37/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.8084 - accuracy: 0.6761 - val_loss: 0.7852 - val_accuracy: 0.6832\n","Epoch 38/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.7876 - accuracy: 0.6859 - val_loss: 0.7715 - val_accuracy: 0.6939\n","Epoch 39/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.7891 - accuracy: 0.6887 - val_loss: 0.7800 - val_accuracy: 0.6944\n","Epoch 40/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.7875 - accuracy: 0.6820 - val_loss: 0.7768 - val_accuracy: 0.6868\n","Epoch 41/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.7824 - accuracy: 0.6890 - val_loss: 0.7996 - val_accuracy: 0.6779\n","Epoch 42/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.7830 - accuracy: 0.6865 - val_loss: 0.7825 - val_accuracy: 0.6862\n","Epoch 43/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.7739 - accuracy: 0.6837 - val_loss: 0.7747 - val_accuracy: 0.6909\n","Epoch 44/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.7697 - accuracy: 0.6919 - val_loss: 0.7860 - val_accuracy: 0.6791\n","Epoch 45/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.7786 - accuracy: 0.6894 - val_loss: 0.7635 - val_accuracy: 0.6956\n","Epoch 46/100\n","106/106 [==============================] - 2s 20ms/step - loss: 0.7720 - accuracy: 0.6925 - val_loss: 0.7795 - val_accuracy: 0.6962\n","Epoch 47/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.7496 - accuracy: 0.7033 - val_loss: 0.7609 - val_accuracy: 0.6897\n","Epoch 48/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.7611 - accuracy: 0.6996 - val_loss: 0.7654 - val_accuracy: 0.6921\n","Epoch 49/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.7547 - accuracy: 0.6949 - val_loss: 0.7622 - val_accuracy: 0.6939\n","Epoch 50/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.7363 - accuracy: 0.7095 - val_loss: 0.7715 - val_accuracy: 0.6950\n","Epoch 51/100\n","106/106 [==============================] - 2s 18ms/step - loss: 0.7532 - accuracy: 0.7004 - val_loss: 0.7439 - val_accuracy: 0.7027\n","Epoch 52/100\n","106/106 [==============================] - 2s 18ms/step - loss: 0.7519 - accuracy: 0.6986 - val_loss: 0.7754 - val_accuracy: 0.6933\n","Epoch 53/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.7517 - accuracy: 0.7049 - val_loss: 0.7384 - val_accuracy: 0.7063\n","Epoch 54/100\n","106/106 [==============================] - 2s 20ms/step - loss: 0.7492 - accuracy: 0.7108 - val_loss: 0.7525 - val_accuracy: 0.6927\n","Epoch 55/100\n","106/106 [==============================] - 2s 20ms/step - loss: 0.7563 - accuracy: 0.6958 - val_loss: 0.7429 - val_accuracy: 0.7027\n","Epoch 56/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.7377 - accuracy: 0.7083 - val_loss: 0.7626 - val_accuracy: 0.6891\n","Epoch 57/100\n","106/106 [==============================] - 2s 22ms/step - loss: 0.7233 - accuracy: 0.7126 - val_loss: 0.7545 - val_accuracy: 0.7009\n","Epoch 58/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.7483 - accuracy: 0.7012 - val_loss: 0.7564 - val_accuracy: 0.6885\n","Epoch 59/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.7256 - accuracy: 0.7086 - val_loss: 0.7487 - val_accuracy: 0.7069\n","Epoch 60/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.7312 - accuracy: 0.7117 - val_loss: 0.7620 - val_accuracy: 0.6820\n","Epoch 61/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.7245 - accuracy: 0.7166 - val_loss: 0.7604 - val_accuracy: 0.6879\n","Epoch 62/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.7303 - accuracy: 0.7105 - val_loss: 0.7439 - val_accuracy: 0.7086\n","Epoch 63/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.7229 - accuracy: 0.7169 - val_loss: 0.7548 - val_accuracy: 0.6950\n","Epoch 64/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.7018 - accuracy: 0.7253 - val_loss: 0.7664 - val_accuracy: 0.6844\n","Epoch 65/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.7159 - accuracy: 0.7139 - val_loss: 0.7626 - val_accuracy: 0.6950\n","Epoch 66/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.7196 - accuracy: 0.7117 - val_loss: 0.7473 - val_accuracy: 0.6998\n","Epoch 67/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.7143 - accuracy: 0.7190 - val_loss: 0.7391 - val_accuracy: 0.7015\n","Epoch 68/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.7119 - accuracy: 0.7172 - val_loss: 0.7697 - val_accuracy: 0.6814\n","Epoch 69/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.7232 - accuracy: 0.7138 - val_loss: 0.7559 - val_accuracy: 0.7039\n","Epoch 70/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.6951 - accuracy: 0.7287 - val_loss: 0.7501 - val_accuracy: 0.6974\n","Epoch 71/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.7049 - accuracy: 0.7207 - val_loss: 0.7657 - val_accuracy: 0.6986\n","Epoch 72/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.7057 - accuracy: 0.7145 - val_loss: 0.7795 - val_accuracy: 0.6909\n","Epoch 73/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.7010 - accuracy: 0.7244 - val_loss: 0.7316 - val_accuracy: 0.7015\n","Epoch 74/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.7085 - accuracy: 0.7151 - val_loss: 0.7553 - val_accuracy: 0.7015\n","Epoch 75/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.6987 - accuracy: 0.7233 - val_loss: 0.7329 - val_accuracy: 0.7092\n","Epoch 76/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.6971 - accuracy: 0.7270 - val_loss: 0.7475 - val_accuracy: 0.7151\n","Epoch 77/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.7032 - accuracy: 0.7219 - val_loss: 0.7545 - val_accuracy: 0.6998\n","Epoch 78/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.6953 - accuracy: 0.7277 - val_loss: 0.7756 - val_accuracy: 0.6862\n","Epoch 79/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.6953 - accuracy: 0.7292 - val_loss: 0.7358 - val_accuracy: 0.7039\n","Epoch 80/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.6926 - accuracy: 0.7314 - val_loss: 0.7356 - val_accuracy: 0.6962\n","Epoch 81/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.6950 - accuracy: 0.7298 - val_loss: 0.7435 - val_accuracy: 0.6980\n","Epoch 82/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.6887 - accuracy: 0.7293 - val_loss: 0.7368 - val_accuracy: 0.7039\n","Epoch 83/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.6939 - accuracy: 0.7250 - val_loss: 0.7169 - val_accuracy: 0.7151\n","Epoch 84/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.6871 - accuracy: 0.7348 - val_loss: 0.7307 - val_accuracy: 0.7098\n","Epoch 85/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.6783 - accuracy: 0.7352 - val_loss: 0.7568 - val_accuracy: 0.6868\n","Epoch 86/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.6858 - accuracy: 0.7358 - val_loss: 0.7178 - val_accuracy: 0.7116\n","Epoch 87/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.6954 - accuracy: 0.7204 - val_loss: 0.7312 - val_accuracy: 0.7199\n","Epoch 88/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.6851 - accuracy: 0.7326 - val_loss: 0.7243 - val_accuracy: 0.7139\n","Epoch 89/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.7026 - accuracy: 0.7255 - val_loss: 0.7175 - val_accuracy: 0.7104\n","Epoch 90/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.6706 - accuracy: 0.7380 - val_loss: 0.7585 - val_accuracy: 0.6956\n","Epoch 91/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.6801 - accuracy: 0.7407 - val_loss: 0.7340 - val_accuracy: 0.7110\n","Epoch 92/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.6907 - accuracy: 0.7255 - val_loss: 0.7201 - val_accuracy: 0.7069\n","Epoch 93/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.6955 - accuracy: 0.7244 - val_loss: 0.7272 - val_accuracy: 0.7074\n","Epoch 94/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.6877 - accuracy: 0.7281 - val_loss: 0.7368 - val_accuracy: 0.7015\n","Epoch 95/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.6766 - accuracy: 0.7305 - val_loss: 0.7302 - val_accuracy: 0.7063\n","Epoch 96/100\n","106/106 [==============================] - 2s 17ms/step - loss: 0.6756 - accuracy: 0.7346 - val_loss: 0.7137 - val_accuracy: 0.7199\n","Epoch 97/100\n","106/106 [==============================] - 2s 18ms/step - loss: 0.6806 - accuracy: 0.7357 - val_loss: 0.7274 - val_accuracy: 0.7163\n","Epoch 98/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.6822 - accuracy: 0.7337 - val_loss: 0.7404 - val_accuracy: 0.7057\n","Epoch 99/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.6674 - accuracy: 0.7366 - val_loss: 0.7276 - val_accuracy: 0.7204\n","Epoch 100/100\n","106/106 [==============================] - 2s 16ms/step - loss: 0.6717 - accuracy: 0.7314 - val_loss: 0.7115 - val_accuracy: 0.7258\n","56/56 [==============================] - 0s 3ms/step - loss: 0.6768 - accuracy: 0.7421\n","Shape of X after trimming: (1692, 22, 700)\n","Shape of X after trimming: (423, 22, 700)\n","Shape of X after trimming: (443, 22, 700)\n","Shape of training set: (6768, 22, 350)\n","Shape of validation set: (1692, 22, 350)\n","Shape of training labels: (6768,)\n","Shape of validation labels: (1692,)\n","Shape of testing set: (1772, 22, 350)\n","Shape of testing labels: (1772,)\n","Shape of training labels after categorical conversion: (6768, 4)\n","Shape of validation labels after categorical conversion: (1692, 4)\n","Shape of test labels after categorical conversion: (1772, 4)\n","Shape of training set after adding width info: (6768, 22, 350, 1)\n","Shape of validation set after adding width info: (1692, 22, 350, 1)\n","Shape of test set after adding width info: (1772, 22, 350, 1)\n","Shape of training set after dimension reshaping: (6768, 350, 1, 22)\n","Shape of validation set after dimension reshaping: (1692, 350, 1, 22)\n","Shape of test set after dimension reshaping: (1772, 350, 1, 22)\n","Epoch 1/100\n","106/106 [==============================] - 2s 18ms/step - loss: 1.9998 - accuracy: 0.2726 - val_loss: 1.4920 - val_accuracy: 0.3233\n","Epoch 2/100\n","106/106 [==============================] - 2s 17ms/step - loss: 1.5765 - accuracy: 0.3156 - val_loss: 1.3295 - val_accuracy: 0.3487\n","Epoch 3/100\n","106/106 [==============================] - 2s 17ms/step - loss: 1.4222 - accuracy: 0.3452 - val_loss: 1.2804 - val_accuracy: 0.3889\n","Epoch 4/100\n","106/106 [==============================] - 2s 17ms/step - loss: 1.3066 - accuracy: 0.4019 - val_loss: 1.2331 - val_accuracy: 0.4303\n","Epoch 5/100\n","106/106 [==============================] - 2s 17ms/step - loss: 1.2433 - accuracy: 0.4331 - val_loss: 1.2017 - val_accuracy: 0.4628\n","Epoch 6/100\n","106/106 [==============================] - 2s 17ms/step - loss: 1.2032 - accuracy: 0.4659 - val_loss: 1.1826 - val_accuracy: 0.4728\n","Epoch 7/100\n","106/106 [==============================] - 2s 18ms/step - loss: 1.1739 - accuracy: 0.4874 - val_loss: 1.1309 - val_accuracy: 0.5083\n","Epoch 8/100\n","106/106 [==============================] - 2s 18ms/step - loss: 1.1444 - accuracy: 0.4987 - val_loss: 1.1124 - val_accuracy: 0.5361\n","Epoch 9/100\n","106/106 [==============================] - 2s 18ms/step - loss: 1.1200 - accuracy: 0.5179 - val_loss: 1.0857 - val_accuracy: 0.5414\n","Epoch 10/100\n","106/106 [==============================] - 2s 18ms/step - loss: 1.0981 - accuracy: 0.5232 - val_loss: 1.0945 - val_accuracy: 0.5325\n","Epoch 11/100\n","106/106 [==============================] - 2s 18ms/step - loss: 1.0864 - accuracy: 0.5381 - val_loss: 1.0342 - val_accuracy: 0.5881\n","Epoch 12/100\n","106/106 [==============================] - 2s 18ms/step - loss: 1.0707 - accuracy: 0.5443 - val_loss: 1.0403 - val_accuracy: 0.5727\n","Epoch 13/100\n","106/106 [==============================] - 2s 23ms/step - loss: 1.0497 - accuracy: 0.5588 - val_loss: 1.0255 - val_accuracy: 0.5922\n","Epoch 14/100\n","106/106 [==============================] - 2s 20ms/step - loss: 1.0439 - accuracy: 0.5624 - val_loss: 1.0348 - val_accuracy: 0.5804\n","Epoch 15/100\n","106/106 [==============================] - 3s 27ms/step - loss: 1.0168 - accuracy: 0.5720 - val_loss: 0.9838 - val_accuracy: 0.6123\n","Epoch 16/100\n","106/106 [==============================] - 3s 29ms/step - loss: 1.0094 - accuracy: 0.5822 - val_loss: 0.9830 - val_accuracy: 0.6099\n","Epoch 17/100\n","106/106 [==============================] - 2s 23ms/step - loss: 1.0099 - accuracy: 0.5810 - val_loss: 0.9630 - val_accuracy: 0.6235\n","Epoch 18/100\n","106/106 [==============================] - 3s 24ms/step - loss: 0.9712 - accuracy: 0.5983 - val_loss: 0.9708 - val_accuracy: 0.5969\n","Epoch 19/100\n","106/106 [==============================] - 3s 26ms/step - loss: 0.9505 - accuracy: 0.6150 - val_loss: 0.9345 - val_accuracy: 0.6353\n","Epoch 20/100\n","106/106 [==============================] - 2s 21ms/step - loss: 0.9460 - accuracy: 0.6132 - val_loss: 0.9720 - val_accuracy: 0.5887\n","Epoch 21/100\n","106/106 [==============================] - 2s 21ms/step - loss: 0.9343 - accuracy: 0.6204 - val_loss: 0.8973 - val_accuracy: 0.6407\n","Epoch 22/100\n","106/106 [==============================] - 2s 21ms/step - loss: 0.9317 - accuracy: 0.6175 - val_loss: 0.9428 - val_accuracy: 0.6152\n","Epoch 23/100\n","106/106 [==============================] - 2s 20ms/step - loss: 0.9072 - accuracy: 0.6296 - val_loss: 0.8663 - val_accuracy: 0.6673\n","Epoch 24/100\n","106/106 [==============================] - 2s 20ms/step - loss: 0.8929 - accuracy: 0.6399 - val_loss: 0.8552 - val_accuracy: 0.6749\n","Epoch 25/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.8872 - accuracy: 0.6395 - val_loss: 0.8715 - val_accuracy: 0.6525\n","Epoch 26/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.8987 - accuracy: 0.6325 - val_loss: 0.8576 - val_accuracy: 0.6602\n","Epoch 27/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.8815 - accuracy: 0.6436 - val_loss: 0.8645 - val_accuracy: 0.6501\n","Epoch 28/100\n","106/106 [==============================] - 2s 21ms/step - loss: 0.8687 - accuracy: 0.6495 - val_loss: 0.8323 - val_accuracy: 0.6761\n","Epoch 29/100\n","106/106 [==============================] - 2s 20ms/step - loss: 0.8574 - accuracy: 0.6483 - val_loss: 0.8394 - val_accuracy: 0.6832\n","Epoch 30/100\n","106/106 [==============================] - 2s 20ms/step - loss: 0.8618 - accuracy: 0.6554 - val_loss: 0.8359 - val_accuracy: 0.6684\n","Epoch 31/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.8301 - accuracy: 0.6615 - val_loss: 0.8230 - val_accuracy: 0.6726\n","Epoch 32/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.8360 - accuracy: 0.6596 - val_loss: 0.8743 - val_accuracy: 0.6424\n","Epoch 33/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.8376 - accuracy: 0.6622 - val_loss: 0.8228 - val_accuracy: 0.6738\n","Epoch 34/100\n","106/106 [==============================] - 2s 20ms/step - loss: 0.8264 - accuracy: 0.6692 - val_loss: 0.8164 - val_accuracy: 0.6767\n","Epoch 35/100\n","106/106 [==============================] - 2s 20ms/step - loss: 0.8302 - accuracy: 0.6637 - val_loss: 0.8146 - val_accuracy: 0.6785\n","Epoch 36/100\n","106/106 [==============================] - 2s 21ms/step - loss: 0.8154 - accuracy: 0.6687 - val_loss: 0.8177 - val_accuracy: 0.6909\n","Epoch 37/100\n","106/106 [==============================] - 2s 21ms/step - loss: 0.8109 - accuracy: 0.6723 - val_loss: 0.8059 - val_accuracy: 0.6879\n","Epoch 38/100\n","106/106 [==============================] - 2s 21ms/step - loss: 0.8295 - accuracy: 0.6624 - val_loss: 0.8288 - val_accuracy: 0.6631\n","Epoch 39/100\n","106/106 [==============================] - 3s 26ms/step - loss: 0.8068 - accuracy: 0.6775 - val_loss: 0.8055 - val_accuracy: 0.6850\n","Epoch 40/100\n","106/106 [==============================] - 2s 20ms/step - loss: 0.7979 - accuracy: 0.6865 - val_loss: 0.8174 - val_accuracy: 0.6814\n","Epoch 41/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.8009 - accuracy: 0.6766 - val_loss: 0.8090 - val_accuracy: 0.6838\n","Epoch 42/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.8013 - accuracy: 0.6825 - val_loss: 0.7789 - val_accuracy: 0.7057\n","Epoch 43/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.7954 - accuracy: 0.6814 - val_loss: 0.7788 - val_accuracy: 0.7104\n","Epoch 44/100\n","106/106 [==============================] - 2s 20ms/step - loss: 0.7908 - accuracy: 0.6856 - val_loss: 0.8370 - val_accuracy: 0.6655\n","Epoch 45/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.7835 - accuracy: 0.6875 - val_loss: 0.8170 - val_accuracy: 0.6631\n","Epoch 46/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.7883 - accuracy: 0.6801 - val_loss: 0.8076 - val_accuracy: 0.6885\n","Epoch 47/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.7875 - accuracy: 0.6819 - val_loss: 0.7836 - val_accuracy: 0.7128\n","Epoch 48/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.7782 - accuracy: 0.6882 - val_loss: 0.8463 - val_accuracy: 0.6466\n","Epoch 49/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.7690 - accuracy: 0.6900 - val_loss: 0.7698 - val_accuracy: 0.7157\n","Epoch 50/100\n","106/106 [==============================] - 2s 20ms/step - loss: 0.7683 - accuracy: 0.6925 - val_loss: 0.7776 - val_accuracy: 0.7134\n","Epoch 51/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.7652 - accuracy: 0.6927 - val_loss: 0.7937 - val_accuracy: 0.6968\n","Epoch 52/100\n","106/106 [==============================] - 2s 20ms/step - loss: 0.7730 - accuracy: 0.6874 - val_loss: 0.7728 - val_accuracy: 0.7009\n","Epoch 53/100\n","106/106 [==============================] - 3s 24ms/step - loss: 0.7736 - accuracy: 0.6922 - val_loss: 0.7774 - val_accuracy: 0.6992\n","Epoch 54/100\n","106/106 [==============================] - 2s 20ms/step - loss: 0.7585 - accuracy: 0.7005 - val_loss: 0.7665 - val_accuracy: 0.7033\n","Epoch 55/100\n","106/106 [==============================] - 2s 20ms/step - loss: 0.7623 - accuracy: 0.6918 - val_loss: 0.7706 - val_accuracy: 0.7027\n","Epoch 56/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.7544 - accuracy: 0.6984 - val_loss: 0.7647 - val_accuracy: 0.7015\n","Epoch 57/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.7455 - accuracy: 0.7030 - val_loss: 0.7811 - val_accuracy: 0.7069\n","Epoch 58/100\n","106/106 [==============================] - 3s 24ms/step - loss: 0.7500 - accuracy: 0.7045 - val_loss: 0.7993 - val_accuracy: 0.6891\n","Epoch 59/100\n","106/106 [==============================] - 2s 21ms/step - loss: 0.7449 - accuracy: 0.7018 - val_loss: 0.7717 - val_accuracy: 0.7128\n","Epoch 60/100\n","106/106 [==============================] - 2s 20ms/step - loss: 0.7450 - accuracy: 0.6987 - val_loss: 0.7629 - val_accuracy: 0.7199\n","Epoch 61/100\n","106/106 [==============================] - 2s 22ms/step - loss: 0.7514 - accuracy: 0.6975 - val_loss: 0.8045 - val_accuracy: 0.6743\n","Epoch 62/100\n","106/106 [==============================] - 2s 21ms/step - loss: 0.7471 - accuracy: 0.7007 - val_loss: 0.7656 - val_accuracy: 0.7181\n","Epoch 63/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.7342 - accuracy: 0.7042 - val_loss: 0.7747 - val_accuracy: 0.6998\n","Epoch 64/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.7274 - accuracy: 0.7042 - val_loss: 0.7654 - val_accuracy: 0.7134\n","Epoch 65/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.7232 - accuracy: 0.7100 - val_loss: 0.7663 - val_accuracy: 0.7027\n","Epoch 66/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.7204 - accuracy: 0.7154 - val_loss: 0.7745 - val_accuracy: 0.7063\n","Epoch 67/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.7239 - accuracy: 0.7092 - val_loss: 0.7629 - val_accuracy: 0.7039\n","Epoch 68/100\n","106/106 [==============================] - 2s 21ms/step - loss: 0.7340 - accuracy: 0.7027 - val_loss: 0.7802 - val_accuracy: 0.6962\n","Epoch 69/100\n","106/106 [==============================] - 2s 20ms/step - loss: 0.7299 - accuracy: 0.7131 - val_loss: 0.7575 - val_accuracy: 0.7187\n","Epoch 70/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.7134 - accuracy: 0.7108 - val_loss: 0.7718 - val_accuracy: 0.6974\n","Epoch 71/100\n","106/106 [==============================] - 2s 22ms/step - loss: 0.7162 - accuracy: 0.7194 - val_loss: 0.7397 - val_accuracy: 0.7335\n","Epoch 72/100\n","106/106 [==============================] - 2s 20ms/step - loss: 0.7233 - accuracy: 0.7131 - val_loss: 0.7678 - val_accuracy: 0.6974\n","Epoch 73/100\n","106/106 [==============================] - 2s 20ms/step - loss: 0.7201 - accuracy: 0.7153 - val_loss: 0.7453 - val_accuracy: 0.7193\n","Epoch 74/100\n","106/106 [==============================] - 2s 22ms/step - loss: 0.7104 - accuracy: 0.7202 - val_loss: 0.7361 - val_accuracy: 0.7323\n","Epoch 75/100\n","106/106 [==============================] - 2s 20ms/step - loss: 0.7250 - accuracy: 0.7091 - val_loss: 0.7539 - val_accuracy: 0.7139\n","Epoch 76/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.7185 - accuracy: 0.7148 - val_loss: 0.7460 - val_accuracy: 0.7240\n","Epoch 77/100\n","106/106 [==============================] - 3s 29ms/step - loss: 0.7161 - accuracy: 0.7168 - val_loss: 0.7617 - val_accuracy: 0.7027\n","Epoch 78/100\n","106/106 [==============================] - 2s 22ms/step - loss: 0.7235 - accuracy: 0.7126 - val_loss: 0.7538 - val_accuracy: 0.7234\n","Epoch 79/100\n","106/106 [==============================] - 2s 21ms/step - loss: 0.7124 - accuracy: 0.7185 - val_loss: 0.7480 - val_accuracy: 0.7216\n","Epoch 80/100\n","106/106 [==============================] - 2s 24ms/step - loss: 0.7050 - accuracy: 0.7175 - val_loss: 0.7385 - val_accuracy: 0.7329\n","Epoch 81/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.7062 - accuracy: 0.7240 - val_loss: 0.7321 - val_accuracy: 0.7299\n","Epoch 82/100\n","106/106 [==============================] - 2s 20ms/step - loss: 0.7039 - accuracy: 0.7187 - val_loss: 0.7344 - val_accuracy: 0.7252\n","Epoch 83/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.7060 - accuracy: 0.7227 - val_loss: 0.7532 - val_accuracy: 0.7080\n","Epoch 84/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.7002 - accuracy: 0.7219 - val_loss: 0.7525 - val_accuracy: 0.7015\n","Epoch 85/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.7061 - accuracy: 0.7153 - val_loss: 0.7293 - val_accuracy: 0.7293\n","Epoch 86/100\n","106/106 [==============================] - 2s 19ms/step - loss: 0.6859 - accuracy: 0.7357 - val_loss: 0.7352 - val_accuracy: 0.7163\n","Epoch 87/100\n","106/106 [==============================] - 2s 20ms/step - loss: 0.6912 - accuracy: 0.7243 - val_loss: 0.7427 - val_accuracy: 0.7175\n","Epoch 88/100\n","106/106 [==============================] - 3s 27ms/step - loss: 0.6935 - accuracy: 0.7240 - val_loss: 0.7315 - val_accuracy: 0.7299\n","Epoch 89/100\n","106/106 [==============================] - 4s 34ms/step - loss: 0.6763 - accuracy: 0.7337 - val_loss: 0.7372 - val_accuracy: 0.7364\n","Epoch 90/100\n","106/106 [==============================] - 3s 26ms/step - loss: 0.6961 - accuracy: 0.7219 - val_loss: 0.7383 - val_accuracy: 0.7317\n","Epoch 91/100\n","106/106 [==============================] - 3s 27ms/step - loss: 0.7064 - accuracy: 0.7156 - val_loss: 0.7487 - val_accuracy: 0.7275\n","Epoch 92/100\n","106/106 [==============================] - 3s 24ms/step - loss: 0.6978 - accuracy: 0.7244 - val_loss: 0.7307 - val_accuracy: 0.7240\n","Epoch 93/100\n","106/106 [==============================] - 3s 24ms/step - loss: 0.6891 - accuracy: 0.7231 - val_loss: 0.7315 - val_accuracy: 0.7222\n","Epoch 94/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.6815 - accuracy: 0.7270 - val_loss: 0.7342 - val_accuracy: 0.7287\n","Epoch 95/100\n","106/106 [==============================] - 2s 20ms/step - loss: 0.6802 - accuracy: 0.7315 - val_loss: 0.7166 - val_accuracy: 0.7364\n","Epoch 96/100\n","106/106 [==============================] - 2s 22ms/step - loss: 0.6851 - accuracy: 0.7336 - val_loss: 0.7123 - val_accuracy: 0.7346\n","Epoch 97/100\n","106/106 [==============================] - 4s 38ms/step - loss: 0.6707 - accuracy: 0.7267 - val_loss: 0.7336 - val_accuracy: 0.7275\n","Epoch 98/100\n","106/106 [==============================] - 4s 35ms/step - loss: 0.6869 - accuracy: 0.7287 - val_loss: 0.7312 - val_accuracy: 0.7299\n","Epoch 99/100\n","106/106 [==============================] - 2s 21ms/step - loss: 0.6882 - accuracy: 0.7246 - val_loss: 0.7267 - val_accuracy: 0.7240\n","Epoch 100/100\n","106/106 [==============================] - 3s 24ms/step - loss: 0.6878 - accuracy: 0.7357 - val_loss: 0.7147 - val_accuracy: 0.7429\n","56/56 [==============================] - 0s 4ms/step - loss: 0.7463 - accuracy: 0.7054\n","Shape of X after trimming: (1692, 22, 800)\n","Shape of X after trimming: (423, 22, 800)\n","Shape of X after trimming: (443, 22, 800)\n","Shape of training set: (6768, 22, 400)\n","Shape of validation set: (1692, 22, 400)\n","Shape of training labels: (6768,)\n","Shape of validation labels: (1692,)\n","Shape of testing set: (1772, 22, 400)\n","Shape of testing labels: (1772,)\n","Shape of training labels after categorical conversion: (6768, 4)\n","Shape of validation labels after categorical conversion: (1692, 4)\n","Shape of test labels after categorical conversion: (1772, 4)\n","Shape of training set after adding width info: (6768, 22, 400, 1)\n","Shape of validation set after adding width info: (1692, 22, 400, 1)\n","Shape of test set after adding width info: (1772, 22, 400, 1)\n","Shape of training set after dimension reshaping: (6768, 400, 1, 22)\n","Shape of validation set after dimension reshaping: (1692, 400, 1, 22)\n","Shape of test set after dimension reshaping: (1772, 400, 1, 22)\n","Epoch 1/100\n","106/106 [==============================] - 5s 38ms/step - loss: 2.0216 - accuracy: 0.2688 - val_loss: 1.4099 - val_accuracy: 0.3688\n","Epoch 2/100\n","106/106 [==============================] - 3s 28ms/step - loss: 1.6251 - accuracy: 0.3042 - val_loss: 1.3154 - val_accuracy: 0.3936\n","Epoch 3/100\n","106/106 [==============================] - 3s 29ms/step - loss: 1.4674 - accuracy: 0.3361 - val_loss: 1.2974 - val_accuracy: 0.4084\n","Epoch 4/100\n","106/106 [==============================] - 4s 36ms/step - loss: 1.3857 - accuracy: 0.3494 - val_loss: 1.2989 - val_accuracy: 0.3741\n","Epoch 5/100\n","106/106 [==============================] - 4s 33ms/step - loss: 1.3255 - accuracy: 0.3778 - val_loss: 1.2660 - val_accuracy: 0.4108\n","Epoch 6/100\n","106/106 [==============================] - 4s 36ms/step - loss: 1.2481 - accuracy: 0.4317 - val_loss: 1.2034 - val_accuracy: 0.4728\n","Epoch 7/100\n","106/106 [==============================] - 4s 34ms/step - loss: 1.2069 - accuracy: 0.4607 - val_loss: 1.1511 - val_accuracy: 0.4876\n","Epoch 8/100\n","106/106 [==============================] - 3s 26ms/step - loss: 1.1632 - accuracy: 0.4932 - val_loss: 1.1217 - val_accuracy: 0.5136\n","Epoch 9/100\n","106/106 [==============================] - 3s 24ms/step - loss: 1.1325 - accuracy: 0.5056 - val_loss: 1.0651 - val_accuracy: 0.5278\n","Epoch 10/100\n","106/106 [==============================] - 3s 24ms/step - loss: 1.1092 - accuracy: 0.5137 - val_loss: 1.0698 - val_accuracy: 0.5219\n","Epoch 11/100\n","106/106 [==============================] - 2s 23ms/step - loss: 1.0892 - accuracy: 0.5334 - val_loss: 1.0782 - val_accuracy: 0.5254\n","Epoch 12/100\n","106/106 [==============================] - 2s 23ms/step - loss: 1.0715 - accuracy: 0.5445 - val_loss: 1.0475 - val_accuracy: 0.5355\n","Epoch 13/100\n","106/106 [==============================] - 3s 26ms/step - loss: 1.0591 - accuracy: 0.5529 - val_loss: 1.0125 - val_accuracy: 0.5597\n","Epoch 14/100\n","106/106 [==============================] - 3s 26ms/step - loss: 1.0325 - accuracy: 0.5649 - val_loss: 1.0118 - val_accuracy: 0.5621\n","Epoch 15/100\n","106/106 [==============================] - 4s 37ms/step - loss: 1.0217 - accuracy: 0.5801 - val_loss: 1.0069 - val_accuracy: 0.5709\n","Epoch 16/100\n","106/106 [==============================] - 3s 26ms/step - loss: 1.0068 - accuracy: 0.5881 - val_loss: 1.0358 - val_accuracy: 0.5485\n","Epoch 17/100\n","106/106 [==============================] - 3s 27ms/step - loss: 0.9766 - accuracy: 0.5888 - val_loss: 0.9401 - val_accuracy: 0.6123\n","Epoch 18/100\n","106/106 [==============================] - 3s 26ms/step - loss: 0.9691 - accuracy: 0.6033 - val_loss: 0.9402 - val_accuracy: 0.6087\n","Epoch 19/100\n","106/106 [==============================] - 3s 33ms/step - loss: 0.9513 - accuracy: 0.6132 - val_loss: 0.9054 - val_accuracy: 0.6271\n","Epoch 20/100\n","106/106 [==============================] - 3s 25ms/step - loss: 0.9443 - accuracy: 0.6123 - val_loss: 0.9240 - val_accuracy: 0.6117\n","Epoch 21/100\n","106/106 [==============================] - 3s 27ms/step - loss: 0.9273 - accuracy: 0.6127 - val_loss: 0.9203 - val_accuracy: 0.6070\n","Epoch 22/100\n","106/106 [==============================] - 3s 28ms/step - loss: 0.9220 - accuracy: 0.6226 - val_loss: 0.8574 - val_accuracy: 0.6537\n","Epoch 23/100\n","106/106 [==============================] - 3s 26ms/step - loss: 0.9080 - accuracy: 0.6325 - val_loss: 0.9083 - val_accuracy: 0.6265\n","Epoch 24/100\n","106/106 [==============================] - 3s 29ms/step - loss: 0.9040 - accuracy: 0.6294 - val_loss: 0.8781 - val_accuracy: 0.6430\n","Epoch 25/100\n","106/106 [==============================] - 3s 26ms/step - loss: 0.8948 - accuracy: 0.6287 - val_loss: 0.8424 - val_accuracy: 0.6525\n","Epoch 26/100\n","106/106 [==============================] - 3s 25ms/step - loss: 0.8885 - accuracy: 0.6374 - val_loss: 0.8090 - val_accuracy: 0.6820\n","Epoch 27/100\n","106/106 [==============================] - 3s 28ms/step - loss: 0.8737 - accuracy: 0.6486 - val_loss: 0.7973 - val_accuracy: 0.6649\n","Epoch 28/100\n","106/106 [==============================] - 3s 32ms/step - loss: 0.8629 - accuracy: 0.6553 - val_loss: 0.8230 - val_accuracy: 0.6773\n","Epoch 29/100\n","106/106 [==============================] - 3s 29ms/step - loss: 0.8557 - accuracy: 0.6529 - val_loss: 0.8216 - val_accuracy: 0.6732\n","Epoch 30/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.8379 - accuracy: 0.6643 - val_loss: 0.8748 - val_accuracy: 0.6478\n","Epoch 31/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.8495 - accuracy: 0.6590 - val_loss: 0.8273 - val_accuracy: 0.6809\n","Epoch 32/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.8394 - accuracy: 0.6630 - val_loss: 0.7903 - val_accuracy: 0.6921\n","Epoch 33/100\n","106/106 [==============================] - 3s 26ms/step - loss: 0.8273 - accuracy: 0.6602 - val_loss: 0.8338 - val_accuracy: 0.6613\n","Epoch 34/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.8405 - accuracy: 0.6613 - val_loss: 0.8111 - val_accuracy: 0.6797\n","Epoch 35/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.8234 - accuracy: 0.6732 - val_loss: 0.8175 - val_accuracy: 0.6637\n","Epoch 36/100\n","106/106 [==============================] - 3s 24ms/step - loss: 0.8149 - accuracy: 0.6782 - val_loss: 0.8074 - val_accuracy: 0.6809\n","Epoch 37/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.8010 - accuracy: 0.6775 - val_loss: 0.8504 - val_accuracy: 0.6619\n","Epoch 38/100\n","106/106 [==============================] - 2s 22ms/step - loss: 0.8048 - accuracy: 0.6766 - val_loss: 0.8007 - val_accuracy: 0.6779\n","Epoch 39/100\n","106/106 [==============================] - 2s 22ms/step - loss: 0.8001 - accuracy: 0.6788 - val_loss: 0.7830 - val_accuracy: 0.6684\n","Epoch 40/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.7933 - accuracy: 0.6856 - val_loss: 0.7722 - val_accuracy: 0.6826\n","Epoch 41/100\n","106/106 [==============================] - 2s 22ms/step - loss: 0.7865 - accuracy: 0.6853 - val_loss: 0.8426 - val_accuracy: 0.6342\n","Epoch 42/100\n","106/106 [==============================] - 2s 22ms/step - loss: 0.8035 - accuracy: 0.6791 - val_loss: 0.7998 - val_accuracy: 0.6678\n","Epoch 43/100\n","106/106 [==============================] - 2s 22ms/step - loss: 0.7798 - accuracy: 0.6835 - val_loss: 0.8323 - val_accuracy: 0.6608\n","Epoch 44/100\n","106/106 [==============================] - 2s 22ms/step - loss: 0.7943 - accuracy: 0.6854 - val_loss: 0.8089 - val_accuracy: 0.6708\n","Epoch 45/100\n","106/106 [==============================] - 2s 21ms/step - loss: 0.7781 - accuracy: 0.6878 - val_loss: 0.7940 - val_accuracy: 0.6785\n","Epoch 46/100\n","106/106 [==============================] - 2s 22ms/step - loss: 0.7905 - accuracy: 0.6804 - val_loss: 0.7681 - val_accuracy: 0.6885\n","Epoch 47/100\n","106/106 [==============================] - 2s 22ms/step - loss: 0.7730 - accuracy: 0.6928 - val_loss: 0.7998 - val_accuracy: 0.6738\n","Epoch 48/100\n","106/106 [==============================] - 2s 22ms/step - loss: 0.7561 - accuracy: 0.6949 - val_loss: 0.7862 - val_accuracy: 0.6832\n","Epoch 49/100\n","106/106 [==============================] - 2s 22ms/step - loss: 0.7634 - accuracy: 0.6974 - val_loss: 0.8058 - val_accuracy: 0.6643\n","Epoch 50/100\n","106/106 [==============================] - 2s 22ms/step - loss: 0.7613 - accuracy: 0.7033 - val_loss: 0.8071 - val_accuracy: 0.6690\n","Epoch 51/100\n","106/106 [==============================] - 2s 22ms/step - loss: 0.7519 - accuracy: 0.7017 - val_loss: 0.8032 - val_accuracy: 0.6708\n","Epoch 52/100\n","106/106 [==============================] - 2s 22ms/step - loss: 0.7695 - accuracy: 0.6974 - val_loss: 0.7700 - val_accuracy: 0.6820\n","Epoch 53/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.7522 - accuracy: 0.7020 - val_loss: 0.8194 - val_accuracy: 0.6489\n","Epoch 54/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.7446 - accuracy: 0.7057 - val_loss: 0.7533 - val_accuracy: 0.7151\n","Epoch 55/100\n","106/106 [==============================] - 4s 41ms/step - loss: 0.7376 - accuracy: 0.7060 - val_loss: 0.7454 - val_accuracy: 0.6956\n","Epoch 56/100\n","106/106 [==============================] - 5s 45ms/step - loss: 0.7644 - accuracy: 0.6970 - val_loss: 0.7804 - val_accuracy: 0.6838\n","Epoch 57/100\n","106/106 [==============================] - 3s 25ms/step - loss: 0.7470 - accuracy: 0.7030 - val_loss: 0.7443 - val_accuracy: 0.6891\n","Epoch 58/100\n","106/106 [==============================] - 3s 26ms/step - loss: 0.7328 - accuracy: 0.7080 - val_loss: 0.7697 - val_accuracy: 0.6773\n","Epoch 59/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.7290 - accuracy: 0.7166 - val_loss: 0.7778 - val_accuracy: 0.6838\n","Epoch 60/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.7313 - accuracy: 0.7086 - val_loss: 0.7745 - val_accuracy: 0.6879\n","Epoch 61/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.7194 - accuracy: 0.7171 - val_loss: 0.7933 - val_accuracy: 0.6862\n","Epoch 62/100\n","106/106 [==============================] - 2s 22ms/step - loss: 0.7374 - accuracy: 0.7045 - val_loss: 0.7503 - val_accuracy: 0.6921\n","Epoch 63/100\n","106/106 [==============================] - 2s 22ms/step - loss: 0.7342 - accuracy: 0.7077 - val_loss: 0.8320 - val_accuracy: 0.6602\n","Epoch 64/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.7371 - accuracy: 0.7054 - val_loss: 0.7417 - val_accuracy: 0.6962\n","Epoch 65/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.7383 - accuracy: 0.7128 - val_loss: 0.7184 - val_accuracy: 0.7134\n","Epoch 66/100\n","106/106 [==============================] - 2s 22ms/step - loss: 0.7278 - accuracy: 0.7067 - val_loss: 0.7563 - val_accuracy: 0.7039\n","Epoch 67/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.7208 - accuracy: 0.7169 - val_loss: 0.7855 - val_accuracy: 0.6933\n","Epoch 68/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.7192 - accuracy: 0.7141 - val_loss: 0.7459 - val_accuracy: 0.7033\n","Epoch 69/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.7075 - accuracy: 0.7162 - val_loss: 0.7687 - val_accuracy: 0.6885\n","Epoch 70/100\n","106/106 [==============================] - 2s 22ms/step - loss: 0.7203 - accuracy: 0.7199 - val_loss: 0.8004 - val_accuracy: 0.6856\n","Epoch 71/100\n","106/106 [==============================] - 2s 22ms/step - loss: 0.7107 - accuracy: 0.7249 - val_loss: 0.7671 - val_accuracy: 0.6885\n","Epoch 72/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.7060 - accuracy: 0.7193 - val_loss: 0.7550 - val_accuracy: 0.6974\n","Epoch 73/100\n","106/106 [==============================] - 2s 22ms/step - loss: 0.7019 - accuracy: 0.7215 - val_loss: 0.8070 - val_accuracy: 0.6791\n","Epoch 74/100\n","106/106 [==============================] - 2s 22ms/step - loss: 0.7077 - accuracy: 0.7156 - val_loss: 0.7831 - val_accuracy: 0.6874\n","Epoch 75/100\n","106/106 [==============================] - 3s 25ms/step - loss: 0.7014 - accuracy: 0.7190 - val_loss: 0.7810 - val_accuracy: 0.6844\n","Epoch 76/100\n","106/106 [==============================] - 3s 24ms/step - loss: 0.7091 - accuracy: 0.7173 - val_loss: 0.7662 - val_accuracy: 0.7004\n","Epoch 77/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.6966 - accuracy: 0.7259 - val_loss: 0.7240 - val_accuracy: 0.6998\n","Epoch 78/100\n","106/106 [==============================] - 3s 24ms/step - loss: 0.6996 - accuracy: 0.7230 - val_loss: 0.7915 - val_accuracy: 0.6921\n","Epoch 79/100\n","106/106 [==============================] - 3s 24ms/step - loss: 0.6907 - accuracy: 0.7283 - val_loss: 0.7608 - val_accuracy: 0.6939\n","Epoch 80/100\n","106/106 [==============================] - 3s 25ms/step - loss: 0.6999 - accuracy: 0.7258 - val_loss: 0.7432 - val_accuracy: 0.6968\n","Epoch 81/100\n","106/106 [==============================] - 3s 26ms/step - loss: 0.7121 - accuracy: 0.7210 - val_loss: 0.7182 - val_accuracy: 0.7092\n","Epoch 82/100\n","106/106 [==============================] - 3s 27ms/step - loss: 0.6880 - accuracy: 0.7289 - val_loss: 0.7606 - val_accuracy: 0.6915\n","Epoch 83/100\n","106/106 [==============================] - 3s 28ms/step - loss: 0.6923 - accuracy: 0.7265 - val_loss: 0.7804 - val_accuracy: 0.7015\n","Epoch 84/100\n","106/106 [==============================] - 3s 25ms/step - loss: 0.6987 - accuracy: 0.7274 - val_loss: 0.7422 - val_accuracy: 0.7009\n","Epoch 85/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.6973 - accuracy: 0.7243 - val_loss: 0.7266 - val_accuracy: 0.7086\n","Epoch 86/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.7079 - accuracy: 0.7202 - val_loss: 0.7401 - val_accuracy: 0.7086\n","Epoch 87/100\n","106/106 [==============================] - 3s 24ms/step - loss: 0.6903 - accuracy: 0.7335 - val_loss: 0.7235 - val_accuracy: 0.7163\n","Epoch 88/100\n","106/106 [==============================] - 3s 24ms/step - loss: 0.6860 - accuracy: 0.7357 - val_loss: 0.8028 - val_accuracy: 0.6826\n","Epoch 89/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.6873 - accuracy: 0.7268 - val_loss: 0.7705 - val_accuracy: 0.6962\n","Epoch 90/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.6887 - accuracy: 0.7272 - val_loss: 0.7715 - val_accuracy: 0.6974\n","Epoch 91/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.7003 - accuracy: 0.7253 - val_loss: 0.7621 - val_accuracy: 0.6950\n","Epoch 92/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.6821 - accuracy: 0.7343 - val_loss: 0.7832 - val_accuracy: 0.6820\n","Epoch 93/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.6815 - accuracy: 0.7265 - val_loss: 0.7488 - val_accuracy: 0.7045\n","Epoch 94/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.6785 - accuracy: 0.7329 - val_loss: 0.8305 - val_accuracy: 0.6714\n","Epoch 95/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.6747 - accuracy: 0.7333 - val_loss: 0.7366 - val_accuracy: 0.7187\n","Epoch 96/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.6678 - accuracy: 0.7373 - val_loss: 0.7210 - val_accuracy: 0.7134\n","Epoch 97/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.6727 - accuracy: 0.7351 - val_loss: 0.7264 - val_accuracy: 0.7086\n","Epoch 98/100\n","106/106 [==============================] - 3s 24ms/step - loss: 0.6611 - accuracy: 0.7435 - val_loss: 0.7527 - val_accuracy: 0.7021\n","Epoch 99/100\n","106/106 [==============================] - 3s 24ms/step - loss: 0.6693 - accuracy: 0.7312 - val_loss: 0.7583 - val_accuracy: 0.6832\n","Epoch 100/100\n","106/106 [==============================] - 2s 23ms/step - loss: 0.6668 - accuracy: 0.7336 - val_loss: 0.7495 - val_accuracy: 0.7004\n","56/56 [==============================] - 0s 5ms/step - loss: 0.7398 - accuracy: 0.7026\n","Shape of X after trimming: (1692, 22, 900)\n","Shape of X after trimming: (423, 22, 900)\n","Shape of X after trimming: (443, 22, 900)\n","Shape of training set: (6768, 22, 450)\n","Shape of validation set: (1692, 22, 450)\n","Shape of training labels: (6768,)\n","Shape of validation labels: (1692,)\n","Shape of testing set: (1772, 22, 450)\n","Shape of testing labels: (1772,)\n","Shape of training labels after categorical conversion: (6768, 4)\n","Shape of validation labels after categorical conversion: (1692, 4)\n","Shape of test labels after categorical conversion: (1772, 4)\n","Shape of training set after adding width info: (6768, 22, 450, 1)\n","Shape of validation set after adding width info: (1692, 22, 450, 1)\n","Shape of test set after adding width info: (1772, 22, 450, 1)\n","Shape of training set after dimension reshaping: (6768, 450, 1, 22)\n","Shape of validation set after dimension reshaping: (1692, 450, 1, 22)\n","Shape of test set after dimension reshaping: (1772, 450, 1, 22)\n","Epoch 1/100\n","106/106 [==============================] - 4s 27ms/step - loss: 2.0161 - accuracy: 0.2713 - val_loss: 1.5265 - val_accuracy: 0.3416\n","Epoch 2/100\n","106/106 [==============================] - 3s 24ms/step - loss: 1.6102 - accuracy: 0.2968 - val_loss: 1.3316 - val_accuracy: 0.3599\n","Epoch 3/100\n","106/106 [==============================] - 4s 34ms/step - loss: 1.4428 - accuracy: 0.3324 - val_loss: 1.3177 - val_accuracy: 0.3511\n","Epoch 4/100\n","106/106 [==============================] - 3s 25ms/step - loss: 1.3514 - accuracy: 0.3652 - val_loss: 1.2991 - val_accuracy: 0.3712\n","Epoch 5/100\n","106/106 [==============================] - 3s 30ms/step - loss: 1.2896 - accuracy: 0.4035 - val_loss: 1.2367 - val_accuracy: 0.4226\n","Epoch 6/100\n","106/106 [==============================] - 3s 32ms/step - loss: 1.2314 - accuracy: 0.4378 - val_loss: 1.1715 - val_accuracy: 0.4864\n","Epoch 7/100\n","106/106 [==============================] - 4s 34ms/step - loss: 1.1973 - accuracy: 0.4605 - val_loss: 1.1518 - val_accuracy: 0.4799\n","Epoch 8/100\n","106/106 [==============================] - 5s 48ms/step - loss: 1.1616 - accuracy: 0.4835 - val_loss: 1.1209 - val_accuracy: 0.4994\n","Epoch 9/100\n","106/106 [==============================] - 3s 30ms/step - loss: 1.1328 - accuracy: 0.5140 - val_loss: 1.0854 - val_accuracy: 0.5242\n","Epoch 10/100\n","106/106 [==============================] - 3s 29ms/step - loss: 1.1136 - accuracy: 0.5294 - val_loss: 1.0737 - val_accuracy: 0.5437\n","Epoch 11/100\n","106/106 [==============================] - 4s 34ms/step - loss: 1.0946 - accuracy: 0.5347 - val_loss: 1.0327 - val_accuracy: 0.5561\n","Epoch 12/100\n","106/106 [==============================] - 3s 30ms/step - loss: 1.0863 - accuracy: 0.5381 - val_loss: 1.0146 - val_accuracy: 0.5745\n","Epoch 13/100\n","106/106 [==============================] - 3s 26ms/step - loss: 1.0538 - accuracy: 0.5541 - val_loss: 0.9778 - val_accuracy: 0.5981\n","Epoch 14/100\n","106/106 [==============================] - 3s 25ms/step - loss: 1.0349 - accuracy: 0.5677 - val_loss: 0.9971 - val_accuracy: 0.5934\n","Epoch 15/100\n","106/106 [==============================] - 3s 26ms/step - loss: 1.0120 - accuracy: 0.5770 - val_loss: 0.9122 - val_accuracy: 0.6596\n","Epoch 16/100\n","106/106 [==============================] - 3s 24ms/step - loss: 1.0014 - accuracy: 0.5810 - val_loss: 0.9123 - val_accuracy: 0.6525\n","Epoch 17/100\n","106/106 [==============================] - 3s 24ms/step - loss: 0.9801 - accuracy: 0.5928 - val_loss: 0.9079 - val_accuracy: 0.6489\n","Epoch 18/100\n","106/106 [==============================] - 3s 25ms/step - loss: 0.9701 - accuracy: 0.6037 - val_loss: 0.9233 - val_accuracy: 0.6288\n","Epoch 19/100\n","106/106 [==============================] - 3s 25ms/step - loss: 0.9469 - accuracy: 0.6077 - val_loss: 0.9257 - val_accuracy: 0.6223\n","Epoch 20/100\n","106/106 [==============================] - 3s 27ms/step - loss: 0.9347 - accuracy: 0.6176 - val_loss: 0.8848 - val_accuracy: 0.6330\n","Epoch 21/100\n","106/106 [==============================] - 3s 27ms/step - loss: 0.9299 - accuracy: 0.6206 - val_loss: 0.8526 - val_accuracy: 0.6797\n","Epoch 22/100\n","106/106 [==============================] - 3s 26ms/step - loss: 0.9112 - accuracy: 0.6256 - val_loss: 0.8272 - val_accuracy: 0.6767\n","Epoch 23/100\n","106/106 [==============================] - 3s 25ms/step - loss: 0.9060 - accuracy: 0.6305 - val_loss: 0.8280 - val_accuracy: 0.6838\n","Epoch 24/100\n","106/106 [==============================] - 3s 27ms/step - loss: 0.8882 - accuracy: 0.6418 - val_loss: 0.8484 - val_accuracy: 0.6720\n","Epoch 25/100\n","106/106 [==============================] - 3s 28ms/step - loss: 0.8975 - accuracy: 0.6377 - val_loss: 0.7933 - val_accuracy: 0.6998\n","Epoch 26/100\n","106/106 [==============================] - 3s 27ms/step - loss: 0.8828 - accuracy: 0.6376 - val_loss: 0.8209 - val_accuracy: 0.6844\n","Epoch 27/100\n","106/106 [==============================] - 3s 26ms/step - loss: 0.8657 - accuracy: 0.6556 - val_loss: 0.8044 - val_accuracy: 0.6909\n","Epoch 28/100\n","106/106 [==============================] - 3s 26ms/step - loss: 0.8650 - accuracy: 0.6478 - val_loss: 0.8142 - val_accuracy: 0.6832\n","Epoch 29/100\n","106/106 [==============================] - 3s 31ms/step - loss: 0.8626 - accuracy: 0.6466 - val_loss: 0.7907 - val_accuracy: 0.6944\n","Epoch 30/100\n","106/106 [==============================] - 3s 31ms/step - loss: 0.8400 - accuracy: 0.6572 - val_loss: 0.7834 - val_accuracy: 0.6903\n","Epoch 31/100\n","106/106 [==============================] - 3s 32ms/step - loss: 0.8367 - accuracy: 0.6649 - val_loss: 0.7468 - val_accuracy: 0.7252\n","Epoch 32/100\n","106/106 [==============================] - 3s 30ms/step - loss: 0.8336 - accuracy: 0.6597 - val_loss: 0.7688 - val_accuracy: 0.7074\n","Epoch 33/100\n","106/106 [==============================] - 3s 29ms/step - loss: 0.8310 - accuracy: 0.6599 - val_loss: 0.7843 - val_accuracy: 0.6998\n","Epoch 34/100\n","106/106 [==============================] - 3s 26ms/step - loss: 0.8142 - accuracy: 0.6684 - val_loss: 0.8189 - val_accuracy: 0.6755\n","Epoch 35/100\n","106/106 [==============================] - 3s 25ms/step - loss: 0.8228 - accuracy: 0.6718 - val_loss: 0.7432 - val_accuracy: 0.7199\n","Epoch 36/100\n","106/106 [==============================] - 3s 25ms/step - loss: 0.8175 - accuracy: 0.6677 - val_loss: 0.7378 - val_accuracy: 0.7163\n","Epoch 37/100\n","106/106 [==============================] - 3s 25ms/step - loss: 0.8150 - accuracy: 0.6698 - val_loss: 0.7699 - val_accuracy: 0.7004\n","Epoch 38/100\n","106/106 [==============================] - 3s 25ms/step - loss: 0.8097 - accuracy: 0.6724 - val_loss: 0.7308 - val_accuracy: 0.7134\n","Epoch 39/100\n","106/106 [==============================] - 3s 25ms/step - loss: 0.8071 - accuracy: 0.6693 - val_loss: 0.7489 - val_accuracy: 0.7157\n","Epoch 40/100\n","106/106 [==============================] - 3s 27ms/step - loss: 0.7927 - accuracy: 0.6874 - val_loss: 0.7367 - val_accuracy: 0.7163\n","Epoch 41/100\n","106/106 [==============================] - 3s 27ms/step - loss: 0.7995 - accuracy: 0.6769 - val_loss: 0.7309 - val_accuracy: 0.7193\n","Epoch 42/100\n","106/106 [==============================] - 3s 26ms/step - loss: 0.7888 - accuracy: 0.6841 - val_loss: 0.7255 - val_accuracy: 0.7151\n","Epoch 43/100\n","106/106 [==============================] - 3s 26ms/step - loss: 0.7899 - accuracy: 0.6804 - val_loss: 0.7058 - val_accuracy: 0.7382\n","Epoch 44/100\n","106/106 [==============================] - 3s 28ms/step - loss: 0.7754 - accuracy: 0.6859 - val_loss: 0.7315 - val_accuracy: 0.7139\n","Epoch 45/100\n","106/106 [==============================] - 3s 26ms/step - loss: 0.7808 - accuracy: 0.6854 - val_loss: 0.7572 - val_accuracy: 0.6915\n","Epoch 46/100\n","106/106 [==============================] - 3s 29ms/step - loss: 0.7681 - accuracy: 0.6965 - val_loss: 0.7299 - val_accuracy: 0.7063\n","Epoch 47/100\n","106/106 [==============================] - 3s 30ms/step - loss: 0.7541 - accuracy: 0.7023 - val_loss: 0.6944 - val_accuracy: 0.7405\n","Epoch 48/100\n","106/106 [==============================] - 3s 26ms/step - loss: 0.7663 - accuracy: 0.6975 - val_loss: 0.6859 - val_accuracy: 0.7317\n","Epoch 49/100\n","106/106 [==============================] - 3s 27ms/step - loss: 0.7592 - accuracy: 0.6981 - val_loss: 0.6908 - val_accuracy: 0.7400\n","Epoch 50/100\n","106/106 [==============================] - 3s 26ms/step - loss: 0.7699 - accuracy: 0.6860 - val_loss: 0.7116 - val_accuracy: 0.7246\n","Epoch 51/100\n","106/106 [==============================] - 3s 28ms/step - loss: 0.7505 - accuracy: 0.7018 - val_loss: 0.7068 - val_accuracy: 0.7116\n","Epoch 52/100\n","106/106 [==============================] - 3s 29ms/step - loss: 0.7674 - accuracy: 0.6913 - val_loss: 0.6767 - val_accuracy: 0.7459\n","Epoch 53/100\n","106/106 [==============================] - 3s 31ms/step - loss: 0.7660 - accuracy: 0.6961 - val_loss: 0.6976 - val_accuracy: 0.7299\n","Epoch 54/100\n","106/106 [==============================] - 3s 29ms/step - loss: 0.7336 - accuracy: 0.7088 - val_loss: 0.6788 - val_accuracy: 0.7465\n","Epoch 55/100\n","106/106 [==============================] - 3s 29ms/step - loss: 0.7507 - accuracy: 0.6908 - val_loss: 0.6937 - val_accuracy: 0.7305\n","Epoch 56/100\n","106/106 [==============================] - 3s 28ms/step - loss: 0.7357 - accuracy: 0.7045 - val_loss: 0.6952 - val_accuracy: 0.7358\n","Epoch 57/100\n","106/106 [==============================] - 3s 27ms/step - loss: 0.7231 - accuracy: 0.7094 - val_loss: 0.6803 - val_accuracy: 0.7311\n","Epoch 58/100\n","106/106 [==============================] - 3s 27ms/step - loss: 0.7284 - accuracy: 0.7091 - val_loss: 0.7062 - val_accuracy: 0.7193\n","Epoch 59/100\n","106/106 [==============================] - 3s 30ms/step - loss: 0.7395 - accuracy: 0.7018 - val_loss: 0.6672 - val_accuracy: 0.7589\n","Epoch 60/100\n","106/106 [==============================] - 3s 29ms/step - loss: 0.7349 - accuracy: 0.7035 - val_loss: 0.6936 - val_accuracy: 0.7240\n","Epoch 61/100\n","106/106 [==============================] - 3s 28ms/step - loss: 0.7297 - accuracy: 0.7057 - val_loss: 0.6707 - val_accuracy: 0.7459\n","Epoch 62/100\n","106/106 [==============================] - 3s 28ms/step - loss: 0.7291 - accuracy: 0.7097 - val_loss: 0.6638 - val_accuracy: 0.7423\n","Epoch 63/100\n","106/106 [==============================] - 3s 28ms/step - loss: 0.7202 - accuracy: 0.7163 - val_loss: 0.6749 - val_accuracy: 0.7388\n","Epoch 64/100\n","106/106 [==============================] - 3s 28ms/step - loss: 0.7342 - accuracy: 0.7072 - val_loss: 0.6745 - val_accuracy: 0.7518\n","Epoch 65/100\n","106/106 [==============================] - 3s 28ms/step - loss: 0.7182 - accuracy: 0.7156 - val_loss: 0.6827 - val_accuracy: 0.7358\n","Epoch 66/100\n","106/106 [==============================] - 3s 28ms/step - loss: 0.7212 - accuracy: 0.7148 - val_loss: 0.6853 - val_accuracy: 0.7199\n","Epoch 67/100\n","106/106 [==============================] - 3s 29ms/step - loss: 0.7198 - accuracy: 0.7142 - val_loss: 0.6742 - val_accuracy: 0.7346\n","Epoch 68/100\n","106/106 [==============================] - 3s 29ms/step - loss: 0.7162 - accuracy: 0.7138 - val_loss: 0.6548 - val_accuracy: 0.7571\n","Epoch 69/100\n","106/106 [==============================] - 3s 30ms/step - loss: 0.7228 - accuracy: 0.7165 - val_loss: 0.6636 - val_accuracy: 0.7459\n","Epoch 70/100\n","106/106 [==============================] - 3s 29ms/step - loss: 0.7147 - accuracy: 0.7134 - val_loss: 0.6850 - val_accuracy: 0.7323\n","Epoch 71/100\n","106/106 [==============================] - 3s 28ms/step - loss: 0.7171 - accuracy: 0.7092 - val_loss: 0.6592 - val_accuracy: 0.7482\n","Epoch 72/100\n","106/106 [==============================] - 3s 31ms/step - loss: 0.6991 - accuracy: 0.7250 - val_loss: 0.6701 - val_accuracy: 0.7317\n","Epoch 73/100\n","106/106 [==============================] - 3s 29ms/step - loss: 0.7098 - accuracy: 0.7181 - val_loss: 0.6945 - val_accuracy: 0.7145\n","Epoch 74/100\n","106/106 [==============================] - 3s 30ms/step - loss: 0.7123 - accuracy: 0.7221 - val_loss: 0.6807 - val_accuracy: 0.7358\n","Epoch 75/100\n","106/106 [==============================] - 3s 30ms/step - loss: 0.7134 - accuracy: 0.7249 - val_loss: 0.6452 - val_accuracy: 0.7488\n","Epoch 76/100\n","106/106 [==============================] - 3s 30ms/step - loss: 0.7028 - accuracy: 0.7250 - val_loss: 0.6765 - val_accuracy: 0.7411\n","Epoch 77/100\n","106/106 [==============================] - 3s 31ms/step - loss: 0.7031 - accuracy: 0.7241 - val_loss: 0.6437 - val_accuracy: 0.7583\n","Epoch 78/100\n","106/106 [==============================] - 3s 31ms/step - loss: 0.7064 - accuracy: 0.7218 - val_loss: 0.6595 - val_accuracy: 0.7476\n","Epoch 79/100\n","106/106 [==============================] - 3s 29ms/step - loss: 0.7146 - accuracy: 0.7160 - val_loss: 0.6492 - val_accuracy: 0.7530\n","Epoch 80/100\n","106/106 [==============================] - 3s 30ms/step - loss: 0.7105 - accuracy: 0.7148 - val_loss: 0.6723 - val_accuracy: 0.7346\n","Epoch 81/100\n","106/106 [==============================] - 3s 29ms/step - loss: 0.6944 - accuracy: 0.7222 - val_loss: 0.6500 - val_accuracy: 0.7352\n","Epoch 82/100\n","106/106 [==============================] - 3s 30ms/step - loss: 0.7054 - accuracy: 0.7225 - val_loss: 0.6525 - val_accuracy: 0.7470\n","Epoch 83/100\n","106/106 [==============================] - 3s 30ms/step - loss: 0.6914 - accuracy: 0.7261 - val_loss: 0.6504 - val_accuracy: 0.7488\n","Epoch 84/100\n","106/106 [==============================] - 3s 29ms/step - loss: 0.6904 - accuracy: 0.7302 - val_loss: 0.6712 - val_accuracy: 0.7370\n","Epoch 85/100\n","106/106 [==============================] - 3s 29ms/step - loss: 0.6839 - accuracy: 0.7246 - val_loss: 0.6376 - val_accuracy: 0.7595\n","Epoch 86/100\n","106/106 [==============================] - 3s 30ms/step - loss: 0.7041 - accuracy: 0.7162 - val_loss: 0.6718 - val_accuracy: 0.7453\n","Epoch 87/100\n","106/106 [==============================] - 3s 29ms/step - loss: 0.6940 - accuracy: 0.7230 - val_loss: 0.6599 - val_accuracy: 0.7488\n","Epoch 88/100\n","106/106 [==============================] - 3s 31ms/step - loss: 0.6948 - accuracy: 0.7256 - val_loss: 0.6458 - val_accuracy: 0.7636\n","Epoch 89/100\n","106/106 [==============================] - 3s 30ms/step - loss: 0.6957 - accuracy: 0.7233 - val_loss: 0.6773 - val_accuracy: 0.7228\n","Epoch 90/100\n","106/106 [==============================] - 3s 31ms/step - loss: 0.6958 - accuracy: 0.7249 - val_loss: 0.6271 - val_accuracy: 0.7606\n","Epoch 91/100\n","106/106 [==============================] - 3s 30ms/step - loss: 0.6882 - accuracy: 0.7255 - val_loss: 0.6594 - val_accuracy: 0.7370\n","Epoch 92/100\n","106/106 [==============================] - 3s 33ms/step - loss: 0.6735 - accuracy: 0.7326 - val_loss: 0.6379 - val_accuracy: 0.7512\n","Epoch 93/100\n","106/106 [==============================] - 4s 35ms/step - loss: 0.6788 - accuracy: 0.7312 - val_loss: 0.6345 - val_accuracy: 0.7565\n","Epoch 94/100\n","106/106 [==============================] - 4s 34ms/step - loss: 0.6887 - accuracy: 0.7264 - val_loss: 0.6386 - val_accuracy: 0.7541\n","Epoch 95/100\n","106/106 [==============================] - 4s 35ms/step - loss: 0.6929 - accuracy: 0.7277 - val_loss: 0.6373 - val_accuracy: 0.7518\n","Epoch 96/100\n","106/106 [==============================] - 4s 36ms/step - loss: 0.6705 - accuracy: 0.7302 - val_loss: 0.6443 - val_accuracy: 0.7494\n","Epoch 97/100\n","106/106 [==============================] - 5s 50ms/step - loss: 0.6863 - accuracy: 0.7278 - val_loss: 0.6304 - val_accuracy: 0.7476\n","Epoch 98/100\n","106/106 [==============================] - 5s 46ms/step - loss: 0.6875 - accuracy: 0.7236 - val_loss: 0.6591 - val_accuracy: 0.7252\n","Epoch 99/100\n","106/106 [==============================] - 5s 48ms/step - loss: 0.6717 - accuracy: 0.7370 - val_loss: 0.6197 - val_accuracy: 0.7642\n","Epoch 100/100\n","106/106 [==============================] - 4s 40ms/step - loss: 0.6834 - accuracy: 0.7364 - val_loss: 0.6519 - val_accuracy: 0.7299\n","56/56 [==============================] - 0s 5ms/step - loss: 0.7298 - accuracy: 0.7049\n"]}],"source":["accuracies = []\n","for i, trim_ratio in enumerate(np.arange(0, 1.1, step=.1)):\n","    if i == 0:\n","        continue\n","    cnn, cnn_training_results, cnn_test_score = cnn_model(trim_ratio=trim_ratio)\n","    accuracies.append(cnn_test_score)\n","    "]},{"cell_type":"markdown","metadata":{"id":"K2WzgzC_GKSD"},"source":["# Deeper exploration and analysis into other architectures"]},{"cell_type":"markdown","metadata":{"id":"o90blBQLGKSE"},"source":["## CRNN"]},{"cell_type":"markdown","metadata":{"id":"6VTgXGrQGKSE"},"source":["#### Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DeatquGHGKSE"},"outputs":[],"source":["def preprocess():\n","    X_test = np.load(\"X_test.npy\")\n","    y_test = np.load(\"y_test.npy\")\n","    person_train_valid = np.load(\"person_train_valid.npy\")\n","    X_train_valid = np.load(\"X_train_valid.npy\")\n","    y_train_valid = np.load(\"y_train_valid.npy\")\n","    person_test = np.load(\"person_test.npy\")\n","\n","    ## Adjusting the labels so that \n","\n","    # Cue onset left - 0\n","    # Cue onset right - 1\n","    # Cue onset foot - 2\n","    # Cue onset tongue - 3\n","\n","    y_train_valid -= 769\n","    y_test -= 769\n","    \n","\n","    # shuffle with 5 fold\n","    indicies_valid = np.random.choice(X_train_valid.shape[0], X_train_valid.shape[0] // 5, replace=False)\n","    indicies_train = np.array(list(set(range(X_train_valid.shape[0])).difference(set(indicies_valid))))\n","\n","    # Creating the training and validation sets using the generated indices\n","    X_train, X_valid = X_train_valid[indicies_train], X_train_valid[indicies_valid] \n","    y_train, y_valid = y_train_valid[indicies_train], y_train_valid[indicies_valid]\n","\n","\n","    # Preprocessing the dataset\n","    x_train,y_train = data_prep(X_train,y_train,2,2,True)\n","    x_valid,y_valid = data_prep(X_valid,y_valid,2,2,True)\n","    X_test_prep,y_test_prep = data_prep(X_test,y_test,2,2,True)\n","\n","\n","\n","\n","\n","    # Converting the labels to categorical variables for multiclass classification\n","    y_train = to_categorical(y_train, 4)\n","    y_valid = to_categorical(y_valid, 4)\n","    y_test = to_categorical(y_test_prep, 4)\n","    # print('Shape of training labels after categorical conversion:',y_train.shape)\n","    # print('Shape of validation labels after categorical conversion:',y_valid.shape)\n","    # print('Shape of test labels after categorical conversion:',y_test.shape)\n","\n","    # Adding width of the segment to be 1\n","    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n","    x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)\n","    x_test = X_test_prep.reshape(X_test_prep.shape[0], X_test_prep.shape[1], X_test_prep.shape[2], 1)\n","    # print('Shape of training set after adding width info:',x_train.shape)\n","    # print('Shape of validation set after adding width info:',x_valid.shape)\n","    # print('Shape of test set after adding width info:',x_test.shape)\n","\n","\n","    # Reshaping the training and validation dataset\n","    x_train = np.swapaxes(x_train, 1,3)\n","    x_train = np.swapaxes(x_train, 1,2)\n","    x_valid = np.swapaxes(x_valid, 1,3)\n","    x_valid = np.swapaxes(x_valid, 1,2)\n","    x_test = np.swapaxes(x_test, 1,3)\n","    x_test = np.swapaxes(x_test, 1,2)\n","    # print('Shape of training set after dimension reshaping:',x_train.shape)\n","    # print('Shape of validation set after dimension reshaping:',x_valid.shape)\n","    # print('Shape of test set after dimension reshaping:',x_test.shape)\n","\n","    return (x_train, y_train, x_valid, y_valid, x_test, y_test)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wlttvDj9GKSF"},"source":["#### Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ryZ6wA8kGKSF"},"outputs":[],"source":["def cnn_lstm_model():    \n","    # Building the CNN model using sequential class\n","    cnn_lstm_model = Sequential()\n","\n","    # Conv. block 1\n","    cnn_lstm_model.add(Conv2D(filters=20, kernel_size=(5,1), padding='same', activation='elu', input_shape=(250,1,22)))\n","    cnn_lstm_model.add(MaxPooling2D(pool_size=(3,1), padding='same')) \n","    cnn_lstm_model.add(BatchNormalization())\n","    cnn_lstm_model.add(Dropout(0.5))\n","\n","    # Conv. block 2\n","    cnn_lstm_model.add(Conv2D(filters=20, kernel_size=(15,1), padding='same', activation='elu'))\n","    cnn_lstm_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n","    cnn_lstm_model.add(BatchNormalization())\n","    cnn_lstm_model.add(Dropout(0.5))\n","\n","    # Conv. block 3\n","    cnn_lstm_model.add(Conv2D(filters=10, kernel_size=(10,1), padding='same', activation='elu'))\n","    cnn_lstm_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n","    cnn_lstm_model.add(BatchNormalization())\n","    cnn_lstm_model.add(Dropout(0.5))\n","\n","    # Add LSTM layers\n","    cnn_lstm_model.add(Permute((2, 3, 1)))\n","    cnn_lstm_model.add(TimeDistributed(Flatten()))\n","\n","    cnn_lstm_model.add(LSTM(250, return_sequences=True))\n","    cnn_lstm_model.add(Dropout(0.5))\n","    cnn_lstm_model.add(LSTM(100, return_sequences=True))\n","    cnn_lstm_model.add(Dropout(0.5))\n","    cnn_lstm_model.add(LSTM(50))\n","\n","    # Output layer with Softmax activation\n","    cnn_lstm_model.add(Flatten()) # Flattens the input\n","    cnn_lstm_model.add(Dense(4, activation='softmax')) # Output FC layer with softmax activation\n","\n","\n","    return cnn_lstm_model"]},{"cell_type":"markdown","metadata":{"id":"PSVrfaYHGKSF"},"source":["#### Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eG-vBkq8GKSG"},"outputs":[],"source":["learning_rate = 1e-3\n","batch_size = 64\n","epochs = 100\n","optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n","noise_dim = 100\n","num_classes = 4"]},{"cell_type":"markdown","metadata":{"id":"iJn0Lf-JGKSG"},"source":["#### Training\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g_1PoFDwGKSH","outputId":"289cd950-2bd4-4eba-827f-633d0778c86c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of X after trimming: (1692, 22, 500)\n","Shape of X after GAN: (7024, 22, 250)\n","Shape of X after trimming: (423, 22, 500)\n","Shape of X after GAN: (1948, 22, 250)\n","Shape of X after trimming: (443, 22, 500)\n","Shape of X after GAN: (2028, 22, 250)\n","Model: \"sequential_91\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_141 (Conv2D)         (None, 250, 1, 10)        1110      \n","                                                                 \n"," max_pooling2d_12 (MaxPoolin  (None, 84, 1, 10)        0         \n"," g2D)                                                            \n","                                                                 \n"," batch_normalization_138 (Ba  (None, 84, 1, 10)        40        \n"," tchNormalization)                                               \n","                                                                 \n"," dropout_143 (Dropout)       (None, 84, 1, 10)         0         \n","                                                                 \n"," conv2d_142 (Conv2D)         (None, 84, 1, 10)         1510      \n","                                                                 \n"," max_pooling2d_13 (MaxPoolin  (None, 28, 1, 10)        0         \n"," g2D)                                                            \n","                                                                 \n"," batch_normalization_139 (Ba  (None, 28, 1, 10)        40        \n"," tchNormalization)                                               \n","                                                                 \n"," dropout_144 (Dropout)       (None, 28, 1, 10)         0         \n","                                                                 \n"," flatten_49 (Flatten)        (None, 280)               0         \n","                                                                 \n"," dense_91 (Dense)            (None, 4)                 1124      \n","                                                                 \n","=================================================================\n","Total params: 3,824\n","Trainable params: 3,784\n","Non-trainable params: 40\n","_________________________________________________________________\n","Epoch 1/100\n","110/110 [==============================] - 1s 9ms/step - loss: 1.5127 - accuracy: 0.3690 - val_loss: 1.2761 - val_accuracy: 0.4168\n","Epoch 2/100\n","110/110 [==============================] - 1s 8ms/step - loss: 1.2278 - accuracy: 0.4465 - val_loss: 1.2433 - val_accuracy: 0.4174\n","Epoch 3/100\n","110/110 [==============================] - 1s 8ms/step - loss: 1.1559 - accuracy: 0.4895 - val_loss: 1.1849 - val_accuracy: 0.4646\n","Epoch 4/100\n","110/110 [==============================] - 1s 8ms/step - loss: 1.1180 - accuracy: 0.5188 - val_loss: 1.1595 - val_accuracy: 0.4754\n","Epoch 5/100\n","110/110 [==============================] - 1s 8ms/step - loss: 1.0755 - accuracy: 0.5429 - val_loss: 1.1491 - val_accuracy: 0.4959\n","Epoch 6/100\n","110/110 [==============================] - 1s 8ms/step - loss: 1.0483 - accuracy: 0.5551 - val_loss: 1.1185 - val_accuracy: 0.5169\n","Epoch 7/100\n","110/110 [==============================] - 1s 8ms/step - loss: 1.0233 - accuracy: 0.5730 - val_loss: 1.0861 - val_accuracy: 0.5390\n","Epoch 8/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.9971 - accuracy: 0.5913 - val_loss: 1.0620 - val_accuracy: 0.5539\n","Epoch 9/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.9978 - accuracy: 0.5903 - val_loss: 1.0666 - val_accuracy: 0.5488\n","Epoch 10/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.9612 - accuracy: 0.5995 - val_loss: 1.0345 - val_accuracy: 0.5637\n","Epoch 11/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.9566 - accuracy: 0.6032 - val_loss: 1.0076 - val_accuracy: 0.5708\n","Epoch 12/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.9439 - accuracy: 0.6180 - val_loss: 1.0211 - val_accuracy: 0.5785\n","Epoch 13/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.9277 - accuracy: 0.6143 - val_loss: 1.0298 - val_accuracy: 0.5652\n","Epoch 14/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.9332 - accuracy: 0.6113 - val_loss: 0.9921 - val_accuracy: 0.5821\n","Epoch 15/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.9154 - accuracy: 0.6254 - val_loss: 0.9972 - val_accuracy: 0.5919\n","Epoch 16/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.9026 - accuracy: 0.6230 - val_loss: 0.9905 - val_accuracy: 0.5719\n","Epoch 17/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8962 - accuracy: 0.6367 - val_loss: 0.9892 - val_accuracy: 0.5965\n","Epoch 18/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8886 - accuracy: 0.6315 - val_loss: 0.9720 - val_accuracy: 0.5862\n","Epoch 19/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8728 - accuracy: 0.6438 - val_loss: 0.9658 - val_accuracy: 0.5893\n","Epoch 20/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8613 - accuracy: 0.6492 - val_loss: 0.9599 - val_accuracy: 0.6088\n","Epoch 21/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.8656 - accuracy: 0.6449 - val_loss: 0.9412 - val_accuracy: 0.6068\n","Epoch 22/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.8643 - accuracy: 0.6513 - val_loss: 0.9372 - val_accuracy: 0.6160\n","Epoch 23/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8742 - accuracy: 0.6415 - val_loss: 0.9602 - val_accuracy: 0.6063\n","Epoch 24/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8530 - accuracy: 0.6503 - val_loss: 0.9440 - val_accuracy: 0.6027\n","Epoch 25/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8469 - accuracy: 0.6610 - val_loss: 0.9281 - val_accuracy: 0.6247\n","Epoch 26/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.8436 - accuracy: 0.6587 - val_loss: 0.9472 - val_accuracy: 0.6057\n","Epoch 27/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8401 - accuracy: 0.6604 - val_loss: 0.9324 - val_accuracy: 0.6227\n","Epoch 28/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.8304 - accuracy: 0.6683 - val_loss: 0.9464 - val_accuracy: 0.5996\n","Epoch 29/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8216 - accuracy: 0.6659 - val_loss: 0.9431 - val_accuracy: 0.6104\n","Epoch 30/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.8287 - accuracy: 0.6663 - val_loss: 0.9229 - val_accuracy: 0.6109\n","Epoch 31/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8254 - accuracy: 0.6679 - val_loss: 0.9409 - val_accuracy: 0.5980\n","Epoch 32/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8365 - accuracy: 0.6563 - val_loss: 0.9271 - val_accuracy: 0.6083\n","Epoch 33/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8291 - accuracy: 0.6569 - val_loss: 0.9477 - val_accuracy: 0.5970\n","Epoch 34/100\n","110/110 [==============================] - 1s 7ms/step - loss: 0.8291 - accuracy: 0.6676 - val_loss: 0.9210 - val_accuracy: 0.6196\n","Epoch 35/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8087 - accuracy: 0.6753 - val_loss: 0.9211 - val_accuracy: 0.6078\n","Epoch 36/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8229 - accuracy: 0.6723 - val_loss: 0.9256 - val_accuracy: 0.6150\n","Epoch 37/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8246 - accuracy: 0.6660 - val_loss: 0.9186 - val_accuracy: 0.6258\n","Epoch 38/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8091 - accuracy: 0.6703 - val_loss: 0.9266 - val_accuracy: 0.6104\n","Epoch 39/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8124 - accuracy: 0.6684 - val_loss: 0.9077 - val_accuracy: 0.6319\n","Epoch 40/100\n","110/110 [==============================] - 1s 7ms/step - loss: 0.8121 - accuracy: 0.6670 - val_loss: 0.9198 - val_accuracy: 0.6093\n","Epoch 41/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7937 - accuracy: 0.6811 - val_loss: 0.9174 - val_accuracy: 0.6170\n","Epoch 42/100\n","110/110 [==============================] - 1s 7ms/step - loss: 0.8050 - accuracy: 0.6808 - val_loss: 0.9179 - val_accuracy: 0.6232\n","Epoch 43/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8091 - accuracy: 0.6730 - val_loss: 0.9171 - val_accuracy: 0.6181\n","Epoch 44/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8108 - accuracy: 0.6711 - val_loss: 0.9068 - val_accuracy: 0.6299\n","Epoch 45/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7960 - accuracy: 0.6791 - val_loss: 0.9140 - val_accuracy: 0.6299\n","Epoch 46/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7811 - accuracy: 0.6838 - val_loss: 0.8992 - val_accuracy: 0.6355\n","Epoch 47/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7869 - accuracy: 0.6849 - val_loss: 0.8942 - val_accuracy: 0.6366\n","Epoch 48/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8019 - accuracy: 0.6787 - val_loss: 0.9155 - val_accuracy: 0.6186\n","Epoch 49/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7807 - accuracy: 0.6854 - val_loss: 0.8964 - val_accuracy: 0.6350\n","Epoch 50/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7875 - accuracy: 0.6802 - val_loss: 0.8900 - val_accuracy: 0.6160\n","Epoch 51/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7945 - accuracy: 0.6805 - val_loss: 0.9009 - val_accuracy: 0.6340\n","Epoch 52/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.7885 - accuracy: 0.6827 - val_loss: 0.9208 - val_accuracy: 0.6289\n","Epoch 53/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.8070 - accuracy: 0.6708 - val_loss: 0.8963 - val_accuracy: 0.6458\n","Epoch 54/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.7836 - accuracy: 0.6872 - val_loss: 0.8903 - val_accuracy: 0.6273\n","Epoch 55/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7748 - accuracy: 0.6894 - val_loss: 0.9143 - val_accuracy: 0.6330\n","Epoch 56/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.7774 - accuracy: 0.6839 - val_loss: 0.8956 - val_accuracy: 0.6242\n","Epoch 57/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7712 - accuracy: 0.6862 - val_loss: 0.8947 - val_accuracy: 0.6227\n","Epoch 58/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7753 - accuracy: 0.6913 - val_loss: 0.8936 - val_accuracy: 0.6283\n","Epoch 59/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7757 - accuracy: 0.6902 - val_loss: 0.9155 - val_accuracy: 0.6237\n","Epoch 60/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7696 - accuracy: 0.6885 - val_loss: 0.8901 - val_accuracy: 0.6371\n","Epoch 61/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7859 - accuracy: 0.6775 - val_loss: 0.9035 - val_accuracy: 0.6324\n","Epoch 62/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7757 - accuracy: 0.6862 - val_loss: 0.8917 - val_accuracy: 0.6294\n","Epoch 63/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7852 - accuracy: 0.6834 - val_loss: 0.8949 - val_accuracy: 0.6381\n","Epoch 64/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.7768 - accuracy: 0.6845 - val_loss: 0.8883 - val_accuracy: 0.6314\n","Epoch 65/100\n","110/110 [==============================] - 1s 10ms/step - loss: 0.7657 - accuracy: 0.6898 - val_loss: 0.8881 - val_accuracy: 0.6324\n","Epoch 66/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.7794 - accuracy: 0.6926 - val_loss: 0.9191 - val_accuracy: 0.6165\n","Epoch 67/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7781 - accuracy: 0.6881 - val_loss: 0.8871 - val_accuracy: 0.6314\n","Epoch 68/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7804 - accuracy: 0.6852 - val_loss: 0.8845 - val_accuracy: 0.6484\n","Epoch 69/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7740 - accuracy: 0.6855 - val_loss: 0.8900 - val_accuracy: 0.6319\n","Epoch 70/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7569 - accuracy: 0.7027 - val_loss: 0.8780 - val_accuracy: 0.6330\n","Epoch 71/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7693 - accuracy: 0.6906 - val_loss: 0.8862 - val_accuracy: 0.6432\n","Epoch 72/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7748 - accuracy: 0.6832 - val_loss: 0.8851 - val_accuracy: 0.6407\n","Epoch 73/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7557 - accuracy: 0.6975 - val_loss: 0.8727 - val_accuracy: 0.6396\n","Epoch 74/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.7746 - accuracy: 0.6844 - val_loss: 0.8803 - val_accuracy: 0.6407\n","Epoch 75/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7630 - accuracy: 0.6945 - val_loss: 0.8865 - val_accuracy: 0.6299\n","Epoch 76/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.7593 - accuracy: 0.6958 - val_loss: 0.8870 - val_accuracy: 0.6340\n","Epoch 77/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7653 - accuracy: 0.6908 - val_loss: 0.8953 - val_accuracy: 0.6314\n","Epoch 78/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7652 - accuracy: 0.6894 - val_loss: 0.8668 - val_accuracy: 0.6422\n","Epoch 79/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7679 - accuracy: 0.6936 - val_loss: 0.9079 - val_accuracy: 0.6140\n","Epoch 80/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7754 - accuracy: 0.6892 - val_loss: 0.8931 - val_accuracy: 0.6299\n","Epoch 81/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7653 - accuracy: 0.6892 - val_loss: 0.8885 - val_accuracy: 0.6360\n","Epoch 82/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.7645 - accuracy: 0.6922 - val_loss: 0.8794 - val_accuracy: 0.6417\n","Epoch 83/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7555 - accuracy: 0.6925 - val_loss: 0.8740 - val_accuracy: 0.6412\n","Epoch 84/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7720 - accuracy: 0.6802 - val_loss: 0.8844 - val_accuracy: 0.6371\n","Epoch 85/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7624 - accuracy: 0.6915 - val_loss: 0.8889 - val_accuracy: 0.6314\n","Epoch 86/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7496 - accuracy: 0.6938 - val_loss: 0.8805 - val_accuracy: 0.6371\n","Epoch 87/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7603 - accuracy: 0.7003 - val_loss: 0.8748 - val_accuracy: 0.6407\n","Epoch 88/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7616 - accuracy: 0.6915 - val_loss: 0.8918 - val_accuracy: 0.6360\n","Epoch 89/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7501 - accuracy: 0.6942 - val_loss: 0.8797 - val_accuracy: 0.6432\n","Epoch 90/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7612 - accuracy: 0.6986 - val_loss: 0.8898 - val_accuracy: 0.6360\n","Epoch 91/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.7559 - accuracy: 0.6959 - val_loss: 0.8676 - val_accuracy: 0.6473\n","Epoch 92/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7419 - accuracy: 0.7024 - val_loss: 0.8902 - val_accuracy: 0.6330\n","Epoch 93/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7544 - accuracy: 0.6952 - val_loss: 0.8896 - val_accuracy: 0.6437\n","Epoch 94/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7669 - accuracy: 0.6889 - val_loss: 0.8808 - val_accuracy: 0.6330\n","Epoch 95/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7652 - accuracy: 0.6856 - val_loss: 0.8801 - val_accuracy: 0.6304\n","Epoch 96/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7558 - accuracy: 0.6921 - val_loss: 0.8875 - val_accuracy: 0.6324\n","Epoch 97/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7482 - accuracy: 0.6982 - val_loss: 0.8821 - val_accuracy: 0.6366\n","Epoch 98/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7610 - accuracy: 0.6996 - val_loss: 0.8726 - val_accuracy: 0.6376\n","Epoch 99/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.7624 - accuracy: 0.6935 - val_loss: 0.8791 - val_accuracy: 0.6350\n","Epoch 100/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7382 - accuracy: 0.7074 - val_loss: 0.8686 - val_accuracy: 0.6381\n"]}],"source":["# Printing the model summary\n","x_train, y_train, x_valid, y_valid, x_test, y_test = preprocess()\n","model = cnn_lstm_model()\n","model.summary()\n","model.compile(loss='categorical_crossentropy',\n","                optimizer=optimizer,\n","                metrics=['accuracy'])\n","\n","model_results = model.fit(x_train,\n","            y_train,\n","            batch_size=batch_size,\n","            epochs=epochs,\n","            validation_data=(x_valid, y_valid), verbose=True)"]},{"cell_type":"markdown","metadata":{"id":"qtkh5-hDGKSI"},"source":["#### Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ue7KdBBkGKSI","outputId":"e330c03f-ca8e-4152-a0ca-21475d47a7da"},"outputs":[{"name":"stdout","output_type":"stream","text":["64/64 [==============================] - 0s 2ms/step - loss: 0.8447 - accuracy: 0.6400\n","Test accuracy of the GAN-CNN model: 0.6400394439697266\n"]}],"source":["model_name = 'CNN-LSTM'\n","model_score = model.evaluate(x_test, y_test, verbose=True)\n","print(f'Test accuracy of the {model_name} model:',model_score[1])"]},{"cell_type":"markdown","metadata":{"id":"T2K4QuXKkrSR"},"source":["## GAN-CNN"]},{"cell_type":"markdown","metadata":{"id":"Li5SMRqLkrSR"},"source":["#### Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6U_RT9TdkrSR"},"outputs":[],"source":["def data_prep_gan(X,y,sub_sample,average,noise, generators, trim_ratio=0.5):\n","    \n","    total_X = None\n","    total_y = None\n","    \n","    # Trimming the data (sample,22,1000) -> (sample,22,500)\n","    X = X[:,:, 0:(int(X.shape[2] * trim_ratio))]\n","    print('Shape of X after trimming:',X.shape)\n","    \n","\n","    \n","    # Maxpooling the data (sample,22,1000) -> (sample,22,500/sub_sample)\n","    X_max = np.max(X.reshape(X.shape[0], X.shape[1], -1, sub_sample), axis=3)\n","    \n","    \n","    total_X = X_max\n","    total_y = y\n","    # print('Shape of X after maxpooling:',total_X.shape)\n","    \n","    # Averaging + noise \n","    X_average = np.mean(X.reshape(X.shape[0], X.shape[1], -1, average), axis=3)\n","    X_average = X_average + np.random.normal(0.0, 0.5, X_average.shape)\n","    \n","    total_X = np.vstack((total_X, X_average))\n","    total_y = np.hstack((total_y, y))\n","    # print('Shape of X after averaging+noise and concatenating:',total_X.shape)\n","    \n","    # Subsampling\n","    \n","    for i in range(sub_sample):\n","        \n","        X_subsample = X[:, :, i::sub_sample] + (np.random.normal(0.0, 0.5, X[:, :,i::sub_sample].shape) if noise else 0.0)\n","        total_X = np.vstack((total_X, X_subsample))\n","        total_y = np.hstack((total_y, y))\n","        \n","    \n","    # GAN\n","    # get generated samples from conditional gan\n","    trimmed_off_data = 1000 - int(X.shape[2] * trim_ratio)\n","    \n","    noise = tf.random.normal([batch_size, noise_dim])\n","\n","    generated_eeg = generators[0](noise, training=False)\n","    generated_samples = generated_eeg.shape[0]\n","    generated_eeg = np.swapaxes(generated_eeg, 2, 3).reshape(-1, 22, 250)\n","    total_X = np.vstack((total_X, generated_eeg))\n","    total_y = np.hstack((total_y, np.full(shape=(generated_samples,), fill_value=0)))\n","\n","    generated_eeg = generators[1](noise, training=False)\n","    generated_samples = generated_eeg.shape[0]\n","    generated_eeg = np.swapaxes(generated_eeg, 2, 3).reshape(-1, 22, 250)\n","    total_X = np.vstack((total_X, generated_eeg))\n","    total_y = np.hstack((total_y, np.full(shape=(generated_samples,), fill_value=1)))\n","\n","    generated_eeg = generators[2](noise, training=False)\n","    generated_samples = generated_eeg.shape[0]\n","    generated_eeg = np.swapaxes(generated_eeg, 2, 3).reshape(-1, 22, 250)\n","    total_X = np.vstack((total_X, generated_eeg))\n","    total_y = np.hstack((total_y, np.full(shape=(generated_samples,), fill_value=2)))\n","\n","    generated_eeg = generators[3](noise, training=False)\n","    generated_samples = generated_eeg.shape[0]\n","    generated_eeg = np.swapaxes(generated_eeg, 2, 3).reshape(-1, 22, 250)\n","    total_X = np.vstack((total_X, generated_eeg))\n","    total_y = np.hstack((total_y, np.full(shape=(generated_samples,), fill_value=3)))\n","\n","\n","    \n","    print('Shape of X after GAN:',total_X.shape)\n","    \n","    return total_X,total_y\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EcLBYTY_krSS"},"outputs":[],"source":["def preprocess():\n","    X_test = np.load(\"X_test.npy\")\n","    y_test = np.load(\"y_test.npy\")\n","    person_train_valid = np.load(\"person_train_valid.npy\")\n","    X_train_valid = np.load(\"X_train_valid.npy\")\n","    y_train_valid = np.load(\"y_train_valid.npy\")\n","    person_test = np.load(\"person_test.npy\")\n","\n","    ## Adjusting the labels so that \n","\n","    # Cue onset left - 0\n","    # Cue onset right - 1\n","    # Cue onset foot - 2\n","    # Cue onset tongue - 3\n","\n","    y_train_valid -= 769\n","    y_test -= 769\n","    \n","\n","    # shuffle with 5 fold\n","    indicies_valid = np.random.choice(X_train_valid.shape[0], X_train_valid.shape[0] // 5, replace=False)\n","    indicies_train = np.array(list(set(range(X_train_valid.shape[0])).difference(set(indicies_valid))))\n","\n","    # Creating the training and validation sets using the generated indices\n","    X_train, X_valid = X_train_valid[indicies_train], X_train_valid[indicies_valid] \n","    y_train, y_valid = y_train_valid[indicies_train], y_train_valid[indicies_valid]\n","\n","\n","    # Preprocessing the dataset\n","    x_train,y_train = data_prep(X_train,y_train,2,2,True)\n","    x_valid,y_valid = data_prep(X_valid,y_valid,2,2,True)\n","    X_test_prep,y_test_prep = data_prep(X_test,y_test,2,2,True)\n","\n","\n","\n","\n","\n","    # Converting the labels to categorical variables for multiclass classification\n","    y_train = to_categorical(y_train, 4)\n","    y_valid = to_categorical(y_valid, 4)\n","    y_test = to_categorical(y_test_prep, 4)\n","\n","\n","    # Adding width of the segment to be 1\n","    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n","    x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)\n","    x_test = X_test_prep.reshape(X_test_prep.shape[0], X_test_prep.shape[1], X_test_prep.shape[2], 1)\n","\n","\n","\n","    # Reshaping the training and validation dataset\n","    x_train = np.swapaxes(x_train, 1,3)\n","    x_train = np.swapaxes(x_train, 1,2)\n","    x_valid = np.swapaxes(x_valid, 1,3)\n","    x_valid = np.swapaxes(x_valid, 1,2)\n","    x_test = np.swapaxes(x_test, 1,3)\n","    x_test = np.swapaxes(x_test, 1,2)\n","\n","\n","    return (x_train, y_train, x_valid, y_valid, x_test, y_test)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7_Fp0qqakrSS"},"outputs":[],"source":["def preprocess_gan():\n","    X_test = np.load(\"X_test.npy\")\n","    y_test = np.load(\"y_test.npy\")\n","    person_train_valid = np.load(\"person_train_valid.npy\")\n","    X_train_valid = np.load(\"X_train_valid.npy\")\n","    y_train_valid = np.load(\"y_train_valid.npy\")\n","    person_test = np.load(\"person_test.npy\")\n","\n","    ## Adjusting the labels so that \n","\n","    # Cue onset left - 0\n","    # Cue onset right - 1\n","    # Cue onset foot - 2\n","    # Cue onset tongue - 3\n","\n","    y_train_valid -= 769\n","    y_test -= 769\n","    \n","\n","    # print(f'X_test Shape for Subject {subject}: {subject_X_test.shape}')\n","    # print(f'y_test Shape for Subject {subject}: {suject_y_test.shape}')\n","    # print(f'X_train_valid Shape for Subject {subject}: {suject_X_train_valid.shape}')\n","    # print(f'y_train_valid Shape for Subject {subject}: {suject_y_train_valid.shape}')\n","\n","    # shuffle with 5 fold\n","    indicies_valid = np.random.choice(X_train_valid.shape[0], X_train_valid.shape[0] // 5, replace=False)\n","    indicies_train = np.array(list(set(range(X_train_valid.shape[0])).difference(set(indicies_valid))))\n","\n","    # Creating the training and validation sets using the generated indices\n","    X_train, X_valid = X_train_valid[indicies_train], X_train_valid[indicies_valid] \n","    y_train, y_valid = y_train_valid[indicies_train], y_train_valid[indicies_valid]\n","\n","\n","    # Preprocessing the dataset\n","    x_train,y_train = data_prep_gan(X_train,y_train,2,2,True, generators)\n","    x_valid,y_valid = data_prep_gan(X_valid,y_valid,2,2,True, generators)\n","    X_test_prep,y_test_prep = data_prep_gan(X_test,y_test,2,2,True, generators)\n","\n","\n","\n","    # print('Shape of training set:',x_train.shape)\n","    # print('Shape of validation set:',x_valid.shape)\n","    # print('Shape of training labels:',y_train.shape)\n","    # print('Shape of validation labels:',y_valid.shape)\n","    # print('Shape of testing set:',X_test_prep.shape)\n","    # print('Shape of testing labels:',y_test_prep.shape)\n","\n","\n","    # Converting the labels to categorical variables for multiclass classification\n","    y_train = to_categorical(y_train, 4)\n","    y_valid = to_categorical(y_valid, 4)\n","    y_test = to_categorical(y_test_prep, 4)\n","    # print('Shape of training labels after categorical conversion:',y_train.shape)\n","    # print('Shape of validation labels after categorical conversion:',y_valid.shape)\n","    # print('Shape of test labels after categorical conversion:',y_test.shape)\n","\n","    # Adding width of the segment to be 1\n","    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n","    x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)\n","    x_test = X_test_prep.reshape(X_test_prep.shape[0], X_test_prep.shape[1], X_test_prep.shape[2], 1)\n","    # print('Shape of training set after adding width info:',x_train.shape)\n","    # print('Shape of validation set after adding width info:',x_valid.shape)\n","    # print('Shape of test set after adding width info:',x_test.shape)\n","\n","\n","    # Reshaping the training and validation dataset\n","    x_train = np.swapaxes(x_train, 1,3)\n","    x_train = np.swapaxes(x_train, 1,2)\n","    x_valid = np.swapaxes(x_valid, 1,3)\n","    x_valid = np.swapaxes(x_valid, 1,2)\n","    x_test = np.swapaxes(x_test, 1,3)\n","    x_test = np.swapaxes(x_test, 1,2)\n","    # print('Shape of training set after dimension reshaping:',x_train.shape)\n","    # print('Shape of validation set after dimension reshaping:',x_valid.shape)\n","    # print('Shape of test set after dimension reshaping:',x_test.shape)\n","\n","    return (x_train, y_train, x_valid, y_valid, x_test, y_test)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EnGeKFxwkrSS"},"source":["#### Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jaepc3yDkrSS"},"outputs":[],"source":["learning_rate = 1e-3\n","batch_size = 64\n","epochs = 100\n","optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n","noise_dim = 100\n","num_classes = 4"]},{"cell_type":"markdown","metadata":{"id":"HA_xFVvwkrSS"},"source":["#### Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vaogYdr5krST"},"outputs":[],"source":["\n","def make_generator_model():\n","    model = Sequential()\n","    model.add(Dense(64 * 125 * 1, use_bias=False, input_shape=(100,)))\n","    model.add(BatchNormalization())\n","    model.add(LeakyReLU())\n","\n","    model.add(Reshape((125, 1, 64)))\n","    assert model.output_shape == (None, 125, 1, 64)  # Note: None is the batch size\n","\n","    model.add(Conv2DTranspose(32, (3, 3), strides=(2, 1), padding='same', use_bias=False))\n","    assert model.output_shape == (None, 250, 1, 32)\n","    model.add(BatchNormalization())\n","    model.add(LeakyReLU())\n","\n","    model.add(Conv2DTranspose(16, (3, 3), strides=(1, 1), padding='same', use_bias=False))\n","    assert model.output_shape == (None, 250, 1, 16)\n","    model.add(BatchNormalization())\n","    model.add(LeakyReLU())\n","\n","    model.add(Conv2DTranspose(1, (3, 3), strides=(1, 1), padding='same', use_bias=False, activation='tanh'))\n","    assert model.output_shape == (None, 250, 1, 1)\n","\n","    model.add(Reshape((250, 1, 1)))\n","    assert model.output_shape == (None, 250, 1, 1)\n","\n","    model.add(Conv2DTranspose(22, (3, 3), strides=(1, 1), padding='same', use_bias=False, activation='tanh'))\n","    assert model.output_shape == (None, 250, 1, 22)\n","\n","    return model\n","\n","\n","def make_discriminator_model():\n","    model = Sequential()\n","    model.add(Conv2D(16, (3, 3), strides=(1, 1), padding='same',\n","                                     input_shape=[250, 1, 22]))\n","    model.add(LeakyReLU())\n","    model.add(Dropout(0.3))\n","\n","    model.add(Conv2D(32, (3, 3), strides=(2, 1), padding='same'))\n","    model.add(LeakyReLU())\n","    model.add(Dropout(0.3))\n","\n","    model.add(Conv2D(64, (3, 3), strides=(2, 1), padding='same'))\n","    model.add(LeakyReLU())\n","    model.add(Dropout(0.3))\n","\n","    model.add(Flatten())\n","    model.add(Dense(1))\n","\n","    return model\n","\n","def discriminator_loss(real_output, fake_output):\n","    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n","    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n","    total_loss = real_loss + fake_loss\n","    return total_loss\n","\n","def generator_loss(fake_output):\n","    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","    return cross_entropy(tf.ones_like(fake_output), fake_output)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"L6pdUteYkrST"},"source":["#### Training\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KPI-qdb1krST"},"outputs":[],"source":["\n","# Define the generator and discriminator models\n","generators = [make_generator_model() for _ in range(4)]\n","discriminators = [make_discriminator_model() for _ in range(4)]\n","\n","# Define the optimizer for the generator and discriminator\n","generator_optimizer = keras.optimizers.Adam(learning_rate)\n","discriminator_optimizer = keras.optimizers.Adam(learning_rate)\n","\n","def train_step(images, class_label):\n","    noise = tf.random.normal([batch_size, noise_dim])\n","\n","    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","        generated_images = generators[class_label](noise, training=True)\n","\n","\n","        real_output = discriminators[class_label](images, training=True)\n","        fake_output = discriminators[class_label](generated_images, training=True)\n","\n","\n","        gen_loss = generator_loss(fake_output)\n","        disc_loss = discriminator_loss(real_output, fake_output)\n","\n","    gradients_of_generator = gen_tape.gradient(gen_loss, generators[class_label].trainable_variables)\n","    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminators[class_label].trainable_variables)\n","\n","    generator_optimizer.apply_gradients(zip(gradients_of_generator, generators[class_label].trainable_variables))\n","    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminators[class_label].trainable_variables))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HwXe6TEakrST"},"outputs":[],"source":["def train_gan(dataset, labels, epochs):\n","    dataset_0 = dataset[np.where(labels[:,0] == 1)[0]]\n","    dataset_1 = dataset[np.where(labels[:,1] == 1)[0]]\n","    dataset_2 = dataset[np.where(labels[:,2] == 1)[0]]\n","    dataset_3 = dataset[np.where(labels[:,3] == 1)[0]]\n","\n","    for epoch in range(epochs):\n","        print(f'Epoch: {epoch}, Label: 0')\n","        train_step(dataset_0,0)\n","        print(f'Epoch: {epoch}, Label: 1')\n","        train_step(dataset_1,1)\n","        print(f'Epoch: {epoch}, Label: 2')\n","        train_step(dataset_2,2)\n","        print(f'Epoch: {epoch}, Label: 3')\n","        train_step(dataset_3,3)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9GQM04jrkrST","outputId":"d4e29bce-cae9-4553-e6c0-9950141f8422"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of X after trimming: (1692, 22, 500)\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n","Cell \u001b[0;32mIn[115], line 1\u001b[0m\n","\u001b[0;32m----> 1\u001b[0m x_train, y_train, x_valid, y_valid, x_test, y_test \u001b[39m=\u001b[39m preprocess()\n","\u001b[1;32m      2\u001b[0m train_gan(x_train, y_train, epochs)\n","\n","Cell \u001b[0;32mIn[104], line 35\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m()\u001b[0m\n","\u001b[1;32m     31\u001b[0m y_train, y_valid \u001b[39m=\u001b[39m y_train_valid[indicies_train], y_train_valid[indicies_valid]\n","\u001b[1;32m     34\u001b[0m \u001b[39m# Preprocessing the dataset\u001b[39;00m\n","\u001b[0;32m---> 35\u001b[0m x_train,y_train \u001b[39m=\u001b[39m data_prep_gan(X_train,y_train,\u001b[39m2\u001b[39;49m,\u001b[39m2\u001b[39;49m,\u001b[39mTrue\u001b[39;49;00m, generators)\n","\u001b[1;32m     36\u001b[0m x_valid,y_valid \u001b[39m=\u001b[39m data_prep_gan(X_valid,y_valid,\u001b[39m2\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39mTrue\u001b[39;00m, generators)\n","\u001b[1;32m     37\u001b[0m X_test_prep,y_test_prep \u001b[39m=\u001b[39m data_prep_gan(X_test,y_test,\u001b[39m2\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39mTrue\u001b[39;00m, generators)\n","\n","Cell \u001b[0;32mIn[114], line 32\u001b[0m, in \u001b[0;36mdata_prep_gan\u001b[0;34m(X, y, sub_sample, average, noise, generators, trim_ratio)\u001b[0m\n","\u001b[1;32m     26\u001b[0m \u001b[39m# print('Shape of X after averaging+noise and concatenating:',total_X.shape)\u001b[39;00m\n","\u001b[1;32m     27\u001b[0m \n","\u001b[1;32m     28\u001b[0m \u001b[39m# Subsampling\u001b[39;00m\n","\u001b[1;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(sub_sample):\n","\u001b[0;32m---> 32\u001b[0m     X_subsample \u001b[39m=\u001b[39m X[:, :, i::sub_sample] \u001b[39m+\u001b[39m (np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mnormal(\u001b[39m0.0\u001b[39;49m, \u001b[39m0.5\u001b[39;49m, X[:, :,i::sub_sample]\u001b[39m.\u001b[39;49mshape) \u001b[39mif\u001b[39;00m noise \u001b[39melse\u001b[39;00m \u001b[39m0.0\u001b[39m)\n","\u001b[1;32m     33\u001b[0m     total_X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack((total_X, X_subsample))\n","\u001b[1;32m     34\u001b[0m     total_y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mhstack((total_y, y))\n","\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["x_train, y_train, x_valid, y_valid, x_test, y_test = preprocess()\n","train_gan(x_train, y_train, epochs)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PjEDUnDVkrST","outputId":"a930c78d-a4c6-4a40-ed98-ba722ef0942f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of X after trimming: (1692, 22, 500)\n","Shape of X after GAN: (7024, 22, 250)\n","Shape of X after trimming: (423, 22, 500)\n","Shape of X after GAN: (1948, 22, 250)\n","Shape of X after trimming: (443, 22, 500)\n","Shape of X after GAN: (2028, 22, 250)\n","Model: \"sequential_91\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_141 (Conv2D)         (None, 250, 1, 10)        1110      \n","                                                                 \n"," max_pooling2d_12 (MaxPoolin  (None, 84, 1, 10)        0         \n"," g2D)                                                            \n","                                                                 \n"," batch_normalization_138 (Ba  (None, 84, 1, 10)        40        \n"," tchNormalization)                                               \n","                                                                 \n"," dropout_143 (Dropout)       (None, 84, 1, 10)         0         \n","                                                                 \n"," conv2d_142 (Conv2D)         (None, 84, 1, 10)         1510      \n","                                                                 \n"," max_pooling2d_13 (MaxPoolin  (None, 28, 1, 10)        0         \n"," g2D)                                                            \n","                                                                 \n"," batch_normalization_139 (Ba  (None, 28, 1, 10)        40        \n"," tchNormalization)                                               \n","                                                                 \n"," dropout_144 (Dropout)       (None, 28, 1, 10)         0         \n","                                                                 \n"," flatten_49 (Flatten)        (None, 280)               0         \n","                                                                 \n"," dense_91 (Dense)            (None, 4)                 1124      \n","                                                                 \n","=================================================================\n","Total params: 3,824\n","Trainable params: 3,784\n","Non-trainable params: 40\n","_________________________________________________________________\n","Epoch 1/100\n","110/110 [==============================] - 1s 9ms/step - loss: 1.5127 - accuracy: 0.3690 - val_loss: 1.2761 - val_accuracy: 0.4168\n","Epoch 2/100\n","110/110 [==============================] - 1s 8ms/step - loss: 1.2278 - accuracy: 0.4465 - val_loss: 1.2433 - val_accuracy: 0.4174\n","Epoch 3/100\n","110/110 [==============================] - 1s 8ms/step - loss: 1.1559 - accuracy: 0.4895 - val_loss: 1.1849 - val_accuracy: 0.4646\n","Epoch 4/100\n","110/110 [==============================] - 1s 8ms/step - loss: 1.1180 - accuracy: 0.5188 - val_loss: 1.1595 - val_accuracy: 0.4754\n","Epoch 5/100\n","110/110 [==============================] - 1s 8ms/step - loss: 1.0755 - accuracy: 0.5429 - val_loss: 1.1491 - val_accuracy: 0.4959\n","Epoch 6/100\n","110/110 [==============================] - 1s 8ms/step - loss: 1.0483 - accuracy: 0.5551 - val_loss: 1.1185 - val_accuracy: 0.5169\n","Epoch 7/100\n","110/110 [==============================] - 1s 8ms/step - loss: 1.0233 - accuracy: 0.5730 - val_loss: 1.0861 - val_accuracy: 0.5390\n","Epoch 8/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.9971 - accuracy: 0.5913 - val_loss: 1.0620 - val_accuracy: 0.5539\n","Epoch 9/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.9978 - accuracy: 0.5903 - val_loss: 1.0666 - val_accuracy: 0.5488\n","Epoch 10/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.9612 - accuracy: 0.5995 - val_loss: 1.0345 - val_accuracy: 0.5637\n","Epoch 11/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.9566 - accuracy: 0.6032 - val_loss: 1.0076 - val_accuracy: 0.5708\n","Epoch 12/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.9439 - accuracy: 0.6180 - val_loss: 1.0211 - val_accuracy: 0.5785\n","Epoch 13/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.9277 - accuracy: 0.6143 - val_loss: 1.0298 - val_accuracy: 0.5652\n","Epoch 14/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.9332 - accuracy: 0.6113 - val_loss: 0.9921 - val_accuracy: 0.5821\n","Epoch 15/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.9154 - accuracy: 0.6254 - val_loss: 0.9972 - val_accuracy: 0.5919\n","Epoch 16/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.9026 - accuracy: 0.6230 - val_loss: 0.9905 - val_accuracy: 0.5719\n","Epoch 17/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8962 - accuracy: 0.6367 - val_loss: 0.9892 - val_accuracy: 0.5965\n","Epoch 18/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8886 - accuracy: 0.6315 - val_loss: 0.9720 - val_accuracy: 0.5862\n","Epoch 19/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8728 - accuracy: 0.6438 - val_loss: 0.9658 - val_accuracy: 0.5893\n","Epoch 20/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8613 - accuracy: 0.6492 - val_loss: 0.9599 - val_accuracy: 0.6088\n","Epoch 21/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.8656 - accuracy: 0.6449 - val_loss: 0.9412 - val_accuracy: 0.6068\n","Epoch 22/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.8643 - accuracy: 0.6513 - val_loss: 0.9372 - val_accuracy: 0.6160\n","Epoch 23/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8742 - accuracy: 0.6415 - val_loss: 0.9602 - val_accuracy: 0.6063\n","Epoch 24/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8530 - accuracy: 0.6503 - val_loss: 0.9440 - val_accuracy: 0.6027\n","Epoch 25/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8469 - accuracy: 0.6610 - val_loss: 0.9281 - val_accuracy: 0.6247\n","Epoch 26/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.8436 - accuracy: 0.6587 - val_loss: 0.9472 - val_accuracy: 0.6057\n","Epoch 27/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8401 - accuracy: 0.6604 - val_loss: 0.9324 - val_accuracy: 0.6227\n","Epoch 28/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.8304 - accuracy: 0.6683 - val_loss: 0.9464 - val_accuracy: 0.5996\n","Epoch 29/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8216 - accuracy: 0.6659 - val_loss: 0.9431 - val_accuracy: 0.6104\n","Epoch 30/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.8287 - accuracy: 0.6663 - val_loss: 0.9229 - val_accuracy: 0.6109\n","Epoch 31/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8254 - accuracy: 0.6679 - val_loss: 0.9409 - val_accuracy: 0.5980\n","Epoch 32/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8365 - accuracy: 0.6563 - val_loss: 0.9271 - val_accuracy: 0.6083\n","Epoch 33/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8291 - accuracy: 0.6569 - val_loss: 0.9477 - val_accuracy: 0.5970\n","Epoch 34/100\n","110/110 [==============================] - 1s 7ms/step - loss: 0.8291 - accuracy: 0.6676 - val_loss: 0.9210 - val_accuracy: 0.6196\n","Epoch 35/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8087 - accuracy: 0.6753 - val_loss: 0.9211 - val_accuracy: 0.6078\n","Epoch 36/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8229 - accuracy: 0.6723 - val_loss: 0.9256 - val_accuracy: 0.6150\n","Epoch 37/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8246 - accuracy: 0.6660 - val_loss: 0.9186 - val_accuracy: 0.6258\n","Epoch 38/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8091 - accuracy: 0.6703 - val_loss: 0.9266 - val_accuracy: 0.6104\n","Epoch 39/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8124 - accuracy: 0.6684 - val_loss: 0.9077 - val_accuracy: 0.6319\n","Epoch 40/100\n","110/110 [==============================] - 1s 7ms/step - loss: 0.8121 - accuracy: 0.6670 - val_loss: 0.9198 - val_accuracy: 0.6093\n","Epoch 41/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7937 - accuracy: 0.6811 - val_loss: 0.9174 - val_accuracy: 0.6170\n","Epoch 42/100\n","110/110 [==============================] - 1s 7ms/step - loss: 0.8050 - accuracy: 0.6808 - val_loss: 0.9179 - val_accuracy: 0.6232\n","Epoch 43/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8091 - accuracy: 0.6730 - val_loss: 0.9171 - val_accuracy: 0.6181\n","Epoch 44/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8108 - accuracy: 0.6711 - val_loss: 0.9068 - val_accuracy: 0.6299\n","Epoch 45/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7960 - accuracy: 0.6791 - val_loss: 0.9140 - val_accuracy: 0.6299\n","Epoch 46/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7811 - accuracy: 0.6838 - val_loss: 0.8992 - val_accuracy: 0.6355\n","Epoch 47/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7869 - accuracy: 0.6849 - val_loss: 0.8942 - val_accuracy: 0.6366\n","Epoch 48/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.8019 - accuracy: 0.6787 - val_loss: 0.9155 - val_accuracy: 0.6186\n","Epoch 49/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7807 - accuracy: 0.6854 - val_loss: 0.8964 - val_accuracy: 0.6350\n","Epoch 50/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7875 - accuracy: 0.6802 - val_loss: 0.8900 - val_accuracy: 0.6160\n","Epoch 51/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7945 - accuracy: 0.6805 - val_loss: 0.9009 - val_accuracy: 0.6340\n","Epoch 52/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.7885 - accuracy: 0.6827 - val_loss: 0.9208 - val_accuracy: 0.6289\n","Epoch 53/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.8070 - accuracy: 0.6708 - val_loss: 0.8963 - val_accuracy: 0.6458\n","Epoch 54/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.7836 - accuracy: 0.6872 - val_loss: 0.8903 - val_accuracy: 0.6273\n","Epoch 55/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7748 - accuracy: 0.6894 - val_loss: 0.9143 - val_accuracy: 0.6330\n","Epoch 56/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.7774 - accuracy: 0.6839 - val_loss: 0.8956 - val_accuracy: 0.6242\n","Epoch 57/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7712 - accuracy: 0.6862 - val_loss: 0.8947 - val_accuracy: 0.6227\n","Epoch 58/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7753 - accuracy: 0.6913 - val_loss: 0.8936 - val_accuracy: 0.6283\n","Epoch 59/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7757 - accuracy: 0.6902 - val_loss: 0.9155 - val_accuracy: 0.6237\n","Epoch 60/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7696 - accuracy: 0.6885 - val_loss: 0.8901 - val_accuracy: 0.6371\n","Epoch 61/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7859 - accuracy: 0.6775 - val_loss: 0.9035 - val_accuracy: 0.6324\n","Epoch 62/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7757 - accuracy: 0.6862 - val_loss: 0.8917 - val_accuracy: 0.6294\n","Epoch 63/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7852 - accuracy: 0.6834 - val_loss: 0.8949 - val_accuracy: 0.6381\n","Epoch 64/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.7768 - accuracy: 0.6845 - val_loss: 0.8883 - val_accuracy: 0.6314\n","Epoch 65/100\n","110/110 [==============================] - 1s 10ms/step - loss: 0.7657 - accuracy: 0.6898 - val_loss: 0.8881 - val_accuracy: 0.6324\n","Epoch 66/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.7794 - accuracy: 0.6926 - val_loss: 0.9191 - val_accuracy: 0.6165\n","Epoch 67/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7781 - accuracy: 0.6881 - val_loss: 0.8871 - val_accuracy: 0.6314\n","Epoch 68/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7804 - accuracy: 0.6852 - val_loss: 0.8845 - val_accuracy: 0.6484\n","Epoch 69/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7740 - accuracy: 0.6855 - val_loss: 0.8900 - val_accuracy: 0.6319\n","Epoch 70/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7569 - accuracy: 0.7027 - val_loss: 0.8780 - val_accuracy: 0.6330\n","Epoch 71/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7693 - accuracy: 0.6906 - val_loss: 0.8862 - val_accuracy: 0.6432\n","Epoch 72/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7748 - accuracy: 0.6832 - val_loss: 0.8851 - val_accuracy: 0.6407\n","Epoch 73/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7557 - accuracy: 0.6975 - val_loss: 0.8727 - val_accuracy: 0.6396\n","Epoch 74/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.7746 - accuracy: 0.6844 - val_loss: 0.8803 - val_accuracy: 0.6407\n","Epoch 75/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7630 - accuracy: 0.6945 - val_loss: 0.8865 - val_accuracy: 0.6299\n","Epoch 76/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.7593 - accuracy: 0.6958 - val_loss: 0.8870 - val_accuracy: 0.6340\n","Epoch 77/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7653 - accuracy: 0.6908 - val_loss: 0.8953 - val_accuracy: 0.6314\n","Epoch 78/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7652 - accuracy: 0.6894 - val_loss: 0.8668 - val_accuracy: 0.6422\n","Epoch 79/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7679 - accuracy: 0.6936 - val_loss: 0.9079 - val_accuracy: 0.6140\n","Epoch 80/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7754 - accuracy: 0.6892 - val_loss: 0.8931 - val_accuracy: 0.6299\n","Epoch 81/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7653 - accuracy: 0.6892 - val_loss: 0.8885 - val_accuracy: 0.6360\n","Epoch 82/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.7645 - accuracy: 0.6922 - val_loss: 0.8794 - val_accuracy: 0.6417\n","Epoch 83/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7555 - accuracy: 0.6925 - val_loss: 0.8740 - val_accuracy: 0.6412\n","Epoch 84/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7720 - accuracy: 0.6802 - val_loss: 0.8844 - val_accuracy: 0.6371\n","Epoch 85/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7624 - accuracy: 0.6915 - val_loss: 0.8889 - val_accuracy: 0.6314\n","Epoch 86/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7496 - accuracy: 0.6938 - val_loss: 0.8805 - val_accuracy: 0.6371\n","Epoch 87/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7603 - accuracy: 0.7003 - val_loss: 0.8748 - val_accuracy: 0.6407\n","Epoch 88/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7616 - accuracy: 0.6915 - val_loss: 0.8918 - val_accuracy: 0.6360\n","Epoch 89/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7501 - accuracy: 0.6942 - val_loss: 0.8797 - val_accuracy: 0.6432\n","Epoch 90/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7612 - accuracy: 0.6986 - val_loss: 0.8898 - val_accuracy: 0.6360\n","Epoch 91/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.7559 - accuracy: 0.6959 - val_loss: 0.8676 - val_accuracy: 0.6473\n","Epoch 92/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7419 - accuracy: 0.7024 - val_loss: 0.8902 - val_accuracy: 0.6330\n","Epoch 93/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7544 - accuracy: 0.6952 - val_loss: 0.8896 - val_accuracy: 0.6437\n","Epoch 94/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7669 - accuracy: 0.6889 - val_loss: 0.8808 - val_accuracy: 0.6330\n","Epoch 95/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7652 - accuracy: 0.6856 - val_loss: 0.8801 - val_accuracy: 0.6304\n","Epoch 96/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7558 - accuracy: 0.6921 - val_loss: 0.8875 - val_accuracy: 0.6324\n","Epoch 97/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7482 - accuracy: 0.6982 - val_loss: 0.8821 - val_accuracy: 0.6366\n","Epoch 98/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7610 - accuracy: 0.6996 - val_loss: 0.8726 - val_accuracy: 0.6376\n","Epoch 99/100\n","110/110 [==============================] - 1s 9ms/step - loss: 0.7624 - accuracy: 0.6935 - val_loss: 0.8791 - val_accuracy: 0.6350\n","Epoch 100/100\n","110/110 [==============================] - 1s 8ms/step - loss: 0.7382 - accuracy: 0.7074 - val_loss: 0.8686 - val_accuracy: 0.6381\n"]}],"source":["# Printing the model summary\n","x_train, y_train, x_valid, y_valid, x_test, y_test = preprocess_gan()\n","model = cnn_model()\n","model.summary()\n","model.compile(loss='categorical_crossentropy',\n","                optimizer=optimizer,\n","                metrics=['accuracy'])\n","\n","model_results = model.fit(x_train,\n","            y_train,\n","            batch_size=batch_size,\n","            epochs=epochs,\n","            validation_data=(x_valid, y_valid), verbose=True)"]},{"cell_type":"markdown","metadata":{"id":"P4mtY_eBkrSU"},"source":["#### Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vWAhzbSRkrSU","outputId":"ce98fd7f-a5c3-40c7-e6b9-a6d27675430a"},"outputs":[{"name":"stdout","output_type":"stream","text":["64/64 [==============================] - 0s 2ms/step - loss: 0.8447 - accuracy: 0.6400\n","Test accuracy of the GAN-CNN model: 0.6400394439697266\n"]}],"source":["model_name = 'GAN-CNN'\n","model_score = model.evaluate(x_test, y_test, verbose=True)\n","print(f'Test accuracy of the {model_name} model:',model_score[1])"]},{"cell_type":"markdown","metadata":{"id":"wws_tK7ZkrSU"},"source":["## Attention & Transformers"]},{"cell_type":"markdown","metadata":{"id":"XzRek6t9krSU"},"source":["#### Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dHXQSXYbkrSU"},"outputs":[],"source":["from tensorflow.python.ops.array_ops import expand_dims_eager_fallback\n","from keras.layers import Lambda\n","import keras.backend as K\n","#!pip install keras-position-wise-feed-forward\n","\n","from keras_position_wise_feed_forward import FeedForward\n","\n","# hyperparameters\n","dim = 250\n","num_heads = 4\n","dim_heads = 22\n","dropout = 0.1\n","mlp_dim = 22\n","transformer_layers = 2\n","\n","class PatchEncoder(keras.layers.Layer):\n","    def __init__(self, num_patches, projection_dim):\n","        super().__init__()\n","        self.num_patches = num_patches\n","        self.projection = keras.layers.Dense(units=projection_dim)\n","        self.position_embedding = keras.layers.Embedding(\n","            input_dim=num_patches, output_dim=projection_dim\n","        )\n","\n","    def call(self, patch):\n","        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n","        encoded = self.projection(patch) + self.position_embedding(positions)\n","        return encoded\n","\n","def create_vit_classifier(batch_size=64):\n","    inputs = keras.Input(shape=(250,1,22))\n","\n","    # preprocessing\n","    x = keras.layers.Permute((1,3,2), input_shape=(250,1,22)) (inputs) # shape = (250, 22, 1)\n","    x = keras.layers.Reshape((250,22), input_shape=(250,22,1)) (x) # shape = (250,22)\n","\n","    # Patches\n","\n","    x = keras.layers.LayerNormalization()(x)\n","    x = keras.layers.Dense(dim)(x)\n","    residual2 = keras.layers.LayerNormalization()(x)\n","\n","    #residual2 = keras.layers.Embedding(dim, dim_heads)(x)\n","\n","    residual2 = PatchEncoder(250, 22) (residual2)\n","\n","    # Create multiple layers of the Transformer block.\n","    for _ in range(transformer_layers):\n","        # Layer normalization 1.\n","        x1 = keras.layers.LayerNormalization(epsilon=1e-6)(residual2)\n","        # Create a multi-head attention layer.\n","        attention_output = keras.layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=dim_heads, dropout=0.1\n","        )(x1, x1)\n","        # Skip connection 1.\n","        x2 = keras.layers.Add()([attention_output, residual2])\n","        # Layer normalization 2.\n","        x3 = keras.layers.LayerNormalization(epsilon=1e-6)(x2)\n","        # MLP.\n","        #x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n","        x3 = FeedForward(mlp_dim)(x3)\n","        # Skip connection 2.\n","        residual2 = keras.layers.Add()([x3, x2])\n","\n","    # Create a [batch_size, projection_dim] tensor.\n","    representation = keras.layers.LayerNormalization(epsilon=1e-6)(residual2)\n","    representation = keras.layers.Flatten()(representation)\n","    representation = keras.layers.Dropout(0.5)(representation)\n","    # Add MLP.\n","    features = FeedForward(mlp_dim)(representation)\n","    # Classify outputs.\n","    logits = keras.layers.Dense(4)(features) # 4 is num_classes\n","    # Create the Keras model.\n","    model = keras.Model(inputs=inputs, outputs=logits)\n","    return model\n","\n","vitmodel = create_vit_classifier()\n","vitmodel.summary()"]},{"cell_type":"markdown","metadata":{"id":"RxrdFeeWkrSU"},"source":["#### Training & Validation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ddev67SSkrSU"},"outputs":[],"source":["learning_rate = 1e-5\n","epochs = 50\n","batch_size = 64\n","\n","vitmodel = create_vit_classifier(batch_size)\n","\n","cnn_optimizer = keras.optimizers.Adam(lr=learning_rate)\n","\n","# Compiling the model\n","vitmodel.compile(loss='categorical_crossentropy',\n","                 optimizer=cnn_optimizer,\n","                 metrics=['accuracy'])\n","\n","# Training and validating the model\n","vitmodel_results = vitmodel.fit(x_train,\n","             y_train,\n","             batch_size=batch_size,\n","             epochs=epochs,\n","             validation_data=(x_valid, y_valid), verbose=True)\n"]},{"cell_type":"code","source":["model_name = 'Transformer'\n","model_score = vitmodel.evaluate(x_test, y_test, verbose=True)\n","print(f'Test accuracy of the {model_name} model:',model_score[1])"],"metadata":{"id":"TdSZWg1Xk0RX"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"ee147","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}