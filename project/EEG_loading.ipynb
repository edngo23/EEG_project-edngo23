{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf                                \n",
    "from tensorflow import keras             \n",
    "import numpy as np                       \n",
    "from sklearn.model_selection import train_test_split   \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten,Dropout\n",
    "from keras.layers import Conv2D,BatchNormalization,MaxPooling2D,Reshape, LSTM\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples:\n",
    "* CNN\n",
    "* CNN-LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = np.load(\"X_test.npy\")\n",
    "y_test = np.load(\"y_test.npy\")\n",
    "person_train_valid = np.load(\"person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"y_train_valid.npy\")\n",
    "person_test = np.load(\"person_test.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Valid data shape: (2115, 22, 1000)\n",
      "Test data shape: (443, 22, 1000)\n",
      "Training/Valid target shape: (2115,)\n",
      "Test target shape: (443,)\n",
      "Person train/valid shape: (2115, 1)\n",
      "Person test shape: (443, 1)\n",
      "testing data: (443, 22, 1000)\n"
     ]
    }
   ],
   "source": [
    "print ('Training/Valid data shape: {}'.format(X_train_valid.shape))\n",
    "print ('Test data shape: {}'.format(X_test.shape))\n",
    "print ('Training/Valid target shape: {}'.format(y_train_valid.shape))\n",
    "print ('Test target shape: {}'.format(y_test.shape))\n",
    "print ('Person train/valid shape: {}'.format(person_train_valid.shape))\n",
    "print ('Person test shape: {}'.format(person_test.shape))\n",
    "print(\"testing data:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X after trimming: (2115, 22, 500)\n",
      "Shape of X after maxpooling: (2115, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (4230, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (8460, 22, 250)\n"
     ]
    }
   ],
   "source": [
    "def data_prep(X,y,sub_sample,average,noise):\n",
    "    \n",
    "    total_X = None\n",
    "    total_y = None\n",
    "    \n",
    "    # Trimming the data (sample,22,1000) -> (sample,22,500)\n",
    "    X = X[:,:,0:500]\n",
    "    print('Shape of X after trimming:',X.shape)\n",
    "    \n",
    "    # Maxpooling the data (sample,22,1000) -> (sample,22,500/sub_sample)\n",
    "    X_max = np.max(X.reshape(X.shape[0], X.shape[1], -1, sub_sample), axis=3)\n",
    "    \n",
    "    \n",
    "    total_X = X_max\n",
    "    total_y = y\n",
    "    print('Shape of X after maxpooling:',total_X.shape)\n",
    "    \n",
    "    # Averaging + noise \n",
    "    X_average = np.mean(X.reshape(X.shape[0], X.shape[1], -1, average),axis=3)\n",
    "    X_average = X_average + np.random.normal(0.0, 0.5, X_average.shape)\n",
    "    \n",
    "    total_X = np.vstack((total_X, X_average))\n",
    "    total_y = np.hstack((total_y, y))\n",
    "    print('Shape of X after averaging+noise and concatenating:',total_X.shape)\n",
    "    \n",
    "    # Subsampling\n",
    "    \n",
    "    for i in range(sub_sample):\n",
    "        \n",
    "        X_subsample = X[:, :, i::sub_sample] + \\\n",
    "                            (np.random.normal(0.0, 0.5, X[:, :,i::sub_sample].shape) if noise else 0.0)\n",
    "            \n",
    "        total_X = np.vstack((total_X, X_subsample))\n",
    "        total_y = np.hstack((total_y, y))\n",
    "        \n",
    "    \n",
    "    print('Shape of X after subsampling and concatenating:',total_X.shape)\n",
    "    return total_X,total_y\n",
    "\n",
    "\n",
    "X_train_valid_prep,y_train_valid_prep = data_prep(X_train_valid,y_train_valid,2,2,True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fda686f3d30>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiIAAAGdCAYAAAAvwBgXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAADsLUlEQVR4nOydd3gUdf7HX7N900OAECQSUJqICnaxg2I57AXP81BPvd+JhRPv7F3BhgVBz3IInhV7BwQBFQsgXXoJNSEJ6cnWmfn9MWVndjchQEIo39fz5Mnu1O/szs6851MlVVVVBAKBQCAQCFoBR2sPQCAQCAQCwYGLECICgUAgEAhaDSFEBAKBQCAQtBpCiAgEAoFAIGg1hBARCAQCgUDQagghIhAIBAKBoNUQQkQgEAgEAkGrIYSIQCAQCASCVsPV2gNoDEVR2Lp1K+np6UiS1NrDEQgEAoFA0ARUVaWmpoaOHTvicDRu89irhcjWrVvJz89v7WEIBAKBQCDYBTZt2kSnTp0aXWavFiLp6emAdiAZGRmtPBqBQNBc1AEd9ddbgdRWHItAIGh+qquryc/PN+/jjbFXCxHDHZORkSGEiECwH+G0vM5ACBGBYH+lKWEVIlhVIBAIBAJBqyGEiEAgEAgEglZDCBGBQCAQCAStxl4dIyIQCAQHGqqqEo1GkWW5tYciEDSK2+3G6XTueMEdIISIQCAQ7CWEw2GKioqor69v7aEIBDtEkiQ6depEWlrabm1HCBGBQCDYC1AUhfXr1+N0OunYsSMej0cUchTstaiqSmlpKZs3b6Zbt267ZRkRQkQgEAj2AsLhMIqikJ+fT0pKSmsPRyDYIe3ataOwsJBIJLJbQkQEqwoEAsFexI7KYQsEewvNZbETZ7xAIBAIBIJWQwgRgUAgEAgErYYQIgKBQCAQ7EFmzpyJJElUVlY2eZ36+nouvfRSMjIydnrdvR0hRAQCgUCw2xQXF3PrrbfStWtXvF4v+fn5DB48mOnTp7f4vmVZ5vnnn6dPnz74fD6ys7M599xzmT17dovv20phYSGSJLFw4cJm3/bEiRP58ccf+fnnnykqKqKioqLF9rWnEUKkBYmWlVH22utEt29v7aEIBAJBi1FYWMjRRx/N999/zzPPPMOSJUuYPHkyZ5xxBsOGDWvRfauqypAhQ3j00Ue5/fbbWb58OTNnziQ/P5/TTz+dzz77rEX3v6dYu3YtvXr14vDDD6dDhw77V2q3uhdTVVWlAmpVVVVrD2WXWD/kKnVZj57qhr/d0NpDEQj2KmpVVUX/q23lsewtBAIBddmyZWogEDCnKYqi1oUirfKnKEqTx37uueeqBx10kFpbm/htVlRUqKqqquvXr1cBdcGCBbZ5gDpjxgxz2pIlS9RzzjlHTU1NVdu3b6/+5S9/UUtLSxvc9/vvv68C6hdffJEw75JLLlFzcnLMcT300EPqkUceqb711ltq586d1YyMDPXKK69Uq6urzXU+/PBD9fDDD1d9Pp/apk0bdcCAAbbjev3119WePXuqXq9X7dGjhzpu3DhzHmD7O+2005KOecaMGSpgfjaqqqo//vijevLJJ6s+n0/t1KmTeuutt5r7Pe200xK229R9tSTJzlmDnbl/izoiLUhgwQIA6n76qZVHIhAI9kUCEZnDHpzSKvte9uggUjw7vkWUl5czefJknnjiCVJTUxPmZ2VlNXmflZWVnHnmmdxwww08//zzBAIB7rrrLq644gq+//77pOu8++67dO/encGDByfMGzFiBJ988gnfffcdF110EaBZFj777DO++uorKioquOKKK3jyySd54oknKCoq4qqrruLpp5/m4osvpqamhh9//BFVVQF45513ePDBBxk7dix9+/ZlwYIF3HjjjaSmpjJ06FDmzJnDcccdx7Rp0+jduzcej6dJx7127VrOOeccHn/8ccaPH09paSm33HILt9xyC2+++SaffPIJd999N0uXLuWTTz7B4/Gwdu3aXdrX3ogQInsCl/iYBQLB/smaNWtQVZWePXvu9raMG/zIkSPNaePHjyc/P59Vq1bRvXv3hHVWrVpFr169km7PmL5q1SpzmqIoTJgwgfT0dACuueYapk+fbgqRaDTKJZdcQufOnQHo06ePue5DDz3E6NGjueSSSwDo0qULy5Yt49VXX2Xo0KG0a9cOgJycHDp06NDk4x41ahRXX301w4cPB6Bbt26MGTOG0047jVdeeYU2bdqQkpKCx+Mxt1tdXb1L+9obEXfIPYAjyVOCQCAQ7Ai/28myRwe12r6bgmEtaA4WLVrEjBkzkvYuWbt2bVIhsrNjKCgoMEUIQF5eHiUlJQAceeSRDBgwgD59+jBo0CDOPvtsLrvsMrKzs6mrq2Pt2rX87W9/48YbbzTXj0ajZGZmNnn/yVi0aBGLFy/mnXfesR2TUfa/IaG1vyCESAth/WE4RLlmgUCwC0iS1CT3SGvSrVs3JElixYoVjS5nVIy1XhsjkYhtmdraWgYPHsxTTz2VsH5eXl7S7Xbv3p3ly5cnnWdMtwoYt9ttW0aSJBRFAcDpdPLdd9/x888/M3XqVF566SXuu+8+fvvtN7Ps/uuvv87xxx9v28budqCtra3l73//O7fddlvCvIMPPni3tr0vILJmWgg1GDRfS8I1IxAI9lPatGnDoEGDGDduHHV1dQnzjXoXhtuiqKjInBefetqvXz/++OMPCgoKOPTQQ21/yeJPAIYMGcLq1av58ssvE+aNHj2anJwczjrrrCYfjyRJ9O/fn0ceeYQFCxbg8Xj49NNPyc3NpWPHjqxbty5hbF26dAEw4zRkWW7y/ozjXrZsWcJ2Dz300AZjP3Z1X3sjQoi0EIrlBxnZtAklHG7F0QgEAkHLMW7cOGRZ5rjjjuPjjz9m9erVLF++nDFjxnDiiScC4Pf7OeGEE3jyySdZvnw5s2bN4v7777dtZ9iwYZSXl3PVVVcxd+5c1q5dy5QpU7juuusavOEOGTKEiy++mKFDh/Lf//6XwsJCFi9ezN///ne++OIL3njjjQZFTDy//fYbI0eOZN68eWzcuJFPPvmE0tJS0zXyyCOPMGrUKMaMGcOqVatYsmQJb775Js899xwA7du3x+/3M3nyZLZt20ZVVVWT9nvXXXfx888/c8stt7Bw4UJWr17N559/zi233NLgOru6r70RIURaCCXuyaDy/fdbaSQCgUDQsnTt2pX58+dzxhlnMGLECA4//HDOOusspk+fziuvvGIuN378eKLRKEcffTTDhw/n8ccft22nY8eOzJ49G1mWOfvss+nTpw/Dhw8nKyurwWaAkiQxadIk7r33Xp5//nl69OjBKaecwoYNG5g5c6aZLdMUMjIy+OGHHzjvvPPo3r07999/P6NHj+bcc88F4IYbbuCNN97gzTffpE+fPpx22mlMmDDBtIi4XC7GjBnDq6++SseOHbnwwgubtN8jjjiCWbNmsWrVKk455RT69u3Lgw8+SMeOHRtcZ1f3tTciqc0ZadTMVFdXk5mZSVVVFRkZGa09nJ0i8McfFF56mfk+47zzOOi50a04IoFg76EOMMIRawERzg3BYJD169fTpUsXfD5faw9HINghjZ2zO3P/blGLiCzLPPDAA3Tp0gW/388hhxzCY4891qxR1nsr8RYRZ9buRVULBAKBQLA/0qJRlE899RSvvPIKEydOpHfv3sybN4/rrruOzMzMpNHB+xPxQkSu3Hf9dwKBQCAQtBQtKkR+/vlnLrzwQs4//3xAy99+7733mDNnTkvudq9Aqau3vZf34UAigUAgEAhaihZ1zZx00klMnz7drGq3aNEifvrpJzPwJ55QKER1dbXtb19Fqa3VXuj55UKICAQCgUCQSItaRO6++26qq6vp2bMnTqcTWZZ54oknuPrqq5MuP2rUKB555JGWHNIew3DNuDt2JLJpE7KeSy8QCAQCgSBGi1pEJk2axDvvvMO7777L/PnzmThxIs8++ywTJ05Muvw999xDVVWV+bdp06aWHF6LYggRV/v22vtAoDWHIxAIBALBXkmLWkT+9a9/cffddzNkyBBAax60YcMGRo0axdChQxOW93q9eL3elhzSHsMUIm2yAVBFQTOBQCAQCBJoUYtIfX19QhEap9Np1vXfn5HrtBgRZ3YbANRQqDWHIxAIBALBXkmLCpHBgwfzxBNP8PXXX1NYWMinn37Kc889x8UXX9ySu90rMCwiTotF5EConyIQCASCXePhhx/mqKOO2ql1Tj/9dIYPH94i49lTtKgQeemll7jsssu4+eab6dWrF3feeSd///vfeeyxx1pyt3sFMddMG3OaGtdpUiAQCPYXiouLufXWW+natSter5f8/HwGDx7M9OnTW3zfsizz/PPP06dPH3w+H9nZ2Zx77rnMnj27xfdtpbCwEEmSEpr5NZU777yzRT4vSZL47LPPmn27zUWLxoikp6fzwgsv8MILL7TkbvZKjDoihmsGdPdMA50UBQKBYF+lsLCQ/v37k5WVxTPPPEOfPn2IRCJMmTKFYcOGsWLFihbbt6qqDBkyhGnTpvHMM88wYMAAqqurGTduHKeffjoffvjhTvWbaQ1UVUWWZdLS0khLS9vxCvsZouldC2HUEXFmZZnTRJyIQCDYH7n55puRJIk5c+Zw6aWX0r17d3r37s0dd9zBr7/+CiS3FlRWViJJEjNnzjSnLV26lHPPPZe0tDRyc3O55pprKCsra3DfkyZN4qOPPuKtt97ihhtuoEuXLhx55JG89tprXHDBBdxwww3U6RZqw/Xxv//9j4KCAjIzMxkyZAg1NTXm9j766CP69OmD3+8nJyeHgQMHmusDvPHGG/Tq1Qufz0fPnj15+eWXzXlG87u+ffsiSRKnn3560jHPnDkTSZL49ttvOfroo/F6vfz0008JrploNMptt91GVlYWOTk53HXXXQwdOjRBWCmKwr///W/atGlDhw4dePjhh815BQUFAFx88cVIkmS+35sQQqQFqJk+ndDKlQA40lKR9EwgkTkjEAh2ClWFcF3r/DUxpq28vJzJkyczbNgwUlMT2xdmWR7GdkRlZSVnnnkmffv2Zd68eWaL+yuuuKLBdd599126d+/O4MGDE+aNGDGC7du3891335nT1q5dy2effcZXX33FV199xaxZs3jyyScBKCoq4qqrruL6669n+fLlzJw5k0suucSM73vnnXd48MEHeeKJJ1i+fDkjR47kgQceMEtSGFXDp02bRlFREZ988kmjx3v33Xfz5JNPsnz5co444oiE+U899RTvvPMOb775JrNnz6a6ujqpi2XixImkpqby22+/8fTTT/Poo4+axzx37lwA3nzzTYqKisz3exMt6po5UNk87BbztTMjA8nrRQ2FUIRFRCAQ7AyRehjZcCv4FuXereDZcV/kNWvWoKoqPXv23O1djh07lr59+zJy5Ehz2vjx48nPz2fVqlV07949YZ1Vq1bRq1evpNszphvVvUGzHkyYMIH09HQArrnmGqZPn84TTzxBUVER0WiUSy65hM6dOwNa2QmDhx56iNGjR3PJJZcAmgVk2bJlvPrqqwwdOpR27doBkJOTQ4cOHXZ4vI8++ihnnXVWg/Nfeukl7rnnHjPBY+zYsXzzzTcJyx1xxBE89NBDAHTr1o2xY8cyffp0zjrrLHNMWVlZTRpTayCESAvjatcOSY8LERYRgUCwv9Gc2YCLFi1ixowZSeMk1q5dm1SI7OwYCgoKTBECkJeXR0lJCQBHHnkkAwYMoE+fPgwaNIizzz6byy67jOzsbOrq6li7di1/+9vfuPHGG831o9EomZm71l39mGOOaXBeVVUV27Zt47jjjjOnOZ1Ojj766IQSGPHWFOsx7QsIIdLCONLTcXg8yIgYEYFAsJO4UzTLRGvtuwl069YNSZJ2GJBq1JSyioZIXCZhbW0tgwcP5qmnnkpYPy8vL+l2u3fvzvLly5POM6ZbBYzb7bYtI0mSeWN3Op189913/Pzzz0ydOpWXXnqJ++67j99++42UFO3zeP311zn++ONt23DqPcV2lmSurF2hsWPaFxAxIi2MJEkiRkQgEOwakqS5R1rjT5KaNMQ2bdowaNAgxo0bZwvqNKjU+2wZLoKioiJzXnyaa79+/fjjjz8oKCjg0EMPtf01dNMeMmQIq1ev5ssvv0yYN3r0aHJychp1f8QjSRL9+/fnkUceYcGCBXg8Hj799FNyc3Pp2LEj69atSxibEaTq0a3fsiw3eX8NkZmZSW5uri2mQ5Zl5s+fv9PbcrvdzTKmlkIIkT2AIUREjIhAINgfGTduHLIsc9xxx/Hxxx+zevVqli9fzpgxYzjxxBMB8Pv9nHDCCWZw5qxZs7j//vtt2xk2bBjl5eVcddVVzJ07l7Vr1zJlyhSuu+66Bm+kQ4YM4eKLL2bo0KH897//pbCwkMWLF/P3v/+dL774gjfeeKPJlofffvuNkSNHMm/ePDZu3Mgnn3xCaWmpGWvyyCOPMGrUKMaMGcOqVatYsmQJb775Js899xwA7du3x+/3m0G2VbvZdf3WW29l1KhRfP7556xcuZLbb7+diooKpCaKRIOCggKmT59OcXExFRUVuzWmlkAIkRYk54a/ASB59RiRkLCICASC/Y+uXbsyf/58zjjjDEaMGMHhhx/OWWedxfTp03nllVfM5caPH080GuXoo49m+PDhPP7447btdOzYkdmzZyPLMmeffTZ9+vRh+PDhZGVlJbQLMZAkiUmTJnHvvffy/PPP06NHD0455RQ2bNjAzJkzd6qGSEZGBj/88APnnXce3bt35/7772f06NGce+65ANxwww288cYbvPnmm/Tp04fTTjuNCRMmmBYRl8vFmDFjePXVV+nYsSMXXnjhTn6Sdu666y6uuuoq/vrXv3LiiSeSlpbGoEGD8Pl8O7Wd0aNH891335Gfn0/fvn13a0wtgaTuxXXHq6uryczMpKqqioyMjNYeTpNZefwJKFVVdP3ma7xdu7LhL9dQP28eB73wPBnnnNPawxMIWp06wAhHrAWax1O+bxMMBlm/fj1dunTZ6RuN4MBAURR69erFFVdcsVdUKG/snN2Z+7cIVm0BjFLukkv7eB26WdAociYQCAQCwY7YsGEDU6dO5bTTTiMUCjF27FjWr1/Pn//859YeWrMiXDMtQbwQydTUoFxV3WpDEggEAsG+hcPhYMKECRx77LH079+fJUuWMG3atAbrpuyrCItIC6BGo9oLXYg403UhUiOEiEAgEAiaRn5+/h5v3NcaCItIM6PKslkaWdJzu526RUSpFkJEIBAIBAIrQog0M6Y1BItrJkO4ZgQCgUAgSIYQIs2MGkkUIqZrRlhEBAKBQCCwIYRIcxONlSw2hYgRrFq9e8VtBAKBQCDY3xBCpJmxumaIS99V6+tbY0gCgUAgEOy1CCHSzJhCxO02y/A69GZJSp0QIgKBQCAQWBFCpJkxhIjhlgGLEAkEWmVMAoFAINg7ePjhh8nNzUWSJD777LPWHs5egRAizUx8VVWwCBHhmhEIBPspxcXF3HrrrXTt2hWv10t+fj6DBw9m+vTprT20Fufhhx/mqKOO2uFyy5cv55FHHuHVV1+lqKjI7GGzu1x77bU71VNnb0MUNGtuklhEJF2IqKEQqiwjOZ2tMjSBQCBoCQoLC+nfvz9ZWVk888wz9OnTh0gkwpQpUxg2bBgrVqxo7SHuFaxduxaACy+8cKc76O7PCItIM9OYawaEe0YgEOx/3HzzzUiSxJw5c7j00kvp3r07vXv35o477uDXX38FNLEiSRILFy4016usrESSJGbOnGlOW7p0Keeeey5paWnk5uZyzTXXUFZW1uj+P/74Y3r37o3X66WgoIDRo0fb5hcUFDBy5Eiuv/560tPTOfjgg3nttdfM+eFwmFtuuYW8vDx8Ph+dO3dm1KhRtnHecMMNtGvXjoyMDM4880wWLVoEwIQJE3jkkUdYtGgRkiQhSRITJkxIGOPDDz/M4MGDAa10uyFEFEXh0UcfpVOnTni9Xo466igmT55sW3fJkiWceeaZ+P1+cnJyuOmmm6jVe5c9/PDDTJw4kc8//9zcv/Xz3BcQQqSZiQWruqgJ1wAgeTygW0FEwKpAIGgqqqpSH6lvlb+mNmYvLy9n8uTJDBs2jNTUxD7KWVlZTT7eyspKzjzzTPr27cu8efOYPHky27Zt44orrmhwnd9//50rrriCIUOGsGTJEh5++GEeeOCBBDEwevRojjnmGBYsWMDNN9/MP/7xD1auXAnAmDFj+OKLL5g0aRIrV67knXfeoaCgwFz38ssvp6SkhG+//Zbff/+dfv36MWDAAMrLy7nyyisZMWIEvXv3pqioiKKiIq688sqEcd555528+eabAOZyAC+++CKjR4/m2WefZfHixQwaNIgLLriA1atXA1BXV8egQYPIzs5m7ty5fPjhh0ybNo1bbrnF3O4VV1zBOeecY273pJNOavJnvjcgXDPNjBEjElJCnPSedjJ8PfgTHCkpKDU1KPV1rTk8gUCwDxGIBjj+3eNbZd+//fk3UtwpO1xuzZo1qKpKz549d3ufY8eOpW/fvowcOdKcNn78ePLz81m1ahXdu3dPWOe5555jwIABPPDAAwB0796dZcuW8cwzz3Dttdeay5133nncfPPNANx11108//zzzJgxgx49erBx40a6devGySefjCRJdO7c2Vzvp59+Ys6cOZSUlOD1egF49tln+eyzz/joo4+46aabSEtLw+Vy0aFDhwaPLS0tzRRl1uWeffZZ7rrrLoYMGQLAU089xYwZM3jhhRcYN24c7777LsFgkLfeessUemPHjmXw4ME89dRT5Obm4vf7CYVCje5/b0ZYRJoZo7JqaaTCnPbPb4aa7hlVuGYEAsF+RFMtJ01h0aJFzJgxg7S0NPPPEDhGfEU8y5cvp3///rZp/fv3Z/Xq1ciybE474ogjzNeSJNGhQwdKSkoALdhz4cKF9OjRg9tuu42pU6faxlRbW0tOTo5tXOvXr29wTE2lurqarVu3Jh3/8uXLzeM78sgjbdam/v37oyiKadHZ1xEWkWbGsIjIjtiPc5VcgyMlHwClTlhEBAJB0/C7/Pz2599abd9NoVu3bkiStMOAVIdDe+61CpdIJGJbpra21nzSjycvL69J42kIt96E1ECSJBRFAaBfv36sX7+eb7/9lmnTpnHFFVcwcOBAPvroI2pra8nLy0sad7EzbidBwwgh0syo4TAAsisWEe1XFJxm4ztR5l0gEDQNSZKa5B5pTdq0acOgQYMYN24ct912W0KcSGVlJVlZWbRr1w7Q4iP69u0LYAtcBU0QfPzxxxQUFOByNe321KtXL2bPnm2bNnv2bLp3745zJzIUMzIyuPLKK7nyyiu57LLLOOeccygvL6dfv34UFxfjcrlscSNWPB6PzfqyM/vs2LEjs2fP5rTTTrON/7jjjjOPb8KECdTV1Zmf7ezZs3E4HPTo0WO39r+3IFwzzYwaDgEgW35DaYqCMzsbgGhFRbLVBAKBYJ9l3LhxyLLMcccdx8cff8zq1atZvnw5Y8aM4cQTTwTA7/dzwgkn8OSTT7J8+XJmzZrF/fffb9vOsGHDKC8v56qrrmLu3LmsXbuWKVOmcN111zV4ox0xYgTTp0/nscceY9WqVUycOJGxY8dy5513Nnn8zz33HO+99x4rVqxg1apVfPjhh3To0IGsrCwGDhzIiSeeyEUXXcTUqVMpLCzk559/5r777mPevHmAlpWzfv16Fi5cSFlZGaFQqMn7/te//sVTTz3FBx98wMqVK7n77rtZuHAht99+OwBXX301Pp+PoUOHsnTpUmbMmMGtt97KNddcQ25urrn/xYsXs3LlSsrKyhIsTXs7Qog0M4ZFRLFYREpdLqQs3SJSUdkawxIIBIIWo2vXrsyfP58zzjiDESNGcPjhh3PWWWcxffp0XnnlFXO58ePHE41GOfrooxk+fDiPP/64bTuGdUCWZc4++2z69OnD8OHDycrKMl078fTr149Jkybx/vvvc/jhh/Pggw/y6KOP2gJVd0R6ejpPP/00xxxzDMceeyyFhYV88803ZprtN998w6mnnsp1111H9+7dGTJkCBs2bDCFwKWXXso555zDGWecQbt27XjvvfeavO/bbruNO+64gxEjRtCnTx8mT57MF198Qbdu3QBISUlhypQplJeXc+yxx3LZZZcxYMAAxo4da27jxhtvpEePHhxzzDG0a9cuwUK0tyOpzRlp1MxUV1eTmZlJVVUVGbprY2+n4sMPKX7gQVYf6uS+y2NiZPTiPuR/vYA2119P7r//1YojFAhanzogTX9dCyQmfR54BINB1q9fT5cuXfD5fK09HIFghzR2zu7M/VtYRJoZwyISdtr13YLQGgBk4ZoRCAQCgcBECJFmRg1pQiQYJ0Tq9AB0ubJyD49IIBAIBIK9FyFEmhkjWDUQF/AddGvCRAmKOiICgUAgEBgIIdLMGK6ZYJwQCekWEjWYGE1d/d131M6a1eJjE+zdbKvbxjvL36EuImrNCASCAwdRR6SZUULJLSIBp6zPD9qmh9atY8uttwHQfc5vZr0RwYHH9VOuZ2PNRlaWr+TR/o+29nAEAoFgj9DiFpEtW7bwl7/8hZycHPx+P3369DFzr/dH1LCWvx2JEyJVDs1SEm8RqZn6nfk6sGBByw5OsFezsWYjALM2C+uYQCA4cGhRIVJRUUH//v1xu918++23LFu2jNGjR5OtF/faH1F1i0jEKdmmh/W6ImrQbhGJlpaar4Mr9o++AYLdQ1VVqkJVbKrZ1NpDEQgEghanRV0zTz31FPn5+WbrY4AuXbq05C5bHSNGJN4iEtbbHChxFffk6urYa1H+XaBz5qQzCSthvrvsOzqk7psdNQUCgaAptKhF5IsvvuCYY47h8ssvp3379vTt25fXX3+9weVDoRDV1dW2v30NI2smEtfiIKwLk3iLiGI5RqVm3zteQfNTEaogrGiCdknZklYejUAgELQsLSpE1q1bxyuvvEK3bt2YMmUK//jHP7jtttuYOHFi0uVHjRpFZmam+Zefn9+Sw2sRFItFpFNYxqloLhlDiMSn78o1NbHXVUKIHKh8s+6bpNMdkkhsEwgE+zctepVTFIV+/foxcuRI+vbty0033cSNN97If/7zn6TL33PPPVRVVZl/mzbtez5yo6BZxAkZCly59lgAQkYHallBjUYBUAIBAvPnm+vK+6AFSNA83PXjXUmny8q+21FTcGBRXFzMrbfeSteuXfF6veTn5zN48GCmT5/e2kNrcR5++GGOOuqoHS537bXXctFFF7X4ePY1WjRGJC8vj8MOO8w2rVevXnz88cdJl/d6vXi93pYcUotjlnh3QaoikZGaab43UIIhnGkutltiZ8DuphEIAGojta09BIFghxQWFtK/f3+ysrJ45pln6NOnD5FIhClTpjBs2DBWrFjR2kMU7MW0qEWkf//+rFxpzwRZtWoVnTt3bsndtipG1kzUBVmqE8XpJUVRbMGrql5LJLJhg23d4LJlVEyatMfGKtj7qQnX7HghgaCVufnmm5EkiTlz5nDppZfSvXt3evfuzR133MGvv/4KaGJFkiQWLlxorldZWYkkScycOdOctnTpUs4991zS0tLIzc3lmmuuoaysrNH9f/zxx/Tu3Ruv10tBQQGjR4+2zS8oKGDkyJFcf/31pKenc/DBB/Paa6+Z88PhMLfccgt5eXn4fD46d+7MqFGjbOO84YYbaNeuHRkZGZx55pksWrQIgAkTJvDII4+waNEiJElCkiQmTJiQMMaHH36YiRMn8vnnn5vLGce9ZMkSzjzzTPx+Pzk5Odx0003U1sYeQgxLyrPPPkteXh45OTkMGzaMSCRiLlNUVMT555+P3++nS5cuvPvuuxQUFPDCCy+0+Oe/u7SoEPnnP//Jr7/+ysiRI1mzZg3vvvsur732GsOGDWvJ3bYqsaZ3kKW4cDocvLe1GKRYOu/28eMBcKSmJaxf/OBDe2aggn0CYRE5sFFVFaW+vlX+mtqYvby8nMmTJzNs2DBSUxP7KGdlZTX5eCsrKznzzDPp27cv8+bNY/LkyWzbto0rrriiwXV+//13rrjiCoYMGcKSJUt4+OGHeeCBBxLEwOjRoznmmGNYsGABN998M//4xz/MB+UxY8bwxRdfMGnSJFauXMk777xDQUGBue7ll19OSUkJ3377Lb///jv9+vVjwIABlJeXc+WVVzJixAh69+5NUVERRUVFXHnllQnjvPPOO7niiis455xzzOVOOukk6urqGDRoENnZ2cydO5cPP/yQadOmccstt9jWnzFjBmvXrmXGjBlMnDiRCRMm2I7xr3/9K1u3bmXmzJl8/PHHvPbaa5SUlDT5s9/Vz785aFHXzLHHHsunn37KPffcw6OPPkqXLl144YUXuPrqq1tyt62KEo5ZRDJUDy6ngxzZ7ucPry8EYg3wcu+5m8CiRVR/8+2eHKpgH2D2ltn8/Yi/43KIIsgHImogwMp+R7fKvnvM/x0pJWWHy61ZswZVVenZs+du73Ps2LH07duXkSNHmtPGjx9Pfn4+q1atonv37gnrPPfccwwYMIAHHngAgO7du7Ns2TKeeeYZrr32WnO58847j5tvvhmAu+66i+eff54ZM2bQo0cPNm7cSLdu3Tj55JORJMlmtf/pp5+YM2cOJSUlZujAs88+y2effcZHH33ETTfdRFpaGi6Xiw4dGk61T0tLw+/3EwqFbMtNnDiRYDDIW2+9ZQq5sWPHMnjwYJ566ilyc3MByM7OZuzYsTidTnr27Mn555/P9OnTufHGG1mxYgXTpk1j7ty5HHPMMQC88cYbdOvWrcU//+agxUPy//SnP7FkyRKCwSDLly/nxhtvbOldtipGZdWwUyJd9aIiofe7Y+IA7eM2UngNIeLMyqLtLbcC4EhLtJIIDlyWlC3hpQUvtfYwBIIGaarlpCksWrSIGTNmkJaWZv4ZAmft2rVJ11m+fDn9+/e3Tevfvz+rV69GtjwEHnHEEeZrSZLo0KGDaTG49tprWbhwIT169OC2225j6tSptjHV1taSk5NjG9f69esbHNPOsHz5co488kibNal///4oimILbejduzdOZ6wuRF5enjn+lStX4nK56Nevnzn/0EMP3eniobvy+TcH4jGrmbHGiPgcKSzcFsCjK5GtbbRlDAFiFSIO/SQ0TKKSZK/MKth/2dGFfPzS8fzz6H/uodEI9iYkv58e839vtX03hW7duiFJ0g4DUh0O/UHMcr5bYxwAamtrTUtAPHl5eU0aT0O43W7be0mSUBQFgH79+rF+/Xq+/fZbpk2bxhVXXMHAgQP56KOPqK2tJS8vzxZHYbAzbqfdpbHxN4XW/vwbQwiRZsYQImEXSA4fCzPOYEPdl0gqVKdo4iJaWQHE0nWdmZmmEEFRUIPBJl8EBPs+sipSdAXJkSSpSe6R1qRNmzYMGjSIcePGcdtttyXEiVRWVpKVlUW7du0ALaiyb9++ALbASdAEwccff0xBQQEuV9NuT7169WL27Nm2abNnz6Z79+42C8KOyMjI4Morr+TKK6/ksssu45xzzqG8vJx+/fpRXFyMy+WyxY1Y8Xg8NutLQyRbrlevXkyYMIG6ujrzs5s9ezYOh4MePXo0aew9evQgGo2yYMECjj5ac+WtWbOGiooKc5mW+vybA1EtqZkxglWjTnA4/Tz755MYEH4WVXVRq2sLuaIS0OqIAEgpKThSYsJDqRNt4A8kokrUfP35RZ+34kgEgl1j3LhxyLLMcccdx8cff8zq1atZvnw5Y8aM4cQTTwTA7/dzwgkn8OSTT7J8+XJmzZrF/fffb9vOsGHDKC8v56qrrmLu3LmsXbuWKVOmcN111zV4ox8xYgTTp0/nscceY9WqVUycOJGxY8dy5513Nnn8zz33HO+99x4rVqxg1apVfPjhh3To0IGsrCwGDhzIiSeeyEUXXcTUqVMpLCzk559/5r777jMbuBYUFLB+/XoWLlxIWVkZobhWHgYFBQUsXryYlStXUlZWRiQS4eqrr8bn8zF06FCWLl3KjBkzuPXWW7nmmmvM+JAd0bNnTwYOHMhNN93EnDlzWLBgATfddBN+v9+0rrfU598cCCHSzCiWOiJOZypH5mdx+dH5oDqp1rWGGgiw/b/jUXUh4vD7kRwOHPqTjxAiBxYRJWYe7ZTWiQsOuaAVRyMQ7Dxdu3Zl/vz5nHHGGYwYMYLDDz+cs846i+nTp/PKK6+Yy40fP55oNMrRRx/N8OHDefzxx23b6dixI7Nnz0aWZc4++2z69OnD8OHDycrKMl0L8fTr149Jkybx/vvvc/jhh/Pggw/y6KOP2gJVd0R6ejpPP/00xxxzDMceeyyFhYV88803OBwOJEnim2++4dRTT+W6666je/fuDBkyhA0bNphC4dJLL+Wcc87hjDPOoF27drz33ntJ93PjjTfSo0cPjjnmGNq1a8fs2bNJSUlhypQplJeXc+yxx3LZZZcxYMAAxo4d2+TxA7z11lvk5uZy6qmncvHFF3PjjTeSnp6Oz+czl2mJz785kNTmjDRqZqqrq8nMzKSqqoqMjIzWHs4OUWWZFb0PB+BvtzsZkTKQS4e+wEOfL+XjshuQnLVMetKiKh0OUBS6/fgDrnbtWH3KqURLS+nyycf44grBCfZfKoIVnPrBqQAs+usiJCT+2P4HV319lbnMwmsW4nQ03cy8t1MHGGHZtUBi0ueBRzAYZP369XTp0sV28xAIdoXNmzeTn5/PtGnTGDBgQIvso7Fzdmfu3yJGpBkx3DKgW0Tc2uXV73Ghqq7EAFQ90MiIB3GkpkJpqbCIHGAYrhmH5DB7y7T1t7UtUx2uJtu3cxHwAoHgwOH777+ntraWPn36UFRUxL///W8KCgo49dRTW3toO0S4ZpoRqxCJOsHl1p75/G4nqA1rPoeem24ErMpCiBxQGK4ZtyMWFZ/mtqdxV4QqEAgEgoaIRCLce++99O7dm4svvph27doxc+bMhGybvRFhEWlGFD1ASZFAdoDLkw5AiseJqiQ/GVSXk08Lv+SSbpfEUniFEDmgMCwi1qJlqe5UerbpyYpyLSWyKlTVKmMTCAT7BoMGDWLQoEGtPYxdQlhEmhGjmFnECUgSHl2I+D1OVDl5Cl69U+ahnx9ic81mIUQOUAyLiFWISJLEe+e/R682vQCoDFa2xtAEAoGgxRFCpBlR9fLuRoM7j1cXIm4nqqzFgQTT7QVojK68hdWFFiFSvwdGK9hbMCwiVtcMaMIkx58DQGWock8PSyAQCPYIQog0I6oldRfA69bER4rFIvLZxVHbOiH93rOheoOwiBygJHPNGKTrVjXRhffAYS9OZBQIbDTXuSqESDNilnd3gktVSUnRbiI+jxMUTZR8clAKhe1j63So1P5vqd0ihMgBSrJgVYMUlyZgA9HAHh2TYM9jBBXW1wuLqGDfIKw/fO9MBdtkiGDVZsRqEfGoKmlpmrBIcTuBmEsmmCRuNRgN4kjVzPBCiBxYJIsRMfC7NAErhMj+j9PpJCsry2xklpKSInpOCfZaFEWhtLSUlJSU3S4HL4RIM6KEYuXd3apKWqqWgpnicaHKsXTMkFsCNJPWJr1cREgOCYvIAcrUQq3Tp1NKfKoQQuTAwmgPb4gRgWBvxuFwcPDBB++2YBZCpBkxglUNi0h6qlHQzEG4/CR82T+juqsJemLrPHexdvOpj9TzfdkvHI4QIgca7698H4A1lWsS5gkhcmAhSRJ5eXm0b98+oTOqQLC34fF4mqX0uxAizYi14Z1HBY9Pu4n4PS5QPUQ3XIvz0DFm8zuAGv31tI3TqC5XOByQKyv37MAFey1CiByYOJ3O3fa7CwT7CiJYtRkxglXDLgm3qoJLq73vdxtWD83sao0RqffGXlekauatcMm2PTBawb6AECICgWB/RwiRZkSxWETcqODUVEaKx3iy0T7uiMUOFXXFfGsV6fq00lKRwicAhBARCAT7P0KINCOqHqwacYFbReuuC3hdsY95ZEkZqjMmMh496VHzdaXeglSKRIV7RgCAT7eqCSEiEAj2V4QQaUaMGJGIE1xqzNJhjSg+rCaT27xl5vvc1FzzddQlUa3HjERLS1t4tIK9AVmRG50vLCICgWB/RwiRZsRa4t1F8nSm6yP/Iq19mPxTt3PIh+PxOr22+UbMiFJb26JjFewdRNVoo/NT3FpBs/qIKHIlEAj2T0TWTDPSkEXEyga1A3WOdNI61kCHNngle4pevWaJR6kRJb0PBMJy2Hydm5KbMD/TkwmIXjMCgWD/RVhEmhElZLWINPzRhiRdbYTr8Dg9tnn1Xk3AyDXCInIgYFRVBXj7vLehbjus/g4UrRJvG38bAOqj9QSjwVYZo0AgELQkQog0I2pYu6mEXeCM+2g9loDVoC5EPpu7BiexXN7b+t5GQNclkZqqFh6tYG8gIuvl3SUXHVI7wBtnwjuXwcJ3AEh3p5ul3yuCFa02ToFAIGgphBBpRmJN76QEi0iWPyY4gpIWCPLZnFV8Or/YnH7BIReYMSKByu0tPFrB3oDZ8M6pnx8Vhdr/ZZ8DWqBzG69mFdlSu2VPD08gEAhaHCFEmhFVL8kcdYILe1XE7JSYCyaAXl+EED+viQmOdE86Ib/29BuqEk+/BwINNrxzxoSrEbB656w799i4BAKBYE8hhEgzospaBoTsAFdcA7MnL+1jvg6omhB52TOGukqFLr7+9EobqN1wUrV0zfqqMgT7P4YQ8TjssUI4YudPt+xuAGwPbhdxIgKBYL9DCJHmJKrVhJAdiRaRvgdn89b1xwFQSawT71nVX7B4wWDmzB1IICyTkqW1462tEN03DwSMGBG30w3WarqOmEVk9GmjyfJmAbCqYtWeHJ5AIBC0OEKINCOqbBEiUmJmdLpPm/ZHMMec1lnaRhuqASirDZGRrfejqSoH4IfNPzBm7vPU/j7PLCEv2H8wY0Qcbghbui5bXDWSJNEupR0AdRHRmVkgEOxfCCHSjKhRi2vG8kRr4HVpVpK1Skdz2uWuH5jv+z9SCFJaGyItu702o04rYDVs+jBqX36dTVdfQ/EDD7TwEQj2NDYhUm0JRlXtFVd9Ti3TKiSH9tjYBAKBYE8ghEhzogsRpQGLiNetfdxfKieyQWlvm5cvlTDktV9xp2sFrJz1sRvOxb9oJvuqz79okWELWg/TNeNww88vxWaE7ZYPo+dMUBYxIgKBYP9CCJFmxHDNRBuwiHic2set4uDP4fts82QchKMKrvRsANyBSML6gv0PM1jV6YF6S8p2yF7QzmgFEIoKi4hAINi/EEKkGTGyZhQHeBzehPmGRQRgC+3s89BuSPVOrQWvJxhFtQYvCvZLbK6Z+vLYjLBdiBiuGZE1IxAI9jeEEGlOLFkz7kZiRAw+l0+KzdOFSDlazQhvUBbxAAcAhqslwSISJ0S8Lq9teYFAINhfEEKkGbEFq7oSLSJ+d0yI9MrL4N+Rm8z3RvO7Gkc6AC4Z6msrW3C0gr0Bo6uu3+WHgNUiEhcjoltEnp33LOur1u+x8QkEAkFLs8eEyJNPPokkSQwfPnxP7XKPY8SIKA5wxTWzA63fzLs3Hs+7NxzPt7efQggPi5UuAAw4VAtSrZfSzeWv/2TIHhi1oDUJRAMA+J1+CMSq6SqhGtaWxqwiRrAqwE3fxQSsQCAQ7OvsESEyd+5cXn31VY444og9sbvWQ7eIRB3gdCZaRABOOqQtJx2qFS17/sojyUrXiptlujURE4h4CesJN9U1orrq/o4pRBxOUJXYjFAdA0bPZHOFZjHxWs6n4rpiBAKBYH+hxYVIbW0tV199Na+//jrZ2dktvbtWxWoRcbsTLSLxXNy3Ewe31z6TdJe2bl3AQUgXIp5oy4xTsPdgCJGUuJYADknFT4hFm7QuzIZrRiAQCPY3WlyIDBs2jPPPP5+BAwe29K5aHVuMSAMWkQR0k3u6bhGpDkaJeCQAPEkyeI19CPYPTIsI2ndOSg6q/jqNIEu3VvH6D+uY+kd5Q5sQCASCfZrEqlvNyPvvv8/8+fOZO3duk5YPhUKEQrFMkerq6pYaWstgChEJt7upQkRbLs2pCZGqQISo2wlE8SbRHKXjxtH+9tubY7SCZqY+Us+naz7ljPwz6JjWcccrYAlWVXUh4s2AaAjCtYz1jGHIzPtRceDOrsfXQV+kqSJXIBAI9gFazCKyadMmbr/9dt555x18vqaZlUeNGkVmZqb5l5+f31LDaxGsvWbc7iaa0nWLSKpDUx2V9RGiHs1M74kk1hEpf+O/zTBSQUvwwvwXeHLOk/zlm780afm6SB2fr/0cAL9eM0b2pFEe0dx6xztWcLJjKQBKqIO5njVwVSAQCPZ1WkyI/P7775SUlNCvXz9cLhcul4tZs2YxZswYXC4XsiwnrHPPPfdQVVVl/m3atKmlhtciGAXNZAd43P6mraRbRFIcmh+mKhBB8WqGKm8UTlqm2Jd3J9YnEewdzNw0E4DSQGmTlp++cbr52q9o33NxwEW9HIsXSUNz3cj1Bea0vNS83RuoQCAQ7EW0mGtmwIABLFmyxDbtuuuuo2fPntx11104nc6EdbxeL17vvmt2VqPWYNUmHoduZvc79BiRQATVo4kNTwRu/8IuRKQkn5tg78CoktpUrP2Itge1GJC11dDFsswrnhc5LzSSZWoBGdU3Up3xOpIRTyIQCAT7AS0mRNLT0zn88MNt01JTU8nJyUmYvr9gDVb1eppqEdHM7D6joFkoiuzRvpZkWTOSq0XDegS7yLiF4ygL7Fy6dX203nydr2oVdStlH0HJnnH1muc5Tg6NIRLWpu+s4BEIBIK9GXFXa0asQsTTVCGix5J41FiQbkSvwPqPb5SExYUQ2ftYWraU/yz6z06vVxOuAcDvSmHz15uoVz3UZvsIxAmRTpImcMJRzZMqSv8LBIL9iT16V5s5c+ae3N0eZ5eCVX1ZADhDVaR5XdSGorj86cC25MsLIbLXUVJfskvrVYe1rLDD0gbQf9qnbKAtnTptY+uJbTmCWBl3Va8xEoo4cCGEiEAg2L8QvWaaE0vTO0dTMxv8epG3QAWZfi02pCLQcDCisIjsfeyqMDAsIqHaWNxPu82VPB24wr6g3i4gFNZ+rmE5vEv7EwgEgr0RIUSaEcM1ozjA6Wqia8YiRLJTNSGyJJja4OKSQ3xlexuGoNhZDItIuNoefBoKuPlL+J7YBF2IqKomQoUQEQgE+xPirtZMqKqKpMQsIs5dsIh0b681vPv0kFPYmppjLuI6vJdtP4K9g4l/TOShnx+iNlK744WTYAS3BqvsP8N2wSrmK91iEzxaICuqJlTro/XCPSMQCPYbhBBpLix1UeRdtIgc1jEDgJDLyz9Pvc1cxN0pVthN0pvkCVqfZ+c9yyerP2HWplkA9GnbB4AUV0qT1t9Uo9XJkavsorVtfSX1+Pi5570ASKqC2ymhqjEXzjFvH8PUwqm7fQwCgUDQ2ggh0kyo8UKkqcGqphCppG9+ljm5xpJ143I4eeJK7SYkh4O7PVZB82IEq3bN7Ao0zXUSlsNsq9MCkpUae82ZG3tncO2JnTnmjEu1CcEqfC4nyPZzasSsEbs7dIFAIGh1hBBpLizN6GQHOJ077r4LgD9L+6/KHNk+9nWokuWrUVWcKdpTthwUQqQlKakv4YLPLmDiHxMbXc5ay6MqrHXIzfFr7rSoGiWqNN6ccGP1RlRUUlwpeIL2InUFHpm/TX6ZDZddhxKRIBok3a0ATsLlJ+3CUQkEAsHeixAizYQaid2YZCc4pCZ+tG4/6G4cV6iKf5/TI2GR7Xjw+7X4ESUkYgNaklcXvcr6qvU8O+9Z2DwP5OTFw4JfxhoPGsGqHVNjje7irSIzNs5gXvE88/2KihUAFGQcii9u2fLx46mdMYNoaRm1xZq1pL1HE6BKqP2uHppAIBDslQgh0kwYAiHqAMUh2cp37xBLnMhp3duZk1868hLWZnbkeuVIJI+WSaOGRcZES1IXrYu9eWMAfPXPpMuFFr9ne5+bksufDvlTbL4cojpczcrylZTWl3LbjNu4bsp13PfTfczaNItV5asA6JzWDV+04e9U8mjumMNztMwaVbbHHongZYFAsK8jhEgzoeouk7CuP5psEQGbEEn1xATMN11O4pYz7mC7PxPFqaf0CovIHkMBWPC/xBmqSlCyp9ye1fksUt2peByaS64+Ws9lX1zGZV9exoxNM8zlvlj7Bbd8fwvbg9sBCAUzSIk2/J1KXu1776u77VTFLkRE9oxAINjXEUKkmVCC2g0housIp2MnmtNZhEiKJ/l60fQsbbuBMIqIE9kjDMzvSFJbRbCSqrjvt0NqB9v/rbVbKaorAuDT1Z8mbCIQ1brqfrN4O5mylv4rORKtG6qegXNwiuYiUmV7Ro61X41AIBDsiwgh0kyoIbtFxCntjBDJ0v4HKkjxJnfphH3p5rajpU1rMy/YeSKWmJBSl4viJJVsN5UuZchBHWzTUt2a5aJTeicA1lfFSrQbwaxWDAGhKh7ya7WsG3eanLCcKmkWkByXdn6pkWz7diJCiAgEgn0bUS+8mVDiXDM7ZxHJ0v4HK0lxJ19PUVxUpEFupSZEPPn5SZcT7B5VIbtoCBgumA0/w4bZIEf4X2Btwnppbq2+S6c0TYg89utjDW4ToDxQDkBuagYH12hpvG0OV9i+4SAiW7aYyylowarZTl2IyKnUrf0nqYc8DwiLiEAg2PcRFpFmQtVjN8Ja8cuds4joT9NEAjgcEh//48SERaKygwp9sWiJsIi0FEa1U4OgQxcib54L3z8Os57Cu3pawnqGRWRg54EJ84xS7la21GpiIyK76FCjxYv4cv0cPOFN23IKWrBqBrHqrUo4l1y/lqGzqHQRETkiglYFAsE+ixAizUSCRWSnhIgegBjWnm6P7twmYZGo7KAiXbspRoqKdn2ggkYpCWhuEkm/sQclCeJu8m4Sb/p+PZbjxI4nUpBRsMP9mOIk7CQjqH3v7pw0HCn2GBBVt4g4g5Wc0q2tOd3r1M6ZR395lH5v9+Om727a4T4FAoFgb0QIkWZC1YNVwy5NLOyUEPEYFpGGzeyRqIMiXZ+ENxTuyhAFVkK1EAnYJgWiAbMmSOeIVpAsKEkJy3mSWB9ufmsZ9326BID2Kclrfdx0xE0cmnWobVpqTQgJwKHizEpPECIKemG8QAVvXX8cXdpq58pBKfbt/Fr0K7KSGGMiEAgEeztCiDQTCcGqOxMjYlhEGhEitQGVomxN5IQLN+zSGAU6kQC81A9ePc1m7VhbqcV++F1+cvSS/QGHA4KVttVdSbwgpTXwzm8bAch3n5p0t2nuNHJTc23TMqq188blk5F8GUg+exl3VdFPqEAFkiSRpgczH5NzdsL2w4qoMSMQCPY9hBBpJoz0XSNGZKfqiDRBiBRXRSnJ0oRIVLhmdo9ty6B2G5SthHotPqM2XMtVX18FQFt/W3xW10yg0rZ6xF5CBABVifWLmTC1LcHiwQnL+F1+2jjsdUAyanW3jF8BTxpSXH0SRdYFbaBC24YezDx5fqLQtZadFwgEgn0FIUSaCTNY1QXOnf1YjWDVcEyI3HSq1kDtxSFH4XU5iEQdprVFEdVVd4/qWFYKlZp1aVXFKnNSmjsNvy5EQpIEU+61rR5fzKzAfyzo9T1+WasJm0jFCTjq+tmW89cUk730E9u01ID2XTp9cix7yoIi6/vShcicQi3bZn5hYi2ZSAPl6AUCgWBvRgiRZkKxuGaknbGGgMUiEotFuOfcnvx27wAuPOog0n1uVNVpFksTZd53k4pYjQ8qNXeK2+G2LWKziKybYZsXtHy/P7Q7m3Pbx4TKVa//qr9yklL1V1tMSMqMJ8mWFfO92+EmPaSJB6dbjRW2s6Aq+r5qihOCZt8+723be2EREQgE+yJCiDQT1hLvOxWoCkmDVSVJIjdDixfwexyguojomxVCZDfR63YAUKVZR2TVHuhpCJGAw2L96HoGACF92q3llWTPf4eL5v+NOd5h9JA22rbhkCTa+GIZUH5VpY0c20/H1INIi2jnjdOrmEIk+89/Npepnv4zgUof1JVA1Sbb9nNT7PEmwiIiEAj2RYQQaSbUeq3OQ9gNzp1peAc7jBHxu52guogKIdI8RCyN7fTPPCjHXB0qKj7dcGG6YTr0gYv/Y5vmVVWIBuhYvZD2UiUj3f+17cbllMjyZpnv/YpKG4tFpK33INJ1d5zDHRMiHR58gLyRI83lNs3K0V5sXci/BsW6MxtF1AxEsKpAINgXEUKkmVCqtdiAsEvCsbMWEbeeshmXJmrgdzsTXDOigNVuYInFIayJkpCl8Zyqqvj1WiH1hhtm0EhI7wBXf8S81HQgZjUxcGK3qrgcEtm+mLslRVXMbByAX1ZKpOnfudOrgC/LnGdN45UD+n7qSrj6+IPN6T6nPdVXuGYEAsG+iCjx3kyoNVoQYcQFLqd7B0vHYQiRcF3S2V7dImK4ZlBViEbBvZP7EWhYLU+6EAjIMRGoopKuaDf/GqcuRDr3B6Ao73C2SZpVIz55Ro3T9S6HI8Ei4iFmEVEiOaSHNwOJMSIOvz2NF4CvR+BVXEA7AKKKXQiFZWEREQgE+x7CItJMKDVaVkPYBR7HzgqRxGBVKydFfmOQNN90zYBwz+wWNiGivbZaRNJcfrL0m3ylwwFXf8ySrbXUh6Nsrt1sLpcfsVsglDhp4nRI9mBV1e6aUSN+OukN71wpsk2ISD57mq+B/9vbzddRReWSbpfEDkVYRAQCwT6IECLNhBrQbmhhF3icnp1b2QhWjQZAURJmDy97iJuck03XDIgU3t3CKvgi9RAJElzxpTnpoexjyNSFSJXDweK6TAaP/YkrX/3VrLwKcMKxt2K1i8jxFhGnxID80znC34GeoTBtZNnmzskr9dM2WI3kVPHnhHdsEYlDllXuO/4+872wiAgEgn0RIUSaCUMY7JIQcVuefqNxVhH9xuVWQXFIKPp9Tw2Lp99dxuoCiwTg9wkEV00G4PzaOroqUswi4nTwY6EmMpdsqaKoWrN8nZh3ItKAB+DgWINCNYlFxP32Zfxv2Rw+3Fps+kE/2lxE/YYbaKdrGk96FIcTu0XEu2MhElEUPE4Ph+Ucpr8X54RAINj3EEJkFwhv3Ehg0SLbNEWvBxF2g3dnhYjLIkTCcZkz+tO7WxckUSNgNSKefneZeNdM1Sazy65PVSESIEv/3De53dS5YsJlsy5E0jx6xoo33ZynqPafUyohKPwx4UfWIxLBUV9Aip6663Ar4PTaBGljFhGXPtaorJ0THod2vv2y9ZdGD1sgEAj2RoQQ2QXWnj2IwiuHEN4Q6/mi6BaKsGsXhIjDERMj8Sm8+nuj42vEqd2EjEqugl3A6poJ10OgQqugCngVFWY9RZYllmP8im/N11VBrWtuukcXIBYhIsdZRA6W7XVFrHiImELEDFS1VGyN7zlD3lHmS7dDF6W6G29p2VIA3l5uL3AmEAgE+wJCiOwGwWXLzNdqWOvWqgkRb0OrNIyngRRe3Y2QohhCRNX3Jywiu0y8a6a+nIBkWEQUUBXaWGN1PFX4DnoXZ+oKqkKaP8Ws4WG1iMT9nNpHNtMQHiKkRi0Wkbiqqg6/PVhV/csX5us2Tu0cMSwiUTUaW06kdQsEgn0MIUR2g7JXXzNfKxHtxhV2Sbh3Nn0XLLVE4lJ4dYtIun5jFNVVmwFbsGodBCqodWg/hVRLSuyt5ZUAuDMX4M5YTMrBE/ih5CMgZhFZXh37rru2j4kSAKds7wdzWehBFL3YnYcoqaZrJrG8uyPOIqLIEuj7bOPQzpH49F2AmkhNwjSBQCDYmxFCZCdRo7Gnz9CKFUQrKvTpuhBxJ/YtaRINpfDqMSN+VUVSJdFvZneJhkC2uLWCVRAop9ypKTxrwbG0JBlMBoYQ+WhZTDj6PPbvPRq2f5clZKPo58Zzl/QkJWq4ZpSEhneS203u/feb75VAELyaFSbDoa0XTTK+ymBlg2NuDVRFoe63OcjV1a09FIFAsJcihMhOEi8A5MpKbXpUezrdpawZsBQ1i4sRCWul4yXAh9u0iChBESOyS9SXx73fDmWrKNcLl1ldMulqw0LEcM1Uk2pOczqd1jAP6uvt3+V2NQNFd9v1WDSKdpFKILlFBKDNX67GkaptXw3Um2ne6ZL23RuuGSvlwfKEaa1J1aefsnHoUDZc89fWHopAINhLEUJkJ1HigkTligpUVTWFSMgdy2LYKUzXTPJgVQCv4iKkP3RXTpq08/sQQEC/Uae0hbaxvi3lDk3hWZvS/eE9pcHNGFkz1apFiKDw47/P4KRDtN4wXuzptHX4QRepbbdM51h5BZA8RsRAStEsZXJdHej7TDctIto59/KAl83l75h5R4Njbg0qP/kUgNDKla08EoFAsLcihMhOkmARqahAqasHVXsUrvPtokXEKGqmW0BMaorNl17VycpOeupmWdnO70MQs4iktIG23QBQgCJXohBRlIa/x3S35pqZr3Qzp7nUCJ2yU8zGdB4pJkT+4XkCANVybmRGNbeOljWTlXQ/Dr8mUAsvvYySX7RzL820iGgWm1M6nUKvNr0AqI3UJtlK6yGyuwQCwY4QQmQnib+wVhaVolRqNzfFoRJ27WKMSGpb7X9daWzatmXw1XDzrU92sriLJkSUoD0QUtBEDIuIPxvS8wD4JjUFVZLwSy7aWdJ2w2rDzQvTPGnUh6OUksXIyFUAOPSCYm1SNbFhWETedV3I7LAmWCRXLKMqGNGWa8wiotTFYlC2/1qp7VvSvvuIxTXz/BnPAxCSQ3tVYTMhRAQCwY4QQmQnibeIhLZvRy7bCkDEC0jSrllEUrVGZtRahMiMJ2yL+BSJsEuvIyKEyK4R0IKL8bfRuukC6/Xmgaend7GVYM8JtrOtKgfyzdfp7nSOfXwaAGvUgwBwqpoA6JyTyut/PYaB3bIAqIs6qQ1pQc4Op6VOv64XGooRAZC3b0+YloYWBCtbsmY6pnbE6/SiqArb6rY1cPB7HiUshIhAIGicFhUio0aN4thjjyU9PZ327dtz0UUXsXIv9RW/OXs9A5+bRVFV8sZzBvExIqHySpTtRQCE9YfdXRIiae21/7WWm0hNkW0RnyyZMSLCItIIjdXSsLpmMjoCmFVVO7gzbYsGFTc1qx5ACbUlUn0EcumF5rx0Tzp1Yc2NE9GLtzssloizDsslVw/7qYo4MTSDwxH7yTkj2vpOt2Km5jaFVMMiYgmslSSJDqmasCqqK0q6XmugiqBqgUCwA1pUiMyaNYthw4bx66+/8t133xGJRDj77LOpq6vb8cp7mEe+XMaakloe+vyPRpdTQ3aLSKSmBrlMEw9Bj3ZD26Vg1bRc7f/Sj+DpQ6BuO1RvtS2SImuuHwA10LhgOmBZ9gU8VQCrpyWfb3PNaDduo5iZP64QXb3iAjmVunV3EtzyZ5vLLcUILiYmRJwlS7V0YB23bvII6/MlCSSLEFEi2n4dbhXcO+4tY5CKHqwalzWT4tLGFJL3npu/cM0IBIId0aJCZPLkyVx77bX07t2bI488kgkTJrBx40Z+//33ltztbjF/Y2Wj8+NdM9HqGuQKzZ0S8Go3ll0qaJYZM/tTXwbP906wiGTJoZhFRFzgkzPpGghWwjuXJp9fr7tmUtqYMSJBSfsZ+F12MVCnuGzvXXIecrAj/dqdSFSO/XQ2KRYXzqqpZgdll6KdKyE0YZrmdSHp+1IVUPVtONwKuJILkeyrr7a9L/sjjY4L1pIRqkOOqyNiCKXW7sIbXLmKjX//O4E//hBdogUCwQ7ZozEiVVXa02KbNm2Szg+FQlRXV9v+9jS1IXugnxIIUDFpEpGSEgCKHnrQNl+urUPRG6HV6xYRv8tenrtJ5B8Px1wfex/fhRdorwRsFhFRznsXMC0isRiRgNHwzhknRKJ2IeJ2uqlffyvu0hs57MEp5vQttGO+cqj2pmQZvHIiPJyJc/lnQMwiku51QUYnAORIrOCI062aab3x5N57D53/95b5vnRJBhkLS7h9wSRbsKoxPmj9Lrybb7mFulk/sOHqvwiLiEAg2CF7TIgoisLw4cPp378/hx9+eNJlRo0aRWZmpvmXn5+fdLnmH1vsgu532zMlyl5+meIHH2KjXpApssHeyEytq0UNaK6mkB5IapjIdwqHA/70PFwwtsFF8pQ6whZji6iuugtYY0R8WYDFNRMnIGtl7Vy47OhO/O9vx+FxOgCJactLEjZ7yIkXay9+eg5KV9jmhVTtS0vzueDcp+DgE1Ei2k9PcipIDhq0iEhOJ97u3ROm9ytdxfyNFbZphkuwMYvIq4te5cLPLmR7IDEItrmIbNoE6AHVlnRogUAgSMYeEyLDhg1j6dKlvP/++w0uc88991BVVWX+bdIvaC1FVX2ETeX1PPJlLC7EFydEamf9AEB4wwY2D/9nwjakujpUvZR3yLkbFhEDvYx3MjoodaZFBEScSALBJljQ9KyZV0p+5Z0V72qTDIuI2/69BXTXzP3n9+KUbu1wO+3dda1kdjy0wXlhdCHidUF2Z7h+Msq54wA9PgTA1XCjxIROvIAiOXhzdqFtmmERiSrRhOUNxi4cy7qqdXyy+pMGl9ldJE9y646w4AkEgmS4drzI7nPLLbfw1Vdf8cMPP9CpU6cGl/N6vXi9u9C5dhc5buQ0QtE4P7vTrs2c2bG0yprJkxM3Ul+HGtKCB8O6htk9IdJw9kSWoiA7JWQHOBUtTqThShcHID88Y38vRyA+XidYRYnTycsbvoINX3HRBS8R/ONlUOrwO+3f21a02i5evdiZy9mIbvcndzcChHQhYqTwAsjphwB6xgw0aBGB5Dd2RbfihKMKHpc2rh1ZREbMHBFbv5Hy9buL5Pcnt9ZFo+DehfgpgUCwX9OiFhFVVbnlllv49NNP+f777+nSpUtL7m6niRchYK/NAOBsk7y+w8Re52jzA3Woeq0EQ4hYMyp2Gm9Gg7N6hcO4JBchkTmTnLXf299bMlhMogFqHTHLxupORxBI11Kn/XJMKDwW+QubVS0I1efWfibxItVGIwLSsIhU1sdiN5RazZ3XJIuIlGiJMVJ4VxRX89mCLdQEI2awarIYkc01m5m6Yar5viqc5LNpJhz+5EJcuBIFAkEyWlSIDBs2jLfffpt3332X9PR0iouLKS4uJrAX30DDsl2cODyJNwgFiSmdjwPAHwqYDeha2iKSoqoUpOSbcSIicyaONnFC9/Nhid2MoyFqLSm0K8pXENS74Potn/1/5fMAGHpiZ1MIeBpxzTTmUluratk51UGrEKkB9EBVaFSIJMMlyRwprWHIa78y/IOF/PODhaZrJqwk3vDjm+G1ZNEzo1FfPCKDRiAQJKNFhcgrr7xCVVUVp59+Onl5eebfBx980JK7bRIN+asjcUJESSKagm4f1XpvGAcqoQqtv0fYpW2z2YWIw42sf1Uu1WGm8AqLiM6c1+HFo6A0rljeqskwe4z5tn7+fKI1YZsQmV8yn5qwJgp8BafCyXdQeNYb5vx7zutlvm7UNRP/vfW/3Xy5WdUsLlYLnFyrnTMOwzXj3DkhIjngc++D1OtF1aYtL4lZROREi0h8D5pt9S0nRBqq+quGd5zNE1i0iC133EFk69YdLisQCPYPWjRGZG8OTou3fBhEojsWIrVuH7LDQdTjxBWWCVdqHXLDThWQml+I3LOJraOOJl/ZgkPFDFhVRNVKjW/ubHhe2SoA6n//nQ1X/wWkXGquqzRnf7v+WwDS3GkcnFkAAx9i5R/FwO8cmZ9lC15uLFg1oTJq93Og07HQ/jBe3OTjXx8uZsxVR5mzlRpDiOi/EWfjP8WMP/2J6q++Mt+rsjYWCQUHKjJOs6JvMotIrd5M0eVwEVWiLSpElNrkjffUyI4tIoVXDgEgsq2EgnfebtZxCQSCvZMDttdMOEl8CJBQm0Gpr09Yps7l4whpHSle7clPqdYusBH9XrJbQsQTZ+IvOAXcfiKS9sTsVDFjRJSgsIgkxWpdWPoR/PAMdT/N1t6rEnWOxNP+2A7Hmjfy4irte83LsAeQNhQjMuqSPomuGU8a9BoMOYdw4VEH8cejgzjn8Dxzdsw107Sg0Y7PPE2PxYs45DstzkPRs2Lf9zzOd55/4SZqBqu+tvg1pm2wV5ati2gxKYdkakGypfWlyEry1NoZG2dw9493Ux9JPPebgpzkNwM7V2U1tGrVLu1bIBDsexywQuTtX+31QFI92pNvWFZslhwlkHhRrXd5SXeEcXq1m4hSpwU5RpsjRsQRlwcz5B1tXA7tpuhQVDNGRPTxaIDc3vb33z9OcPta8211EiHSPkVzn/yxtYqHvtDSufOymiZErjru4MQYj7hU4Ph1Y66ZplkNJUnC4fHg0FN5VVlCVeF4xwq6OorpIW20VfT950x7qrnhmumc0Rmn5ERWZbYHk9cSuW3GbXy97mve/OPNJo3NihqNQsTugnHrmXJyZWWTt6PU1KD80HBNHYFAsP9wwAqRpybbi07lZsZuOlFL5oxal8Qi4vaR6nPj8ulPs3qyhSFEdqnpnZVOWiAsff8CPq0RW0RPLXUqqtmBV1hEknDIAGjXI2Hyim0LzdeVjsQU0hx/DgD/eHu+OS0vM16IJLpmbj79kMQxuHzQtlujw4y5ZnYujVYy09slsKx6hmNhoz2ODCGS6c2kjU9LNS4NlDa4PGhWk53F6i5MPeUU2t52K65crY9StHTntlf63FM7vX9By1AxaRJVX37Z2sMQ7KfskToi+wJ5mT7WlWrm63BUMZ9gk8WI1Ln9pHudpkXEIOoECQcOaTf13VXvw9KPoc9llm1rQsShKhaLiOjAm0DnExOaBQKolZtB7/mywZUoRNr6tZohgUjMXZHpty8XH6y66MGzyfAn+Qldn6TeTBzKTlpEDCRLnR1FkXA6tfVHuD/iDek4c16q2565YsSIpLnTSPekUxoo3aHrZVfOY6OmDkD+a68iSRKhVasBiG7ZCNEwuJILpneWvU0/y/v6bbsp6BthdcVqDs44GO9OBgkfiERLSyl+8CEAMs45B0nUghE0MwekRSRZEG2HjJgp3Zo5k8yvXe/y0t1VErOI6ESd4JSaocRYag4cf5NWhlxH1vugOBXFEiMihEgCHY6EJHVcHJav/OdU7QZ3UNpB5rQcn2YR8bpiP4m8TLt7xRMnRDJT3PYaH8OXwnWToWPfHQ5TNmJEstvBSbfucHkDa3EzI2DVnBeOBYnmpebZ5hkWkTRPmilSasPJg0oNdk2IhMxxGp+Nq71WjyU6+Rl4Y0DS9WRFZsm4UbZp1bITWiDgffqG6VzyxSXcMfMO2/6rw3u+t9W+gGLpli43EIgsEOwOB6QQMVIereRmeDHqXFkzapRIYsqhJKn8vfrFBIuI7GgmIZIEWe9f45KFRSQBw9VyxBA45MyE+AwAyfJVhVRQIhmcfNDJ5jTDIlKnVz+9tF8nTunW1r6NHY0jK1+zyDQB0zVzxStw9uNNWge0WBFJV1XxQqSkaoP5OtObaZtXEdRK22d5s2JCJJJ4U7GK9J05l1VV5b0V77Fs60JtnJay9O5crblguCICxYvN7sRWaiO1DJ0e93sKO5IXpdtN3liipWf/sPkHc9oNU2+g/3v92Vyzudn3t69jrf9iFSUCQXNxQAqRqkCiuDi6c7bpjrFmziSrBpktaxdwp9cuaKJOcDpaxtul6k/5TkWxpO8KIYIcAaOS6DmjtDTYJDE6VovI/32jEKk4kdVbYxONGJG6kPad3nF294SKpsFo7Pv++raT2R0M14wzLXnxr8aQUrR14pNe1m2P9WaylnkvrS9lxqYZAGT7sklzaxk+yVwzITlmAdwZi8gPm39g5G8jue/7f2vrWlxI/r5Hafsr9WgGjmjieVsdSrRGeEMSkZ0ovPbRqo+45ItLKKotanS5ZLEx87bNA2Lp3DsiUlRE+f/eTppVt79hrVek1NS04kgE+ytCiACHtkvl1NpvOdqpZVYYtURUWdb6Y8TRwaldfJK7Zloo7MYUInLMIhIQQsRWOdVwySS5gVotIqctVZGDHfllXazaaI4vh3BUMa1haZ7E73Hhxkrzde+OmQnzdwYzaya94Uq6DSH5tONUFbtQSl8Vc0lZBcXjv8YsLtne7EYtIkaaLyQvLd8Q66vWA+CJGGOMWUT8erdtOeREDjmSCpFkJecdKjz907NNHsMjvzzC6orVnP3x2Xy06iPbPKulpyxQ1uA2nPFZaw2w4a9D2fbEE2x75pkdL7yPYxVbDdWI2S9ZMx3e+zPUFLf2SPZ7hBABplwk4f76dt6V7gNiMSIN9cZo01av6REfrOrQCka1BJJHd80oMiEjayYkhEhMiEixFFrLzeQXn5evUlNwxrkx5EA+SDGR6XP5TLcMQKo38YY0+MiOAJzWvd1uDVlV1ViwalrDpeEbwqHHicS7ZrqGnNRvGgrYLSKLSheZr9v42pCm16qxig6DSSsnma8bap6XDOO890S1G77DF7OISB6PeaVRFSBSD1F77FX99JlJtzt3+S9NatC3qsJed+SRXx4xXxc/+ihrzzobWX+al9Xk9VOg6e6oiN4ZvObbHQcm7+tYhYhccwAJkbcvgZVfw7f/bu2R7DYRJWI+LFipDkZYW1pLaU3rloI4IIWI9YYD4KxYa3mnmk/FDQmR9qdosQPJLCKuFrKIOEwhEj1wLSLl6xP9EYZ7wZ0CxhO8xSJyU14u97RvS70Sd6orKYA9ENLojut1OZKWc791QDdeHHIUL1/dL2HezqAGgyBrx+FI3XkhYmTOhKL27IVbXJ9zvfojAEFZOzfK68JkuHPMZbJ92aTo8UbJLCIvL3rZfB1MYrloCMOS4NF/WpIahgl/glJNIEimEJHgvatgdC/b+ukPjUu63ZSQ2qS4jau+uqrBeRXvvkdk82aqv0nudrFaS6wPEv/7dQPD3p3f6EVarqzcqytINwdKvcU1U3sAumaqG3f17c2ECwtRo1H+OeOfXPDZBXyz7hvb/O+XlzBg9CyGf7CglUaocUAKkQG9cjkyPys2wdKCPZ0A22s1AWI0lVMtJuqgy03XrdpTo9MTF1znbLppd2dx6JU73bJs9ppJllq83/LLOBhzFPz2qn26YRGxBqjqdz2rjAzJiad6pOIklGgqZ+ZdDkBdWLuLpnmTi8k0r4sLjzqI1Abmq4qyw+9Erq01n8wBHCk7X/zOECKfRhLjVK5waBVkDWvGyU99z6otMcFSXeemSItbbdRFATEx0xQMS4IpRCpXQ+GP8OlN2nv9Z6EqkhawGt3B56T/5Fwy1Ed3HIeRrKy9tr/Yb1RyOZOKBuu6RlzMqm01PPDZUr5eXLTDi3TRPffucHz7MjaLSCu4ZqIVFZS+NJbw5lYKJG6ha3pLUz15MmvPOZdNt9/GrM2zAHhn+Tu2ZYJ6uQKfq3WP8YAUIgDt0y31AyxNwtpKVXy9WFPARpOuqKVipdcZWzY+FCEl2HKuGadP8+u7lSgBPRbzgIpgn6Jf7L97wD7duKFZU3b1L6baYtVwJrlPqdFM6lbfRy/v1QBU1Gnfbbpv177DzbfexupTTyO63V6xVK6sJFpeTnDlSlaf1J8t/9TSRiWPBylJldcdIXm1E2Bq5GiuCt/HU5Eh5jyvfqM1YkTqwzKSpF1s7j/+fk57ZhaT9Gr33234ju82fGfbdn56Z/P1zlhEInrAsCFEHHp9E6q2aGOWjLirxHXVJHFYIf0cdymwrnIdby59c5dKzstVsdgTyeNNKq6sx2kIqrOfj2XULNrUeOZO1Wef7fS49iVsMSKt4JopeuABysaN03pFtQa7Wxeqldg+XquMXDd9hjkt/v5kNOK09tRqDfbNT7gZeOTsTpyXV8OLQ44CS8R+W6ooqdEuTIZrJuJwUurXghOlvIYD+JYWSGYH1ObG5dWEiEc9QIWIQZuu9vemRcRSBdWXBdhLuTsiDX1vDuas14JWVxZr58Gh7WPukppp09h4/d+IbCvZ4dBqp09HqamxVaBUZZl1F1/CmjMHsPXOO1HDYQK//w6A5N+1VgAOjyaiXYrML0pvSokFznp0IWKL/3BooiRTr9Ir18fExvxtsUqyAGV1lebrnbGIBHRBaFpE9E7U1JXA1oVIDl2IKInfQ7IAyKD+M3LJcNePd/Hc78/xwvwXmjyeTml6WXmLKFTDoYTsnKgSNccOyeNHakOJQqk1mLGyhDUlzS8EVFUlWlHR4HylLrbP1siaqf/1NwCi21quUWOj7KNCJFmwebzF3rCIeN2te4z75ifcDHT87HJervwHF+ZVwdT7kYEP0tNI8W40q2uqYe0CHna4uPOUYbzR+0+4+9rdMf4cTaw8cOa5BLwS3iRVO5sDj1/LrvAoUQK6Mcd6gdiviVrMGQkxIklcM30uh+7n8nPn2BOUP84i4rBsZ2ulto0VxdpFtmeHDHPe5ltupe7nnyl5+ulGh6hYC9/JsW1HNm0iWlSEGgwSWr3GPgZLZsnOYLhmPLJ2g6xVY8fus7gephRO0ZZ3aAfvdxpWIxc9UwYB9uyat/54i4ASe/rfHkjeiyYZhlXByJqxXe9eO828lldv8qMqMCYrJp7kJILaKkQM5hTNafJ4QnIIpb6edX8abE5T6uoTsnNCcsgmRCJyJKn7pjpoD3CXUhKL5rUkXy7aynVvzuWGiXObfdtFd9/D6hNPotZoDBmHWhlLdw5vWNfs+zeo+X4GRQ88aP8tAZKrFQqA/z4x9nonXTOBP/5g21NPE1iylKovv2q9MgtJhEh8DGMwIiwirUfJCs1PrSrwqxYk91laKo+3bcP8Lt8TCMuoqspX87QCUSHJRUlKGz7udjo+j+WCdNzfef6ykdxy+nAWdNQqWXpayCLiSdGe0v1qhIBHO8GSXcD3S6wVQOMzOazBqgYuD/z5fX7wHW1O8sfFG/os2zGeCrZVaxeMg9ukUPLii2y9667YbivKaQzrk7c1uK/wqj83uI5kySzZGRz6TTBFv/nXEhMiHstN9OWFeuCpLkQUOba/LI/W/yVUuhxeH0Bd+TqemWdPRV1ftYH/zGpaF1xDiPiM9F2XXbAbrpryFWmULUvj7ayY2Ev2lG24ZpyWzexMOnFQDlL56ae2aUp9HeVB+/cYjAZtrpln5j3DL1t+T9hefMBqJGXPljkfP1vLeCjc3rx1S6Lbt1P1+ecA1P8+L+kyVtdMzXffU7+gZQIbN998M5UffkjF2/Y4Btx7WIiEauDL22Lvm2gRWVm+kt+KfqPw0ssof/NNCi+/nK3/+hfbRo7a8cotQRK3b4JFRK+NZK0o3RocoELkD8trrfndo21j5dQDEZlvlxbz5kztIhxxxn4ILiyPaN3OQsrrztqsTqYfvqViRHwpmkXEp0YImq6Z/b+YEgBhi+CKL0uezCKiU1wTMzfHW0R8FiuLYQEz0rozvE62v/Ifqj7/IrbCDp6KrHEhkW1a3QE1GkVuxOTt8O2aa8aZqVkT0sLasYfU2E3RahE5LEfrQuxwa1aAUCR2DIqsrRPc9Bu/bl/CqMk32fahKi5kNcJT036lsGzHgtdw4/jCevquy25VkCwV5So32o87WQBkMotIPGo0Svnb7xBal5iWGIwGE34fpS+8yHO/2sVWUA7aLCIAry9+PWF7lfV2i4i8h58gHRYRVh+OIisqN741j+emrmxwnVBU5td121EUlaisJC3kWFMWOz8byhKML9q2/bXEz6c5iRTZs1T2eG+bYFxxvSamdF/25WXcMPWGxM2tXJFk6T2A5Zy5fop+f0qwiOjBqsIi0gocfimc86T2ess85nu9KNYfeiTK8qJqPIpm+o44XLTTg1t92H+sHbP0i6oejNdSWTPZmVkA+ONiRPb31EHALkQClXb3TDKLiE5ZUDMpu6Iq7rgbmtcSoGyYJ6uD2vedlSyjI0k6r5VoWSwDxWh3H39BjWdXXTNO3a2Rrh/7KrUTskcTqg7gtnJt/4oCTn+h6ZoJBGMXoUBIO56Qw8GNebl8HrH735WwVivF6dvcpBgJ42bu1T9W2R0vRCzbjs+kTiJEjtRdRlYhEl/ptezVV9n2+ONsvPbahPUjSgQlmnjjrV1rv3HXhmsThMjGmk3EU1lv/91L8o5rmzQnirUgW02YYe/M57tl2xjz/RpGfrM86TrPTV3FkNd+5YXpq7n81V848pGpfPz7ZuYWalah8OYtbP2/v5vLb9iwjc0V2jm1PbCdqYVTiZSuRFk2xbZdZ0Y6qixTv2AB6y65pEWLukVLS4lu3cPps6E4C10TrulGsHYynLuQot8cWC2I58xX8YfUhoNVRdZMK2FpFb8xzvQXiASQFRW3LkTCDhe3nHEoz1zam0xJv/Gl50GXUy1raV9oS9UR8aZqN58UImaMCNFog08x+xU2K4gKdfpNX1Xhq39qr3WLyB9lf7CwZCEAAbR0P6s1JJCiXdD/b8lnPPPjOA6qKUmwiKTHX4gAyZn4vZb/722KH30MNRpFqY49Ram6ayZcuCFhHds2LQ3sdgbDIpIe1s7FKtLYflOsaFmKXgCsLhLA4Y+Nwa3GrH5z1mvHuLmBC5BcrwUFO1PXU53kSTqeYJlmPfTpn/UbORk2yW61iMhxu7S6sgy8Hbpr+7e6ZuK6/VS+/wEA0ZIS8wL76QWaO2bAAoWKNyckbNepCxtD1Fz25WUUVhfalgmEE8V9vEXEEbEr25L6Er5Y+0WjmT1VoaoE15AVubaO7ePfJLJlS8I8a+2j+RsrmPxHrNrnaz+sQ1VVIpZgzmBE5tUftHiOMdNXs0CvCjziw0Vc/p9fCIRlSp5+GkdRrFP1gmWbOPkpLcNi6OShjJg1gre+uQklrjeXIy2d7a+9xoar/kxo2XLK/zu+wWPaXUqebXpl3WYjvt1AE1wzyVoUGCTLCtsjxLlmXHKixT5mERGumdYhI9Z51RlX2CqU/iWKCm49GDDidNH/0Bwu752BZCx7+2Jwebmknxad79WfAFvKNYNHT99VVdM1AwdIyeV4d0y1fqEutwTOqSpRJcqQr4dwzbfXUBOuIerSLrIpeghA0A1qJ+32eNy2FRy+fT0nFP9BOKqgKKp5w02tT/xMpSQWkW1PPEHFu+9S+emn9hRHvZZIxLjINxBsF96ya3URHHFC5PW/HkP7trGiZSmKdi4GogEkp3bwueoAHvrcYg1QtTGtTSKGwttPRQm11/blqkpq0gd448d1XP3Gryxb9xNTqzTzsxEjUuuVWO6NbdsmRBwSF/xi7XCdGMznyO8D2C0iG2s22uI5oqWxIMqo/tCQ48/BG1b5+2QlaeyJL6I1/hvUeZA5bfYWe5BmsrTlivr42CT7zWXAhwO476f7GPnbyIR1ARRV4eT3T+a0D05rMC267KUxlDz9NOsvvyJhntEDCeCZKYnumO1vvMGa005nuy6+Xp6xJmEZKyuKq5HL7aIoLaKNS1ZUNlRrAnaqXIkStQtAh9dF6YtjbNNqaxt2QRoUVwVZXqTdsIPRIPf+eC/Pzn0WOT4A3UJ87ZCdvakrtdWweV5ikLuVYBUseh+C1byy8BWu/O0h6qzxSE0QIslaFJhjaK0K2HFCxBNNkr6rW4NFjEgrEZVT2PJLFnXFHkqc9kc0p28FiqraXDOdslOgXv/hetK1gEi0VM8/HhnEg4M1C0uLCRGXDxkHXlVFlSQiepn3A6ID7w9xT0WGEAlWxqYFq8ybEUBlsBJVt14VlOi1NTIUeubb4x0MF011MGKaKf2BJE83SSwiBsUPPEjdnFhGh1JfT7SsjOIHHwLAk5+fdL1o8a6lI5oxInp8zFmHaYGnXPQfAAKKJlrrIwEkvS/SxjKV7XWxm6mqJLfGOPAQKj0bVdFThNOXs6D0t6TLPv71cmav2c7Q6bHaLoZFJOix1621utk3e5xcNjsmRFYULQFgTneJSLqMr8chSH7tGKxCJBANcPP0mwFsReGwxBCkuFNoG2g4psAfUjkk6xBu7XerOW1x2WLbMlESBYz1swNw1sd+dwpo1jng87WfJ92vVXwU1yXvXVI/VwsWjRcIADWWrJ0EUQSUjn4OgJKnnmL7f//LUQ/8nRO3Lk26H4ClW6txF3S2TUvVz6eN5do5c3CJSkpQQonabxNK4VxTDBsMHn8KJfWNp7ifMGo65774I5vK6/lh8w98ue5LJi6byJKyJbbCc0V1mismsnUrgd/t6eXBFQ3HxJj8Mg5eOpqaLyex8rgTqLhrMOqMWMCoqqqU1ARRVZWq+gjBSTfAp3+HL27h5UUvs6xuMx+lW9wpTXB/N2oRCbZS+fS42G53NLGFgYgRaWVKxr5K9YYUNs5sywttsu0zXbX8Ufs1XkW7aalen/ZFBfQLRIp9+VSvC4WWDVZFkgg6UsjQf7BRo9/MgeCaKfzR/n75V/DyiTDzydi0QIVNiEQVGSTt4t1js3YhcbcL4+/gxpkd+/4MIVKiZ0VIErhrEi8qOyo8Zu05ogQClP/vbfO9q3178/VBY16MraTsWpyBU48X6uKJ8t0/Le7BjkcBkK5qx7S0fD6ebE1E+EIufHp/l34HZ9H/kA4J2z0291hq1t8AqssUIgDvbXogYVmDm7zvEvTE4mOMYNWgG2oGPcFKj5u/dWhPiSf2+alxT9i/bNCqPgY8UHxpNQVvv2kGKLpk+01gbrGWvhpaFcvmkX3asm6HG4/DQ7tgw0LEF4GumV3JT8+nc4Z2I64K2Z9mVUcIyWkXrF8vLjLjsao+/xxJiY3Lgb27czKsadKKqsC0R+A/J2tP4zrOnJxkq6Kqqi1Opz7cyNM9UPLMs+RVbeOvy7+ld8eMZFmcjPpmObjtYtQQti/Of44uRSrP/lfm1ldD1OhBPd5M7bxSqsrwHHywbd3smoY7F5e/9T/WDBpEe/1BbukWu4uqpKaIjX8dar5fXq7FvGyfMCFhW7WzZjZy5DpT7oXta9j874dBUSn+PYvNI/9rzn7ky2Uc98R0pvxRzFnPz8K3bqo2Y1lMRH5h7Yq9gyrAANXhxoRI6zwsqhG7JdOdxCJiZM0IIdJKhFavTpjWXf/BKc4wiwMTSU3V1LiZZlmnm4JTEi8Yxk2wxYQIEHKkkKUHyUX0dMgDIkYknsXvQ8kyWD01Ni1YaRMioahspq3m6A+4jkwZh9dDwXvvknbmmUBMiHy/Qnuay/C5UZO1do+zmqmNiAglUI9cVWm+d+XGhEj6wIFkXnwxAGkDB+zgQJNjBqtGg3TLtXTv1QN2M+MC55yyynv/+4ZJ3zyIQ1Von+6jU9wTLcCDx76IEtRvMBYh0hjvdY1ZEyRV5TA9zjPogaqsjgzp2IE5fh+rvTFxkBp3Xa6v1W7GERd4vSlI6e1MIXL0muR3eJsrrL4ep6xy93thyl58ng61DX83581V6JSuuVPT3A0HEaZ2ewKrTWdjeT0VepzI1rvuTljearn5cu2XrK1cyz9n/JOV5doTvFWIXPj5hSyf8xIUL+HT1x7l2yVFBCMyrjaxGJ4t//o3a887n8iWLQQjCsqOH8oTcKDy+l+PoYf1HNGpD8tsLbJbXnKCVUhSHdOLJnHMGt06GIJaRTv3fdna8auKOyG+KatOxeNMtLKpssy2kSOJbNjIJWu0arWyqlIRirlygvMXUj8vljpcXFvE/G3zkZK4ROTKxqvcstmSgmyxZNRu0QLDa0NRJvxcCMDTk1eaDyDxrPJ6KDRcqmu/h0rtxFYUlY9+38yqbXarmSlmk1hP4uui7AkCixYRmGdPQ/dEsV0jIeaaETEirYRVLfqD2snTCfsPye/SXQBePbuhQg/8y7I/DYBFiLRQsCroQkS/AYYPJCFiZMRc8kbDy3Q5laga+5HVhIJIDu19un7P8nhkcHrxFBTgP0KLQfDoQuTJb7UYhwy/K2m/GMlhf6xszFet1gdQamNP1J78g8m58QZy77kbyeGgw4MP0PGZp+n4xBMNH08jGK4ZuarKnjWlxxFlxmV2ZdVqP3S3IuOPhAhGohwqJfaZqQ/FtqXuQIjIiorbYY+l+cfy2OcWcktUh6qJ6o/jlZYLXVqcEHHrVo+wCzzdtNgNQ4gUlEDH7YkXd9VycXfLcOwqlT7rZMr+8zq52xuu/tljC+ToFpPGhIgkKUi6padtmnZdKKoKNGimt4qre3+6l+smX8u0jdMYMWsEkBh38kSOJjoCJWv5xzvzufjln20upuovvyS8bh0lL75ITWjHwcLJkCUnmX433gaeduuq7VYfnxyhfWgbkqKaFXIh5m5z+TS1pUQUMzZN1r/flBB4nXHnzHt/JvzM6eZbp161VlE116lBZVyWkieqBcuSJJC60cqukQC8e2WDs1VVtbm41u0gLX1ruqXL9jzNovL0lJXc+eEi/vXhIu1c+OJWmPmkaRGxCtKDBmu1cqLFxUS2xoKC9wQbb7wpYZpLtlcOVgKBWB0RYRHZ86ypWENZdcxPe9Nk7eY+ojwTv+Vi7I1or2W9pDar9TS27IKEbRpfcEtaRMLOFLL0qp1ho4nY/ixE5Ah8cE0sRTfvyOTLZR0MAx+xqf3iutiNNqNe+x69Xhn0vkGSnjrriyuQluFzowYThUjV51/Y3GBGH6JkGDEiBpLHQ/sRI2gzVDM/O/x+MgcPNgXFzmKuF4nYrTd65pA/7vHZ6jbwKBFOrfqCs+b/M2G7AT0A0+N08Mif+trm3f/Tg7b37/62gTS3Pa3y1JKYOCjOtgfwWS/QKXEPiMZNL+KCgPH9WG5CeeX24ykLlKHE+d3bWSzj509v/LKWs76Cup9/5ugft5FVm1xYHLlW4fkt3zL9luPpkKmdK8VVwcSMCp17JtndJRWhSgCzc7DVIgKatQKgjaTd0JcXVVNhCb41UCMRswfSzuKPBknxOPE4GygElySI8tUpr/LiqzJplp+AEYDs1LuN1/5RRGilZumpSNeOwx/GbG+xrTrIxWNmwMqvUbbGUovb1VcCEI4qTLdUGa6tiBMiRlG8xe8ljE+2dP/dWL2RMfPHxNw8daVQ33AjR7W6hLTJtzPG/RK5JMbhxJ8JDqvlW2+PMGn2H3zkeZh+Re/Btj9g/lswcxRVuvvYa/mqvFmx1+svv6JRK2pzY83iq9S9TJ6oal4jyydOZGXffhyyWrP6+4UQ2fMU1xdTXR8zDfZfrtJ3o4O617fwyKexM8m4QCoer3bSrZupTciyB3lBLI+8RYWIIwUP4MFFVN/Nfi1EVk+F5ZaiYqltky/3j5/Bl2ETIvf+cov5OkO/V/s8Crg0UWkUE7PWEwHI9LvNdNL0s86yzav86KPYmyQ1KkxU1VbILPuqIQ0vuwtIPp9pGrc2dcOlHZM1Cyxa2x13IObv7iZv5urIR/jUxItivS5qurZL5ZzDCmzzPl/7qZndsHBTJQ98/gc+jz3o0hBAm9pCRbrEV2u/MudFGvlZuPWvLeyCbJ9mKbCe1xGnvT/OX7++0WYRAbjm+6Zf5NM2bWfj9X/j9I/W8NpLMgMXaOtmzr2Fh9+O0mujyn2TFHrMXkzGp+/TIUP7XLdWBWOp43EUNBCn2SFVi8WJ79uTqn9WbrSD7yOtI2ONvV4HaOfp1irtfDzIqFnURPzREJIk4W6gBk59daJFwCvLdKiEvmstMTD6S5cuRKx37HLdqGStXPzktyvYvFWzJqtyTAR1r9yEQ5H5aukatsmx4O651uw3wGv8jOsSP1TrDfb6Kdfz+pLXue+n+wBYV7GWFR5NDEWDicesfn0P6cs/4ALnL0zwJLZsCMfptXqrO9avxZVdwVQyvOv55tCZjFz4LgAR4JUSLfPKFCKSitMVux7J27cTLdXOnZ+3/MwFn13Aa4tfM11387fN5/+m/R/rqxKL8+0Kbj1Afmu/fMr0IsZui2tm2ygtvm7o91pjvIY6ju8pDkghkuPLSShwdcFP2hdUsD72kRgnleL2wPz/xRbu+aeEbe6JGJGgS/vVp+ImciBYROIrGvoyE6c5XHywqJxZq0oT/J+gxS2k6093Pt01A7G4nxTVvk6aN+aa8fXpY1oxAOQySwO1HaQRGk+MB785HmdGRqPL7iySJNncMyYOB7LLT9dIFHdlb0KlAwlsup4njoplHb3Z9mO8bg+ZSQpy9Zz5f2RRQ4rHSao7NWH+tnoty6dYvzF63PaUTZ9uQdzYTruib6zZaM77rm/DlxpDiPQPBVi91sv6sjpbbREJqN/wD/P9pro1Zh+oXcEbtB/7TZMV5MBBDFv4FYdtgkfeiV0cNi34kXrfTwBsrw3FMueaSI5Pe6oORe3jTVMUKtf5yfpD+/4GOOcn7Uw8e1MN172pBeh2bZf4nTRGir7PeCFySd+D8EVDFGyKWSv8be3XkTZJqgIsap/41Fyern3XZyxWCOvWxZKaINm6pceqd7NDtRxZ/RslYXvbAF/cJcywiFjThv1t9W7Sc+YSWKJlAxnn409bfmLlljlc+MNtXH5QHvWSxOovchPGqi6Olfzv5diYMD8UF9VbZ33w0R9YHKrC/e1yqHc4eG+b1tzypewsczHjoQe3iiTZrxFGfZjP137O+qr1vLTgJS778jJmb5nN0MlDmb1ltimqdhejWOLc/m0J67cktwwVoQqemvOUuZw/GiItXC+ESGuQ48+xmYcXF0ik18dkvjuqvTZ/EF4fbNeDWy8YC+mJJ7lRyMjv2rWy3U2h1qvt16NgCpH9OmsmPmXW4TTjIAxkh4e7PlnK0PFzWFZkvzF23qZy9nwVoyN9hjtqumYMi8iR7ezVTTdsr0fRXTMOvx/f4b1ju0+NVW81hYjbTXtLT5p4HC3UHM0IWDWquJr782UgAXnFpxAuG0jnnBR6ZcR+5lLpGnA4iA8rvK6ymnbbfuRVz/Okel34XIlVX7/f+D0AaV43EgqHuWOtEiRFpWq+NqZjIokm/9UHSYwZnPxyY1ge+0ZDfL8uyF/H/2YLRvUkMT7tbADgT4dJrMnT37z1kW1exOejfsP/kZWkkN3aLUtYEvwvkns7VYEIcm0JnnRtwMuSZ2XbcDvdKKrCb0//m+GfyWa2TaqiUjQnm5RlQT6Zci811b6Eeh0Aa8pjx5npt2cD/emIPPIyfbiSCHDQYoKUcNgmRD646QQOPyiTf/0ec3vcc9JNZB/aeLxEyAXDDstOmG48bXeohOIZCwGtUnE2hhCxH9PxwU/Z5HnJNs0X9/0aLvFCy1nq9MYUTeHllyeM47JpfzNflzsdkKTLczKhBxBWtYtpvBCp7XhE7I0e41NDCpvj6gJ9nRb7jffUM/SUtlEctYW25QwhYuuMDXyz/hvz9Q4bTYbrYOnHUFHY6GJGDOS2yHYiumvOE4Xft/3O28vfti17/R9fkyqEyJ4ny5tlEyLuqIpkCUJrX6n9NwO2PF6o0Ws+pOeRjPqoduFM9iTZXNR4NTOvT5XN9F01tB8LkWiSm01cTxlZjZ3Cy4orbfOeGS/zt6naBUzyOvA4gf5aMyvDIuKKhjl/3WxuXPIFqCrl9WGzMqrD77O5AKyiwvihSy4XzozErAQDyd8ywtSRrt0B5LjgPSldO0dyJe3JPdvvJvBtrJOoEnUkFHfqEwxxR0UlAMc7VuB3O3FIDo7rcBygZUkAfLpGe6KMRGXO8H3HT220p/m+axQ+eCq2zRRH8it+tAE3tPE7k5wqtfjZVB6wCRHTMhnVPn+33NGszfBj76Y1wos6YV635Je7aEYbUN0JbjqAFD2QXXIGqaqPEKouRYlo+5wwsBG/uqri1VOZ57/8BAO/3cZJy1V6b9QLH1r8G/5AmIG/zUOJJI7PbREZA3q1t81rk+rhrnN62vomxaPU1eFxxT6j47vm4HY5OKkoVmMk7HTjyWjcwhf0AJKUYDmpTItte93PKyipDhKMyGRJ2nlpdc0AXPiryiFb9fgYPd7NsIhU6z+VXpvhwl8UMlfFhEhOj6Y3+KxvoACZqkjUFXuo3mQX2VG0m3CCEPGlQ7+/am+m3g9LPmKIcwY1cRamdEvsx8GlejHB9nJCHTSj5UNUbfizTvc0fC0B4Jt/wUfXw/tXJ51dVhuiuCpoXp+2hkpMt6i7gd32rNgoLCKtgSsYsQXweaOY5nuA1/7QLuLmBdDrRakp0i4d6Yn1FyCmcltSiNT5tH37FfnAcM0kq0AZ11MmavVX1zZcXtvV/iC4ZR701lJnzYZzVZXcsvhTLln7Az0rNvLoBb1N14zk96NYOhxbM1RUPbBTcrnM18lwpLTM+eDQ6xwoNXE2dF0od5S0J6tjVv5C6YexOixqVIIqe3BgIC4jqEDVAixfP/t1TuANglu1J9DKYA1qNEr2iBs5c+40c/l7PrS7OtIlBbecWMujoTgRw5XucKrUqdpNwpq5dFap9l2Fii7V5skeAsuXAVC3Ew2MQw2UFwmnpiOpCu0ClQBELXeQ2AOLTMaKxQQ2bTAtF4HEbFWTez9Q+N9ome5zi0l96V1zuqSfQhHV/plnVdUSqkocoFcOc2SnTObeN5CL+3ayzWuf7uWivgdxZZ92CesZKHV1XHSUVkX6EN21Ux2IUO6N3fAkVcWbvgNXo9tc2DbdenOLOKG8PkwwIid1zRg89j9NgHTSfzemELH8tK+eGVsxu2817tTGa6dYqXckF6fRoJONM9uyZXYbWwxJlAYsIg6HGXcFwMd/o7fD3rbhsZxsW3Vi455hVL8+dPA2s1TA9ldfRg0FqQs3LKp2KEQ26bE125YmZHCpqsoxj0/jhFHTUXQhUidFkLzaj6QhIZIRrhPpu61BfLfPrsWQZTk3PCXaFdN4UlM9ES7PcnFjh/YNWkRqI9o2W1KI1Ps014xfjpgX9f1biCSziNiFSIoS+y5XlljiJeJ+pM422dC2m/ne4ddueLIlrS47WMO5ffLMm6DDn2JL87ZWSFT1YFXJ7UZqpE251Z3TnDjTtAuWUhvnTtA/n8fdb+JA4aT59gDIZOb/g+OE1L3r9eweyUF2qg8lpJ132+triGzejHfdao5eqzaYceJ0qrSvSfydRBu42nhkw+qgUo02/tRBsRLs+UGVIzplouom9KyqILVTtBoy8eJm9fkHk3dcBb7cxHMnLbOBYOdolJdmvIBLv2sWdT/KnGWk5R5ZupYr3xtFyYMfmJVGg/EiSD/n3FGVo9Zrrw9dbI8pcTQgRBrCFw1zULbfbLr5pyNin6sxLVVp+BoQLSrirMNy+eTmk/i20wR4+SROyKwwLS1rMzpS1+0wHO7k36WBcWOd18cSCO2AtEDsfbtIFcq2ZXbXjG4RsWaDu3SNkWtYRHRXTFUDl85pvRw43HZF01iM1vV5ia5zgJotMUuIHHJw25mHaq/122C8EKkIVoC78caUkyzW0M6RCB2D2jEZxX3dqTIpR/cDQKkLUjF6BAtLF9q2YS2oFy9EghGZuz5azHfLNIv87ymp3NUuh+04qPn6Uyre/4DCv/yFmmnTqFs/l3S0hzFZvy9EnZCWmgWApwEtlxOspuqTT1q1geoBKUR21J8lXOvk8lIvHv0H8qs8kVVeD7/5fUS9yRWrESPSkkIk7NMupClK+MC1iHjsN/aNjpijfuGm2EV/2Ff2C5cru43tvSNJAGm7gBZjYqTvOlL8ZF56qTnfFiAZjVlEMs4/n9T+/Wn/738nbNPRYq4Z7TyU4y0iHfqYL9tQQzQuuNcqRN7dUszgmjru3Z6kT4h+UUrxuMxy8LJUy0tTYtkOz74h4w+pxBcylSRwRZsuwMx0TadKtaqXpz9jEFW6Ncm/Lsjor57gwcO07/CwjbFjluOuYGqKg6yuATr0safZbmwnsTmaPOMlc9NaDqnWBGmty0dK+5hgMSwifYv1J2E9xkOWNFfC4/0Hx44jCuOKS8i0PNS0KbMLIuPpX04Sw5AMnxwmOyX2xP3ClUfFxu3Xpqc20vk1tH49kiTRLz8Lz/JPoeQP+i17ihRdvLxw+t/4LntUg+sbGN/xI8el8vyFDmb3krjnWieLu8SO4+jwOg77dBA1lWUx14x+nEZ2jZV6/cbvNy0iiZ/JF8dLbMlyJAglJVnRQZ2CxJ6BAARqYqpViUrccXYP3jkrQrZUSxS4s71dqK6tXAtxsVINfdJHrFN4Zlo9fv2eUW9xhzkt9XZWz01slzBr8yzzdbwQeeuXQj6Yt4kb39IKtV3rKuebtFSmFLZl8533UfzwwwTm/c7mW24l7a2zeND1lj5QbaRRB5DEIhL/UFDxzrtNKmXfUhyYQqSxojhoJrzLqsNmEFXI8tQVbsC/Z7pmXC0pRLQIfJ+qmP52NbI/C5HYRfzb9Mu0F3ExIjcHYm3MkWKS/7Sl8RYRuxCJL1EN0L6+gkhJCSG9xoEjNQ1XdjbZV2v+WGuApDVGxOHzcfB/3yDn+usStmmYRZubmGsm7lw++lrzZaoUwOWxqwTF4rPvEw4zsmy7+WRqQ//sU71OW3GzX8oeNV9nBOCiX5QEF4WqgktJjJ+wuj+tGFYHp0elCu24NleGWJ2nuSKc1QrRwkKO+lpzB2UGYuP11tkDKL3GrcJyT5t0soNvj5Fwp+zA7A1U+tJx+WM3H6cKDkVFcdiF7fYMUB0SS9vG6q34w3BcMGQTInnF9t/njoRIdrda2llElE8O0zFcRenYcUTLynBZ4hO652p3d38jQiS8vlB7YbUurv8Zp/6dT/prF9xbtawcyZfEj6ITdOvjlSR+OczBixc52ZArseAQiYW6GHEFtP8/em+nraQdgxE+UZeaeD7cWKkt00Y/hYsTY2FZmyfRISojtelim67U19MprVPiCsAT/7Ofzxt1fREOxz47JSpBsIrjf9Gysb5P8bNJLygn4URSVdaWr0KJuzlXNpAKff8HCixMpeNG7Tit4twxOyb0lngbL/du1GIxx14eE1yrSmO9qdqtTm6FvdylVa81rk9RJ6h+7fd79nwFp2593KpfDt/qOYgJvc4l9757d9jGoiU5IIWIrFe9bMjHGw06yJZqzItGwBO7aITlMAtKFnDmpDOZUhgze5tCxNNyQsThTqFa9eNV1QPCNbNmq/YE+4l8Mu9k6pUCfbEiYA9FhrJU7RpbQWr4Quru2NH2XnI6ceXaTbjHlKxkzamnAVrfD1+vntqyemCr3TWjW0Tc9gtHu+HD7ftJ1uijGXAaFpG6OItIShvTfXhmgZ9Obe033/g+Lw2iW/hSPC6wNMjzhewX5pxq2NA+ruqsIuFJcqPd1C75vnN1y3TY7SKM9nle9fqvBBx2EefSu/nmVsduvCkB+yXMq1eVLczKJb1TgK1HhvnoFAeyU+LCnn9nR1R60/D47BeGzDpQ4wJwyzLg0HCY+93vUqdfH9IC4FVVMusSa3AY3PCjXueiASHicKvkHFZLx+M1K5U/GuKE1x6nbOxYttyhVWn94KYT+M9f+tG1nS5E5MRrwAY9sy9SrBedi8RuaEog9jo722KqKGj4JhlsqOCVJPFbT+1YnGGJSoeDRSkKf3LMZqnHw0b9xhry2I/3l3WbOFoX9rkVRv2ZxM/k1x4SsgRLQ/ZAXaWurkm/LSU7aj60yaWx71WJSrBmOi49yaDOchN2BHN5ZrzKg2/UsrEudvOPAtd3SO72iccUboDTUisiRW28cFiyEgQG54z9Mjb+HR26fn2KOkFJ0x7e2tTCS/+RyduumuUrFrUv4LB/3UbK0UfvYIMtywEpRFzt2pJ5+eX8dFjct6mnZckhBx3UctNkaPUFh5Uww2cMpzRQyp2z7jSn7wmLiMspUaZm4lPVWPpuK/Qx2BNsqQzw5e9acZ961ctPa8rYXFEPfWKpewGcOP2FgJF2IeOU7RlQBr6ePRKm5T32qO1955rYRSf7yiuQ9PPBoVs1VJtFxEjftT+Z5Nx0IwXvv4crN5f0swY27WB3AYceI5KsZxK6+/ChLsvxxkXDJ4sRSYreAC3F4wRcZnxGfKplxBXz+cd2AhmRREvQhlyJZy5p+JJT67GbwQMu+zYc+k3AF4ntsGe4lOdPuJxKTyoPnnA9Pt0iMl/tQaeTK/jjpNiAlUgDMSIWqjyp4LKLy35r1SRCROKsugCXu36gJFP7bNpXqkhAZsNeA7zVIXKq1aRBnABvtUvju1Q/Lj040y1HSdmiuYXq9Q7Px3fNYVDvDmx7+hkqP/6YtCQOg6JUzXoaLdGqtUY2rqdevxGb54DLhWRpBxA+up4pfWPnh3WIwUaCGY2nf0cUrstrz7AO7fkgM5WrDurAR6nauRh1qSwqiG27bJo2PklVaa8L0Y3tE89N1SERkiRctfYS6Up9vVm35MJsS5pt3G+/+pwauiZpcq1EHFCpfa5/KJ1xW9Y7qEjh4BKFLttgTVWsaN9sv49C3cLoDatmyQd3JPF6E7D87FLah3F6te+zoXOj5yaVHptVXFW1tri0qMVaaXTSBlCSfB3GITiUKJLRHNUJqqWTcNtqrU6OIUQ8Xd7g6O6NhyrsCQ5IIeLr0YOOjz3K22fYD9+Vk4MqSaBKyGGHKUSslpOQHEroGwF7JmvG5XRQTgYeVY25ZhopNb4vs3F7PV69e25Irydw5au/QpdYt9k1OWtJKfgP3g7ak8KRRZt55xmZK36wX+WdOTmkHHdcwj5STzyxwf23uS7mZpH0XkOKpSR2zDVjv2lJDgf+o47i0O+nc9CYMTs+0F3EqV9cgosWE40vDe7RLzw/j0HZtMg2K5kQ+SB6euIOjJgnj2F60044f9zDd9gFrqj9QhxVHMyrPZtQ6QBCZadRuypWpGnhIQ0LoZq4QORlbQts75361+qxuCIcbUKclz+Xq859mLkdDsMraQMM6ZaVWksGzH9mJBFtcdS5faSl2d1/x65Scav239mGXImu+jlQlaWJ0qtnKKz5sj3/+KbxKq+5FWqDrpnNficjctvh0IvfeBpwuwQWLKB8/HiK7ruftCRP0e2PPgrAPDfWXPQXNkxvS6DcbZ4DDr8fwrGbW9At8d9BDuYcoZLZtY56iy6sbSS9M2QKEYk1egbJMzman8Uo7R92qvxvQOy7CGz3oCrQcbsWWxN2xdwFBtXZ2uf4Tkoev2Tab5ZKXR3tN9czcIHCiUWxaqTxDRVDDcSaKlEJtmhN4dapeWbPHICC0tjnucFSgduwmkiKymP/k3llnEy/1QqP/y/RtVnlcfB0mywWeD1IEuQdVwmAp1Zb9qC0g2JjDqg8+rbMY/+TuXLEFLNPTF2kju9qb8PX8X1tv1Yhkuz00X+GWZYA/qgTyLDfk7LqYplqERe8OP9FWps9IkTGjRtHQUEBPp+P448/njlz5ux4pT3A1UfZffqS2000XTP9R+qdZtaMVYiE5TCOuATxsBw2S7ynuFsmSwLA7ZCoUf34FNX88bdWi+mWJhiVTX+/cVPZUhng8xUxB/zqHO0CZLS6P3f1ChwqXPqz/cZ4yDdfJ61uGu9WsWK4PgAk3SVg1GxRFYWKD95vdBuS09libhmA1JNPNl+HN9nTcfHGnoCUOr1KbLY29nghElA9uOMqQFZt8LPpXw8j19Tg8xiKV/vnjzPARVyJ0fhvSufw1wHHMOacuwmXnosqp5MfuYn/O+xeKjfenPR4JJdCpSODv5wQi91ZmtPVvkwozCFbVU5coQ1mWxb07l5FL2kjSBJOh2SeM8tVbTvtLU+Ui9oeypKcLmR3r+XQC4sJ5ScGJZzUqyPZWfYLd/tKFb9sf5RdlwsH60KkRN/MwWUQqdtxPYa8ClAaECJG+mdEDzL2yMlN9dbYoLZjn0yYf/rlWnuCaPEWWzZEoMxDWA/arHPJRC29c+5plwOSxNuDJDoeV2VrTljnafi3ErJYROIxGhpu9ScGNcsRiVuXa7/nFZ0kszaSwcLztQFUeQM8F+dilCsquPuNCm6arHDw97Ho1Oy4h/tQAz9BTYjMZ2qKn6nZ9dRaUn5PXRn7TKqdWebrWl2IHFGoUlCiCai7P9IsJ/H8kubjf5kZ/LWjVnLBKMjmC2j/89NjQfYd42qY1f/6KwCfrphGmArcmQsB1SZEkiVdKVGtTsrB0Vhp/KgTsFhEDLL0TUWcoAYqoWpz4gb3IC0uRD744APuuOMOHnroIebPn8+RRx7JoEGDKClpoDnDHuT240fgfPoB832ba6+FNprJMFIb8+UF4ywi8TcYa6W8lraI1JCCV1UJ6T7IxqLH92W+WVxk+vtDauwKdvsHsSf81GhseorHybb05J99Y83l2v/735T67fOz/3qN7b1RLlnVLSK1M2dSO206gOm+2dO4cnLwHtYLwFbrBABvTHTJegMNr9HC3akXaTr7ccrJ4B+R4aSgqQtVhtXf5bL1l2xqf5nP9tffIHZ91l7El+O+yHUQmWifX8b557M4pyvvdxtIlt/NOYfHau4ckX0aw469imM72pvpGTjdKkvVArpbWtYHiHPNBMM88VZMWHx0ikSOQyZPL9529zk9SXVox7lByeXt6AD+XF3DyVVe6jf+Ddnh5N+nDKNDv2rcfoXUU6WEtNCDO2Tj7WoXQB3L4fwldqtTcRuJPL1z6YbkpYUapEOFaqs4uqSz9lqWYLZeoM3r0o7Dk6TIGjQuogF8h2g3OjUso2wrNKdHAw42/6hd4yqkesZt0PoBqR37UqKfy8VJzum6Ri0i2pilJNa28+ZqQiTitF9HAcJVbnos0ra7Ni7be+WRB0Oa/bsZf1bsdhUqKY25F9Z5TL/EPZsq7ftwSJQk+flXrE5Frd7CiNx2zMyp5DlfLNusR3EsqrrGlcM9Ea1q6/sZ2g09XjgkI2z5uGTA6dGOJUVPdy7IKDDndyxPdO2UVwd46LO15vtOzo0c5NTK0kuqmtA4EqDsj3Q2zmzLqMmvxfbtALen4RTkqBOUooXw8Y07PqgWpMWFyHPPPceNN97Iddddx2GHHcZ//vMfUlJSGD9+fEvvukl0v+DPdJ83j4MnvEn2VUNwtdX8yMZTQ9gJsqV7ZTKLiCFEfE5fi/aacTslalRNiBhPF8la1u/rBMIyH/6+OcEiYjAuegGlagaBSKw7ZqbflbTUddvbbm10XznXX8f/nfkv27TsK+2txCW9+7IRjxPeGOtTsaNU8JbEqae3JoxBz5BQVcxqnW6/diFUOp0KF7wEJ91K7S0rOO7sK+mkPzCFql1Et8cEuFxRgaQLENV0zdgvmv6ogw5e7UqfffWfueuUm6n1pBBfU6pnB01gvDH0mKTH4s2MME/pYStjHsBDh2MqzffhydNtwZ9h/Qk6RQrxwU0n8NeTOpPt0o69Dj+/Kb1IUVXujqQj12k1ZByWyIdu8lJy+1pqz6BlOaWffTbtht9O/uuvgcOREHAKWkdT9dALAFh1UOJ8gNr02IofnOIglKndOdtWQY1+6d2WBY/92ckV97i46m4XVam620T/GjwNBC+qyTKdLDjeGoikD1zeFjtfS7bHbkpBN7xR/BPr3C7KUpKkrFiPxeNpcN5JunsnXohk1qlmTJFLSRQiG75vS0S/zhZnxxUTa6dZfq1MPsZBtKOmhFeuiVlBlBoX3bZpy3ausouXkCeNB/+SGCAaqXNRTGx6yK8JTYei4rW4GoN11bwnn0mZ08Fq/TNIDyQ5IeKwCpEtLhdOr54OH4JMRyq5c/5rzjeqsVr5Y/psrN0Fx6c8SLCtlt30f18r5CXJuC9bnSRHWpKgXZvE6ToRlx4LlJG8PtaeokWFSDgc5vfff2fgwFjQnsPhYODAgfzyyy8Jy4dCIaqrq21/ewJnWiqpJ5yA5HTia69VKTSESPyPJySHcFg+tg3VG0wh0pJuGQCXI2YRMcZljYDfX1hdopmdUyXNAhEvRJ6JDuHY0CuEo9l0KVb5x9cy+eo2PHEX5/SzBtLu5uSuACv1bp+t0qQj3W4GdhhZMwFtPJLlopw0WHQP4UjVhUi8RaSdFpirypL55O3ya5+NYnHlHdw2lZtPP5RDO+kZFvVxF+y6YgZ8cwoDHL+bmTNGjIhRh0AKq2ackuS29AbRlcib1x7Ldf0LuPoErXtuui9J9dCsMLl9q1mqFJDicZktyQN4CRziI7dfVcI6EOu35ELm+K45eCWVzKj2uHrEIfkE9diizjXz+T+n1sXZT1zzuTz7+5TjjkNyOGj7f/9H2imnJKR9G8hOiewU7cJfmuGwPa0bbO4bMx+Vp8P2Y7TPvv9ylT9pzVobrPYqmTEiDbhmdvAAItVsxaE/hUcLV5jTrUW7jH0v83goJLafO7dXsFixp8tGpeQD7RUKc1NtJZAYyJxhTWPert2cIyf1S7qdUt1q0faCUvLP3M5f2s7AmyTo3MhQKi+x14S5b7mXr5Zto/5Xu/kjdOZ9lGck98/8IyeWASM5tO+qXcge1B6ur8GZsp4zDo6lCg8s37E73Fpor8LpwOlWUHVh0U3J4pi6mGutx+bE42x7z610qIvVRSp0u7QYFVXljCXJhVCogWdgh7SFg/qXc1D/xIaNEafu5mmgUOeeokWFSFlZGbIskxuXJpmbm0txcXHC8qNGjSIzM9P8y89vQlepZiY1T0sTqyrUREUgLvg/3jVz9w9375E+M6BlzVSrftIVJRYjUr//WURWFNUAKkc7tC6dq9ROpPvsv7J0r5v+h7bhsbdkzlisMnTG//DG+dOtN8Yd8Xy/K8zX8fEkkt5jxrj4J5RVbyUaFCLH/x1OGYEcMWo/qGYL92TC1XfeSDjoaCLdh9qmS6sn4wuW8l/PaLOWiOGaMWo+1C3dSlTvoSF5PFx0VEfapnm5QC8rfkbP9jw0uHeDregBugzcjjczyhba4pAgEDEEpcQbGbfiSkn+9G+rqlq1GT6MjX/Iyb1MIQJwt1uL6UnFfhNxWmpneLp0Ia2TCpP+Cts1s7irXfLy6R6nF0lP1Y8iMfmYxOPL8MfuzMXZEqTG9nVkoXYzaaj/jiFEGqKx2LCoHstguAPq18aaE1oess3MCxWo0bNSjgiGGFpdQxAPjx0X+zxdDdTnv6GyCp8hmuKFiKWRKBIgSRx2/EZS2iX6FYra6K6/1Chp7bX5yYRIre4G6jbrG9t0d0givDTLNu2uwccTJnEb9fppUZfkw88NH2p7Hw3U4u3wmfneGUkj19s9Yb14rBaRoCTxtXo8sm4VyY+kclQozNAq7UG7oIEohV5lWzl3rkKf9Qpb9Oy8+BgYK/HF/Qy888aTkR/E1zExiDrqBAVp/xYiO8s999xDVVWV+bcpPghvD+Bqk2N7H19rJN41s7FmI7Vh7exIcycxjTUjbqdEDSlkWoTI/hgjsq6sjk5SGblSJSHVxUpPb0Zd0se2zEHZfhQpbAZKFmzemBDYFylJEkWWBEmyZ2zEFyFz+HUhon/W1rLqOf/4v6YdVAtgCBE5XohkHQwDHkRBE1ROt4JDz7GNbNqcWHsmuzPc+D1Ryf7AUL3R4ltW7RYRo9mZGomJBMnj5vkrj+LXe85M6BRrxXf44QnT6lUvIFEbsn+Hi1NOwNH99KTbsV7s+XoErPjKfBtxpRJW7WPwEjatbOaYLQ/LKYe0hQnnw7LPYZJ2EzZctQC/9Ygt7HN6zfLf/9/eecdJUd5//DNld7bd3V6/4yrNowsCUhUFlGIHFQkSMP6sGEFNFKJijCL22GJJ0RhjbLEklmgIFkQRRYqggiDSe7m2t32e3x8zO2V39gpc27vv+/Xixc7MM7PPzu3OfOZbByRJoc8zRG9u7gL4PIk3xXASIRJNIkTkYBBHX3oJwa1bE7atL1KyfT4YpNTMEezKMR75Yak25rBsISg4IFynPC3b1Zv/SdxmfN5F/81tTO9lOZ+8aBS8qDbykwEhytBvm4yf/y+qFSoDgBfGKtdMce83CdKgqiiCw6rVwm7Y6rAQIpV261tW2C9j+0H9+/vQBTw2FqTDFzL/bbb0isKnfq3jM2wAID1qvuDbanym+vShqAeyYC1OFw3VY8vCcULkMEtHLKM9q04GY8BlK8L4+1eH9eaqcfQ9uBeX/U/G7S/L2Kd2Iu9jYT2JkUyIxM7jdiSmrytCBEl7qLUWLRppl5OTA0EQsH+/+Yawf/9+FBQkfnBJkiC1UCXKxiJkmn2lobgzdMPHN5iWvZIXvkjruGZsAo8a5kJGVEZANVF2xBiRnUfqUADFxL6PZcHj8aj1LHRy0yQEDdUiecZgj+u54R45slHvx3McNmWW4t3y4Zg9ZWRCMDLvUi7wsXMdrVausFmzZyP3+uub8Mmal6QWEZUoPABk8HYGTr1ZRI8cwY5fXI6yv7+QMD5y2Gy6jQYF+Pbb4c4PAaEMwLlTs4gczEjYHbwkgeM4iIK1KTxG+csvIctYETazHFN3KhfxIq85ddYu8uCzugD4JnG+ItNKyKDOHEGYn5mBNawHPokOwBhB2fckfjOqWfLfqG3/UmjX6sOKy80o2v48gce2PIavTuAgCZLW1+feg4fx14w0AIZjcww9M+rwxJBMbM9XMkIq3YnnJZlFxCfyYByXUBPn4COP4shzz1nu8/sLQ+i5h8c3XfdiczAXC1SVnlOtHyPN4rlFBodwXgUQ3Q2bKgREtTjgzDNvw9RSG3485IB0xAc5mIdJ6S/iI7WHUs9QWEs1BpSsn4UvKfseTFfWC2VhHMwScG7+cOCnHXDlhOA/qF/nq8vDABR1IBo+rtEiwqIOcEIAR5IIET4go6hacdd8ff04rHR9DO5IFM+s/wMA4HAakF0DLOuRjnGH1et1kMFUghdARlxrgl/8uwb5A0T03xPBrbME+KJOcNXWrsJ1uT3wau+TIWR+ZSrg5lctVEEHg7MakPYfQqDGhgNrMlCfzbZn1Xbt9X5R+aL02548NTy+Rslhr3Lzip3HNawn+kN3Jb8+kkNU4CBHYOrD1Ra0qEXEbrdj8ODBWLpUV+SyLGPp0qUYUU8Nh7ZEyPSalhvqTeW2uVulzwygxIgEYIdXjnboYNUdR+pQwCnRWIf4bDxyySA4bIlCJBA1P9I4eLPfOPsXv2jU+3EAwHF4YuCFyLkyMXqcV10zLGYRUW+itsKCFk3RbQhNiNQmESKiYt1TLCL6Rb1u1SocePTRhCZXUYuYrECl8kVzHDgNvYMhONXKqhvKuIRaBlw9AY2mcaII0Sj4r12Bu6++BPdN7Y8h5eaYDEnkYe+um8LXGnqbmB4S4mIpSrLdeGLmcLgv/xcw8FIAwBT+0wTXDACUjTuErPNOR5ax1bxaTM2ZE9Deq8rN4fXRPHbkcTjoP6j1IcmPRlF2YKD5M/IMDgDrTong4wHKZfYcC8FYcjBhFQAlloOXEq1KVa+/ZjmecUCtE1jTQ6ki+7nLiX0u5X0nrNb/zl7DzYqpN+GgJw+RgdOVj60O/VFWTPUDB1egqmsvAAKC+89BuHIYHj1wCEt27MaSHbvhZky5i6hdeSWDeyZX/TqlOQV8umMX7i4YBwDI6Vtjcj1FDA8Zxl95liHmK6IGG1farX9vjm26YDw6pDvAceBsekTnzb8QcOe0DLzbPVeziMRnngz8UcaFr7yZcOyzvjmK0kPAkM0M3bJzkgaoh3gbnut9Ll4e7YQccUP2K3ElAY7DJ1xXbE1TC2Ye8iPsS1Sg8f14uhuiF/YJyvjceA3EM/ilxN/dt1nlWHil8vt3qUG/W1mh1ln6PyOAV8Yox9wgSfBld7f8TK1Fi7tmbrzxRvzpT3/C888/j++//x7XXHMNfD4fLjMUjGpPiHEWkYbio/Nd+dij+lezHdkNjD4+JBuPMATFNRMLVu2Arpnth33IV1MyT+rXBwNLvFoAY4zcNAnBqPlKYo8rrBVLu20IvgExEWtcp1lEVCHCpyXWJmlNYta72mXLEN6f6IaKZikZKoJDhjD1IdO2w089Dd/yz0zr5CqLJz0GRDkRs/AFXtmzT3PNHEkDfozLFjmeVObBZVmYNjSx/0+awwaxtAKOLOWNKy7SxaUpbGHPmoR9z+xboAibMuWhJ5erQiaX2GfKlRtC/s/PNIk1iA7AdwhZtg+wYVgY111jYbow9D3awswnI3Yurt3jhv+nqzFk0zR8Fk4sqpfuB0rTEj93mOMAMfHyHK22vglyDGY/E4CovXFNzPyDLkVYbSYZqzB6VVix/M4d1xOSYR7XntYd5wXvgj+cgwJVKHCDZoD3qL+FrZcmHN9d6oCLMXDVSq0KXgTKztC/ayHBWsCeYCjWKIcUgdqYXoGimq5qS1+vratxcfi2PAJO9KNOUg7iDgCCQYz/5lUZ7vhu1gYiAtArPzNpr7KQIAKyA74fb4bvx5sRDXsBKEJkTeYR1KqXo6y6MHZ/ZhbcskPGnGsFfFNu/QEPwbqycd7QSoSFxN/dZ136o1JUhK9XrbJaAxfuGH45XuhzJl4bbg7If2NLogBrTVpciEybNg0PPvggFi5ciIEDB2Lt2rV4//33EwJY2wvxrpmGiLIovj/yPQCgd3bvlpiSRrpDRBgiMqIdN0akqi6M6kAE+VwlAIBTg6hcdvOPrWLzaoz9wHzzTeZrbYiGjBqcKkRYKAS5rg7+dUotE3tp6wdTG0k/azLsZWWI7N2Loy+/nLA94lSeIsUh50Ic9fPE7XExNFYWESZziPJ2zBXfAAcgJ6Bc1Px2Dt/0NZuJrToaHwvDuuoXaZddANKLUXb6YXQ714d+Z+ip1WER8M3+X8MHVDOi3JwfuZy1WR3fv21eFiVg23KIDhnevrVaTIwJQ4Xly84705SVwHuzsbnfDbg+eBMigXJ8JA/CZrkI+YOqEBUYdqsf8X8DOZSkJ36PwhxwmGs4O6NepMYJkTrICKv1SkTGsFHqjy1MeZp3S+bCfON65+Mb1h2nh36PafnvAHdUAuc/qV03PTWJcXJpvVU/nqFoliMzhN3d+8Mv2BGNc8fFcBlEQpl7AAROhFB/1jIAwCYkiU/ig+Dth/UYkSCQJiuN4Lrtbdy5ynfmahaR34ybZ9rGVGsDi3qUuBI10+xv8mmo42yoUT9mUVwjRADgA4olK5nQqtVaLMRXMebgCJqP900PAe90HYYwU74/6bJy0mqYC6vzK/CPE85EFcy/1fiHutamVYJVr7vuOmzfvh3BYBArV67EsGHDWuNtjwkh27ofxc8LT8PJBYlPNIFoAFsrlcCxnt6W9bOlO2wIQ4QNAIvdlyMRU2+ClEOOAls/AYLKU0bVVy/jYduTKLapTx1Or/JfnEWk95N3Y9LH5qdD2zEKkaGqO8Drsr6AxVwgAFCzZAnk6mqIeXlwDrIuztVaiJmZyDj/PABA9FBii/vwQSUc31bS3bKzZnwtCishEg3xsBuqirrUa55DiOJCXr+pl/zxmWbr3vns7KHaa1HggIwi8DYGyVUFLqLPJSJwcBck9hDCsGvMy2rJew8CyFUFbgIb/mleFh3AwU3K4ZJlqPj1YxXleJFeoo+zFRZhz4A5OAj9weYI0pBV4cPHl9fhhisF3DpTwHNn8EktIsnSMRtLTU4DD3vqTa+ORbRma7Zup+OZonu0IR5JxJiKXEgij/G985Gfrsd28IJNU/Exl3Z6KNH9xDnVm55BiHAsio9mzcf0Sb+FxOXg9kNH8NS+xPSRuq3Xw7/7YpQ4BiJDStdKxtdHfAfbMQXnm4+pfgRXgMEjy5j5oYx7/2o+8L4e5uB4QAnUnlE6VVs+WlSO3e7k/YuYGiy9hVMyMWudat0bf/I4jwNe6/Uxl5cUd6nn88IJQforejDIaXpvnjTVNVMNo+AzX08Frv5mfC1Nu8qaaQ8IHjcKF92dsL570TDcf+r9CesDkYAWq5Bmb7jN+PGQ5rAhrCpjQ0PU1I4T+fKPwN/OBV5SfNSlH12HKcJyTGZKO+tYlVBHPU23YpQk3osbxcPTTsT/je6KN66xDm41VrHcc8t8AED6pEnghLb98QJ687uowW/NolHIwaDW8EzMy7PcV+vRrhITIq48/ekoGtDPO2N6ifiqXdchc7J+wzKKtePFGJgs8jxgdwNOtXfJkVUAlDomhSKnWDuyzSmXmBRX8lz9DrnhRy4qAQDfy6VYKVtnggAAeBtQpWTtpctJnpYdhqdKp9nUbisqghBnaqtkiiBycAzgOGwu5hAWOVO57xghcKg+ztj3XWkNBP6rH+uvm17SAu7FtEJEbPp1zC2JOL0iD9//biL+PGsI0gxxK8aPJ3qVv483aOE6itXoqd5tWMlQE+UQFO04zNJxcU0tRvvNgu+U4O8RDXZBpPokeJ12ZEgZWr8hbX75ARSNPIJQkTIv98gRCUUlgz5z5+2YEHEHgNJwBJNXJf59DxeUYX25+Zpzdv5Y5MnKH4Wz25GW5sYedz3ueFmZk5i+DpxQp1lEpJrkJtiXT7W+zsULkcUXibjx/wQ82C3RChkRAFeZXl01dsQaQ6B27wKvaZ/KYGXyz9EKkBCxwDtVV71O1Tx4ateJlsGo/ohfy96QhJbN+HHYeDBV7Qu8XlQqpd0zX/9V+X/bpwmdMwFoFzGH4eY0b2zDgVVdHnqw0VPIS3PgtrP7aG3V47EKSE0/++xGH78l4T3KnGMBq0yW8dOUqdh61tkIblQKWcWESOlz5mrGRouIXFenBePaDHU7IkH9EhE4agMYhzpRwmapBLbhP9O2cY2Mx2kMxvMdK4yGdMVVwL19LaLTD+OFK0N4sI5T7ob/9z/gl6uB7mOB0xYkHlDtvePmAsjlFLH1QvQMzA7djENckqqTdrcmRADgjkMWdb2HXgH0mwpc/DegcABwxu+QPWUMhMxM5N14A0Z0N9+kjkL5LjvjWu8WuBMzCMMch+fH1S90oxxQZSFWYgVdl/P1K/P3h+jn+cm1TwJQ3Br5BgETs0Ty6t/BLRmqkUb0zxFzzeQarESC14viJ/+gN2GsNJdj8Kmp2pWwfoDbyXSLTrrThgx7RkKnZ04A0ksD8A1z4avZv0bRww8nHGfpOnMMis+hfpYgMKHiQsv3rnVlYF+m+XpkD0Q1wc+npSHTbcdDgy/B6twT8GyfyZbHAQBBOgjBvUkTljZDAxwpw2ziqHVx+MKQJh77+zrCSml3b7Wy7XC6gF25HN7zJN6TkmVi1RiyulxxNZaOBixKtbYibdMoI4WoCIew4qgNHldOQpYBYLaIOMTmuxhbwXEcbHYJYICdKZURxSAgp3JRM2PK82ePJG5XL2LpDhvuOq8vwHH4Wb8c/JDkcFFJRMV/l8KWn8QK0AyIeXlw9OvbYsdvCrx6IYoF0Mm1tQhu2mQaI+Yq58IV7xKN6lf1ulWKpcHWpRB5A9doBf2iBiHiR18AB7A+uxuivAC304bCRYsQ2rEDjj59mvVzaXOPCZG0fGC/EnzYjwVxf2UQSFcDRJ2Zyr+ZSQLuNNeMH141WPUIS4MfDrwRHYUreUN8iCcfqN0PMNnkSriwxod/pnnwrbG8gOQBLjSIu1FzkTcKyJVlzU11+9l9cNc73wFQGgwCehZDjL7Zid+lEC9gSwGHd4dwOMviiR0AVvfgkFPNEtI2zwpE8bJT0G64ViwZxOHLisTnUBtvw9g++fjzcqWhJB9Xq180FKYLRnTBGqtAm2foVtvzs+WK1fAdtb5LyBzkGasZc5SZHwCCzIZFkZ+Z1jlsAjLsGdiVzcGYQhArYw+Bw9Ehp0DwehHcZY53kIPmuh/GrJmMHmcA+BfiqXZn4FDQ/F62YFT7nQkeDzJdNlRJabh11JUJ+wMA79AtQLxYp5Xv147njqBo5FHs+iwTjr4+xHLHjdk8nrCSYjzjI9nUSsBvjyA+9ThGktpzJotIvPX+aLBthQhZRJKQd/VM8KKMwpOq4FEFhtWTcV2kTuu8a08S/d2cxOqsdJgy70Yh8r/fJm43lF6fOaIclw4rxaGnnrQ8VI0DWPun61pEhHT9l36xcgzo36Zpu0aEmEXEpzypxWqcGIlZROJjOJis30gC3yk3S9fQoRAdMsrGK24doxCJlitPfYfVJoF2gYd36hTk3TCvxc6HZhFxWZjAWXJfuwnVIiJxEeRCiWupgiLgaqJxVsw8VVDtXgUc3mLaZDc8iAzOH5z07Yzn+cw++ZBEHmkOEfuYcrOOL9SV78qHUzQHbAZUd9Pfx/J4/GweP5i9CwCALV24hDpHAJAZURsc1vMn2ZrPoWsoMbZM5EUM75aNey7ojz//3LovUIyjPn3/WIzI+J1fA1CKAmquS8nC0viz1+BUrZzGG+SU4G/RN/gX/C06wTSc54Auni74zxAOhwzeCGMacMx9a6wvpM4OPZje6sEYI+KstY6vO+pIx1c944SDP4zQdqW2h5iXhxxP/RbwkbnnmJYrQ+bvDJM5SBkRdJ98EEVldXhxj5Kv+9IYHkfdwB/O4iGo1ZEHbGO48DNDDxzDrSb+uxEx/MwXHNIDqGsNMSLG769X8sIpWAcMtxYkRJKQPXcBTrg2B47MCDBwRtJx1YY22g6hZS0iAGCX1NQ0U5n3VBYihh+AaPFjkMw+0NqPP8aRv1g3TDzgBewZTct6aiyOihOQPnkyOKcT3ilTWuQ9jgU9RkRxzcg1cQGnPA8x29r9YAxyjlnVeLVTsai2LddcM9evBVPLL43pV4TXrh7RomLs0uGl8LpsmD2yXFlxPELE8PRXziuZQlVMESJ1iPvNuq2rZgJA14rztdePnv5oo966JMuFr28/A29cMxK7kYsrQzfgaZ+5dIHACwmp/587lXMdFTh82p/HAW/iua526Z1vTR8hrNyItxQC25N8nGxRwFP7E4NDY4GePxtWivF96g923V2pW2Ljyx6YXHVSnOvlgmeAE87EHef0Rd8u6ago0j/7N6wbIhaGep7jUJZehojI4TlDX59YkWsO0GoNGTNA/LuUazdXNwA1m+6EVDMZfcqUpAN3EHC//IHlZ/s2IGJXLodfXS7gzRFqv6YaP+rWKGnizoEDceoJyb8rAPDnC//PtLyfM7uU5QiHD6K62CsJKxaiH7twuOp6EZ8MSH57NvYoWnyxgE/76N8Do0XkRDWjZkW0j8k1M73XdAzIGYBLKi7Bp5d8ivvHJMY/tiYkRJLBceB+9jIw/RVgpF49s8ij2McK3UpaaYTpEcstHSMCAHabwSLSEYqaGVPtIhafI+4iFt6zJ3GMSpRv2aJyRQ8/hIrVXyNt7NgWe4+mEu+aibeICNlZSet7sIB+wWZBxb3Iq0JXUIUIi/CQC4cDmeXamNLCTC3TqKW4+/z+WHXreOSlx+zoFkJEbkQKBQAIIuAwl4KNBY76EfebdSYXsjeceC3OKDsDT457EhmSRWnZJHgkEQUZyuf4rzwU6wKjEsfYzVaDlyJmkWAVuFrtAj7ur9yAfKU54Dke94y4E071vEQFDr++XEB1WqJrZ649B0WRxPPXmO7hFwxS+whV6DfihLIHRqtPfBC/mmnSNceNd68/BVeermc+xUTI7WebXX08p19z6wzWgJhFhOOYVu/EKEQiNUr2y+odlYAsIXhoLArylYZ+Hj9gW5lYsRcAvlMLNe3I47TaHtK3PyG8TbGISL0qMLpHDqaepDfDG9vLbInlVPEUo5Y3n6NoRMAepn+vnXGWst7BEHJmWT/0GC1hPieHZf10IRIxVDaOVVV9MTrOtL/L5sKLZ72IW4ffann81oaESH14S4CKiYDB1Dr3pLk4t/u5+MuEv5h6y4i8CIGvP7isORDtMSFirCWSwkIkmLyAEIAEISKkJc9Miggt7x5rLy6ZGLprxgcmywkWkVg2gxVyQP/eyLHOwmqnYd7GwKkXtOjZzwIcB1kVLpzU8pY/wByPcFwWEQC41Bw/EnPNxNenSRDDhrRGr6cAD5/2ME4pPqXx76uS5rDBbQi4dglm4VGfgF65badll95NRRyW9+Ww8FIB/RbfiC9nfIlzisYg25iWzXF4e1oIpWPNgat8cWJ6KpCY+mrFPRf0x71T+uOBi07U1sULEWbsvxPvmnHFpbyWK+dzi6z4GHLTJEzurwfwuuwCfjasDGOKxwAA/JL+G4yVl18nd9MsImNL1QeFiDdh7jWBCOTCPEQ5IKsW4LZbP9jUGiy1m4o5hARAOFKlxVIJ6RkQeA4PXayfg8KMxN+F0UrOZPN58AzuYWrMGN/k7/yoHbm/XmhZ5Mz30/WIBvIR3D8JfT0TIRTrASRG14yk/kbC7TwclIRIE5nUdRIWjV6EkrQSFKfparg1rCEAIKqppJLcQfrNhOppJ8nbAFcWtlVtwwX/ugDvbH0nsWGbAZk3P4F0BviYMGMMcnV1gkWkviJjR/7yLCJq/RHNIuJwAIMvA8dx2s0lcrRSHRNUx7RBPygrlwlrpEUEAIoH63Z8AD7VJXPFqd3M47xxNT2KTgIySoG8vlpJ92Ml33CjssUJ5lOLT026n5MxROLuI9dcK6DarWQNbSzh4Hh7NqQDG4GwD0URc10J0cHgzgtBKtOf2Pkh04EzE8sUNEaIOO0CLjm51BQjIcQJXr4+10xeXGCzKwvLpqzC5NBiAMCh2iAKM5z4fP5YbLhzAlbffgYKMhwQeAFfzfgKvR26i+NZ20T8PjwVj0SmoiaguBqHFgzFK2e/gsJa66d90evFpmLDCqv6N4YHjojImWIyAEBIT3wgilqkeRstTCxqFptdbp6DANMPzMX1KnOOXQiIEvxO8/zmj7oKcrAIdT/dgEsqfo6XptyPv87WGz56AomNA8No+Yfk44GEyHEQMxUCrSdEbKpFxM6YbhFJUnI4JajPItJlICBKuHvl3dhSuQULPl0AX5VFGqVKr7y+OCGz4RbdHQljp+Ct552fYBGJWUy08TbzjeboP14CALO14+zfA/O3Q8hWbv7Ro0pEvayKFc7eBkKkzKLGi9wEiwgQZ0FRbjT56YYb5qh5wOgbgcmG1O8ug4DrVwNXLWu4BG8DFBjey86b72wz+8zE/JPno1tGt/jdwAEIOc03ucMZ+lxmqu3ksXcdEPKhKGwWIi71PGWfo59Dzp0GjPwlHjntEdPYxrhmrBCz4oSIsa6Mx5CePO1FQEy0WtbAgxCU72bMMNDF64RHEk19phyiA2MGLdKWN0mleDQ6FX44TBk+fbL74MnpiS6wl64YDkmQTBksUk9zIcpql+J2k9US7UBi4K9Ve4eIhRAJGKrvPjDFnLUm9hiKrvxefUWcCHao7rpQXCfrb3J0ITakPAscx4Gz2yH16Y2onccPRYmuGaNFpCy7ZZuzHgskRI6DdLv+ZWyNQFUAENXW43bGUKX+1muWLGmV924RfEm6fgFAvtIuvtZgNXlm5WOmIbJdRJbat6jHzXc0//zaOUZXUWT/foQPmGML+DhXVnzhschRJaqeBWIWEUm54ToyIKg3l6g2RhUrbWERcWUpT/AVZ+nrmuKaiWP2yHK8dvUI89P6GXcCvACcfAVw7hOKy+C0BUock0U/j6ZiFD32uDLkNt6GGb1noCLTolIsgPcGJH+inVmlink5CoR8SIsz8aepQkTw6Dcg3qm8Hlc2DgtO1muvNMYiYoXg9SJrlt5GQOyiP6Sh5GTgnEeB4dcCPc+03P+EfOsaPlaMG6KLNZ96PTz1hFxMGVRsGtcjLw0PGdxHADCiezbsgh1+gxYyBnN3ue9e3D31NgBA3bZrgYOXYFrFNLx9idnSKqQlzjcSlRHTQt1yld+Z3+Dqu3BwnLXWlYVVsvL3DjhyEwLzY/eUsMtgNXE6IRgKKRZn6i6krq++im8eOBc+p35NcKjiKGKwiNx5bvsoPWCEhMhxYPTrSmLrXJztWvou8Hlv5c8XUAtXpRyBaiCQpPcHAHgUU3Ks/PDgzTKmL1Muqh/35/D6SA4bH74C+bfcjIo1q+Hs36/Fp9zeCW401xCJNyHHCxGto3AwMf4jlglR+8ky7L/3PkSOKNaoxjYTbHZG/hKY/g+gxxnK8tDGdVfWiAWE5vbCb8/tqwTc9j4XOGESMP5O89iTZgKz31EEUDNRYHDNFLot8nGRvE9K2Mbh077WFplYozpEApqr8/U6/QY1waf8jYU8vYIr79TnkunQrRnHahEBgPwFC1B4912wlZai8E7D+eQ4YPBsYOJiS2sIAPTMT8Pwbo071zaX/tkqVSH51IyTtHRgI5KhIvPVYxRLgsAJJiHiHDgIvMcDMS8P6eeei0O82lsqko6zu52D24bfhkU3vw/B69X2Mbo8e+Yp36vzBhbhn9eMxNheeVrqc10kLqMxziI54dJf4b8Vv4M0Z3lCQLUMNWDcUM2W+f0mQes1WEs4UUR6pp7pxDGG2NaQ2hMkP13CaRUtV2PpWGnfESztHJMQaS3XjPpFtoNhn3r9kH0+MMbaXSBlvax6TreGcAIAlviEq8YF8Kpv/5Z/6tsPpyltrH9XplxcYx1yOzu+z8wddVmc+yJeiMQCnWMWEaO1Q8hUbgzV775r2qe1glWTctFzwPYVQLcxTdvv0teBZQ8AE+/T1wk24GeJDQNbgrIs3SKxcMRC/HbF7ZjVd5ZpjD8uWPbR/brFcFseh1O+TTT/22NFt3yHgDrFenWCPRP/mPwXyJDRjfcA2z8H79FdA8bfizED6FgtIjG8F14I74XW1Uob4q+XnYwnP/4RZzaQNsxxHLZOuxJrvt6EzV7FCiJZdCoGlFo3MU7tqQfJGmM+xMIC9Pj4I3A2GziO0wqtAcCtZxniWQzZZ7zBJfrmnFHYdsiHvl3SwXGcqVdS/N+z+LFHsWvOdci/TYlfOaV3MdB7rrLRkYFz99Xi36q1JdaIsMJWBEB3SRdkOLTUaY/DfAvPdOpB3Q7GtJJnMYtISWb7c8sAJESOC5chuKi1hIjTLiLEBNhlhkDsfiDLYIGA1iW23XNoM/DOPH05vy8weh7e/eezOAvL9fVqpgTPJV5kYpHzLV3NNtUJqemGMXiX+UIUC3SWtUBUg0Uk17qhFye1fOG+epHSgBOsTfz1UjpcESNtxKBS3fLQ1VuK5yc9nzBm89HN2muBEzBWFYq3HzqCe4ZmwhHisLa7+fdgj2kT30Hgm1eU1xnF6J9ryIzJ7g5+3z5tkTN8D/Kc+hNyW3ZhddgE3HhG42K8olOm4c/BNdqyKcPKgGSIL3FL+u1OuX4oJ07MyjLFUmW67KgJKGLEY9gnFisVj0cS0a/IOp07/nymnX46Kr5eZf3g5PTi7kNHNCESK/0/8vpF2PG+XhzNKLrSHWbh6HXpQd3xWTgAcM6J1pa4toZcM8eBMX23tWJEHDYBYYimyqpAivWb2bfevFw6HIGK8zEncA1+Fb5KX6+aqa2ESKyGSmud91Qif8F8uEePBgB4p5rrENiKikzLMSGiWUQMT3q2ksRmbEAbumZSnIoCJWbhL7OSVyzd69ODF40dUS+uqUVU4PDaqQI2F5ktn1rF192rgR/eV16nJ95wbAUFyLn2GuTecAN4u37x6O7Vgx9zXRbZSe2Q/IYa+qkYb9pG64EpRqTA3Ovn8emD0KsgDc//wtxt3a22SHD0t059bixJrbelI8ABeHbvftw27DYMzBuovG83c1NHwRCUG28JynLpotJYCVhCGN1y3Lh0ePvMKiSLyHFgdM20Rnl3QGlCFYGA7GgUjOMQlgSlB4LPB2TX0wmyPbFntXm5xxnYXx0AwOFdfiwexDPK+mwlmt1KiMQKPLVWbE57hrPbTWnNrhEj4L3oIgR/+AGOAQNMY/NuuRnhPXtgKylG9b/f1gSsbEzfVbGXJranB8xihWgaUwcX17t94YiFuHW5Yra3CTYlLubzx+vdR/t17DcIfIvfDADkXn99wjqO4/DeBe/h8z2f4/SS0+t9r/ZCtke/3vYuTJ6ibjcKEdW64RAdpiwYe5zgPrHEi/fnJaZTF9yxEJVvvYXsyy5L2JaM3438HRZ+vhC/Gfabhgf3mwrUHcHQ4sEYWpS8hYDNYP2Jd8fn5/TWXocN27LT3XjmmpEmEdOeIIvIcWB0zbSWi8BpExCCiHI1RS+glniWfb5Wef/jJhoBVv/NvE7y4FCtciPN9tiBa1YAM14H8pQ27VZCZG+W8rnje3R0Rspfe9W0zLtc4F0uOAcOTOgvY8vLQ/lL/0DmtGkAlGDV8J49iOxRnsSN8R/28nLL94s/JtF8nNv9XO11obvQlCU0RkhSnG6GhbtpSNMCeUvSSzCt1zTL31p7pDTLjUyXDTkeO/59XWKabgxjo9KYEBldNBr9Rd0yINRTa8eIvawMeXPnNno8AFzQ8wJ8Nv0zTO81veHBHAcMuxKoR4QAqFdM8DYnpqm1hG45rLiSXoqcjnPOOg+Z7jZ2qdZDanzr2ilG10xrxYhINh5hiChTG1v57EpRp5RxzYRqEjNlbC5U1ilCJMttB/L7oK46Ez9Nm4YDX32GL/Z+AS7O37lXDbBvyZLuqYKjogIZ55+vLcfHgVgRMw/Lfj9qP/lEW2903QhpafCMMQeE2rt3h6MfZSe1JA+OeRDl6eVYNHoRYNf/lg+e/ghuG3YbHjn9EVPKLZxe8wFOmARktk8TfHNhF3l8Pn8clt8y1mQhiCcU0a8bLjWrRuRFTJ+l9FYR8+sPjG0OjGUemgObUL9V49dHjuKl3fswWc2WWhC5AjaxfRc0I9fMcdAWWTOSKMDPJJSqFpE6m/JDSxmLSDiQuM7uxhGfIkS8as789hmXAgCqbroJuFzpCxHjuxIgpFqCij31m7o7DYYLTaOEiDpGrqtD5LCSaZF+1lkQPGZh5zltjEmolDz5B72rKtEiTCifgAnlavfZQ3rwqkNKw7SiadrykIIh8EpeoC6u0WG8MOmgWKXrxtM9T/8+G90Yzv790PWtN2ErLLTarf0hCIBaun9873y8t34fvC7rDCdp0Cz0W60EQv8mfDkAs4uqPUJC5Dhoi6wZSeRRAxdsALJtHgTslQBSSIhYNbazuVBZp0SXZ7psqDWkoIo1inDJUms2celpuG8WA6LKepetfaajtTacoc9RY2I4YlkTcm0tDj3xBADAVpIo6uJTdeMLpBEtjPH7HRcPpVURZnE3mXoa93U28tIc+GDeqUhzJN7qHL16tcGMjo2cOdfi0GOPI+OCC3DBoCK4JREnFnutB0+4BzhxOlA6HP5X16HPvppG12hpK0iIHAdusfULmkkij2qmXJwKxDQE7FUAWOq4Zg5YFF+zu3C0TlEamS47Dj/1sLZJ8Afh9gvIqlEsP/biYtw45GIsWrko8TidGYPfuDH1ZMTcXAhZWYgeOaKvi++gCr27b4z4kvFEC2NovmZswGfCkQGlELzqhnB4W3hSqUVFQeqL55yrroLnlFPgqKgAx3GY0Lcg+WDJA5SNAAD8ftrA1pngcdK+7TXtnPjW3a2BXeRRDVWICE4tDS0lLCJHtwMvWwRt2dw4Whdzzdhg71pu2jzjYxnD7UopZFtuHqb2nIqbh96MN899M/5InZamBpByHAfXYHNQnJCV+NTkOe0083729hvw1iExWkRsSQKzBRvQ3ZDt0ozVYIn2AScIcPbv32F/fyREjgNjxkZEjtQzsvmQRAE1qkUkE4JWSyQlhMi3b1ivF+04WKMIkRyPZIp0B4DyfQw5VWrxofx82AQbZvaZiR6ZPRIO1Wk5hmwHsdD8VBWrpGqEt9uRM2fOMU+LOE5sDuDivwFT/1K/wOg+Vn+dliJxDwShQq6Z48CY6haVm9CS/DiwqzEiAOBisi5E4lwzoR07ENq+HZ5TTmmVeTXI0t8Bnz6UdPPBWiVGJC9NglxrFlV1Dg45u5UeGlIPEh9WOPr0bnhQHGKcBcRRYV3V0qrlOdGK9Dmv4THphkJ1JESIFIOESDMRZa0oRJhiiXFFIziSxCLy45lK1H3ZP16E66STWmVu9VKPCAGAg9VK8GlumgS5tta0zecAeuxQcuIdvVMnwKw1yTj/fESrquEamrxqZzy8xywwxFzrqpreiy+G74uV8IxNjWJXnRKj+EirJ36AINohJESaidZzzfCoRUyIRLHHrgSpJXPNBDZsaB9CJLMcOLrNchNjTLeIpDvgqz5i2u4OAK7DPkAQIPXuY3WITg8nCMj+ReMrPgIAZ0j57f7B+0nH8U4nSp568pjnRrQCRvHhaX/dVQmiPkiINBPH27WysdhFHkEoZhCXLGvBqv71G/DTxdPgPnkocq67ThvP2dtvOe65oWtR2nsoLveHEY4qMSC10d3Yu/9HGMsMddunbHMOGJBQ54I4dozfDXtZxy6A1eHJ6gqcebeSMSO0zrWIIJoLEiLHyc1Db8Ybm9/AlQOubJX3s4s8gky50DjlqNb8LbR1KwAg8M03CO3erY1vN1HWocT04n/Jo4FvgamTlCqxLs8+XPjOfDzpN1uXPGoNNFsxFS9rTtLOPBOHnnoKrqFDGx5MtH9G/rKtZ0AQxwQJkeNkZp+ZmNlnZqu9nyTyCKl/NmckioCFwaPmPwYzeysF0TZIOHmdk+qAIkScGZsRAuBQ+7d91J/D6ev1DJrGVAwlGo/gcaP7fz9oVN0RgiCIloLSd1MMu8AjBNUiEo3A14DnRfZblFRvbRgDQkoMy98j4wAAr0RO0zZX+VWLiNrUy64swhfXR5B3k1umuSERQhBEW0MWkRSD4zhEecXd4oxGcNRT/41EDliUVG9twn7Eqj7eG5mO16Jj8B0r1zbHhIjTJqKKMah9/OBzGKpFAuDdZBEhCILoaJAQSUGY2temIBxBZQPFXdtFobOQPgcfHFjHzLVAjqoN7yR7FLawvr4uztrDu8giQhAE0dEg10wKwgTFIpIbDiIvp0wLWLXi8NPPIFpT00ozS0JYESJ1TAKz+ModqFFSd+1iRIsPASyECFlECIIgOhwkRFIRUQ2eiITQN6cfKhswFPww9GQcfPJJ+L5Y2fJzs0LNmKmDdUDLfrWYmc0WwR3/0INrQ3ECi3cm6bVBEARBpCzkmklBinIygD1ANByAXbCj1tHwPoceexwA0Hvj99o6ORjE0ZdegmfMGEhduzb/RP2VwKq/aI27DrIMy2G7K5U4FpstgpJD+noWN072t4N4F4IgCKJZISGSgnQvzAH2AFw0CDtvT7AcNJZDTz+Nw089jQP33mcSKM3G8oeBzx7VFtfJ3S2H7T6qChHBnGosxmUet5uaKARBEESzQa6ZFCRNrS4qyGHYBTsCtuSZMxlTpiTdVvfVV80+NxO7V5sWN7ESy2F7KhXXjICgaf255Wdpr9MmTkT65MnNPEGCIAiirWkxIbJt2zZcfvnl6Nq1K5xOJ7p374477rgDoVCo4Z2JehFsimXAhjBsvC2pRaTgzjtRcMfCVpxZHFlmd081sw5mCUVliOlr8e1uc7+T4Tl6A7fiR34PniwiBEEQHY4Wc81s3LgRsizjmWeeQY8ePbBhwwZcccUV8Pl8ePDBB1vqbTsFvE0P2rRzPIJxQqTkmacR3rcf3qlTwInmPzFjrPWKWAXN2To+JA9mcRa9DM9h8zr3qJEAkneFJQiCIFKfFhMiEydOxMSJE7Xlbt26YdOmTXjqqadIiBwnvE2/ods5HsG4v6Jr6FBTOfTSvz2PHT+fBQBgoRA4Scle4dDCgsRfaVqsT4gAek+ZGPbiYvT46EMIGdZBrgRBEETq06oxIlVVVcjKykq6PRgMorq62vSPSES06S4KO3gYYzy7L/lvQk8W16BB2mumZp4Et/6EaFVVy03ykweArR+ZVvmYIkSenHGS5S5ZNfF5MoCtsJB6zBAEQXRgWk2IbNmyBY8//jiuuuqqpGMWL16MjIwM7V9JiXVwY2fHJgpaB14bONgMQsRucc44mw2wKePlYBCh7duxdfJkBDdvbrlJfnR3wiofHOiW68bk/oWWuxQbUndL//Z8S82MIAiCaEc0WYjMnz8fHMfV+2/jxo2mfXbv3o2JEyfioosuwhVXXJH02AsWLEBVVZX2b+fOnU3/RJ0Am8AjqHrV7HFCJBm86o5hfj9qP/usJaeXFB8ckEQBAHDX+f0SthcfUiwi2b+6Ee6TT27VuREEQRBtQ5NjRG666SbMnj273jHdunXTXu/Zswenn346Ro4ciT/+8Y/17idJEiSpgXayBGxaB14/7ABCkYb34ZwOoLYWciAAuaY2YTsLhxXLSXOgVlKNx8ccKJQUITJzeBkuPKkYvReqmTIy0HunIkQcxaXNMw+CIAii3dNkIZKbm4vcRmYx7N69G6effjoGDx6M5557DjxPZUuaA7vIIQhFNNgZUN2IEAre4UQUwE/nX2C5XQ4EIDSXEPEdsFxdBwfy0vWAVaddQFGGA47tP2Li/0Rk1SqKSsjMbJ55EARBEO2eFsua2b17N0477TSUlZXhwQcfxMGDB7VtBQUFLfW2nQKbwCPERIAD7AD+cRqPkpAbp16XGJcRg3fUb2mS/X4IaWnHPznGgLfnWW4Kwob8NHPmzKmbP8PPP3/JtE7weo9/HgRBEERK0GJCZMmSJdiyZQu2bNmC4uJi0zbGErMjiMaju2YAG2Oo9HD42/+V4ewzzki6D+es32zCmquPy9aPE7Jl7sl/GMt3BABwsAnmlOEZK19LOAQJEYIgiM5Di/lKZs+eDcaY5T/i+FCCVWOuGeV8huVwvfs01LlW9vmaZ3KHfkhYtdU1AN+xcgBArscOFtGDWoRoYoCLkOltnrkQBEEQ7R4K2khB7AaLSEyIhKIWpfNlWX/N11+87MiLLx77hDa8Duz/VnlddzhhczCizCM3TcIZL9yPH8+cADkQQOVbb1kejkq5EwRBdB5IiKQgosApMSIA7KrYCMlxQmTPGuC+MuDzJwAAHC9omwoXJcaSVL3+BiJHjzZ9Mls/Bv75C+CpkYrw8R00bX40MgU/HlCydO48pw/qPl2G8J49qPv6a+ydvyDhcOnnntv0ORAEQRApCwmRFMTomrHJShGRBIvIpw8DwWrgv7cCe9YCgv6n9k6danlcuabGcn297PxSf/365cBPn2qLK+Ve+H3kQuypUmq3O0KGOBSjtUbljhkCiu6/r+lzIAiCIFIWEiIpiNE1I0YVIXIkcATbqrbpg9yGFOsPfgOOa/hPHa06hpL6YYO4+PYN4LBerTXCBNNQya/XLwn+kFjV9YzuExPWEQRBEB0bEiIpiM1QR0QwBH6e89Y5+iBHuv5ajoJPNywn4Zh6z0SCjR7q/H699jq0fXvC9ksn3Nz09ycIgiBSGhIiKYgxfdcq6wQAEDJkwdgcyLthHuzduiH/9tuSHleuPgYh8sUfkm7ioGdIda/cDefDi7Tl8N69prHvLp4MZ3Ze09+fIAiCSGlarI4I0XKIvB6sarSImDAKEX8lbF26oPt772qrSv78Z1T961/gbDZUvfEGgEZYRBgDOEP2TfWeRs+5a9zY8B59+d4LeZxURmXdCYIgOiNkEUlBOI5DhFdSXIVQwHpQ0BB4GkgUGJ7Ro1D0wP0oWHg7OJdS7CxaVYXQjh3YNu0SVC9Zog9mDFj/T+C+cuCH/+rrN76LxpIZMAfCxiwiH5zEYXVPHl7J2+hjEQRBEB0HEiKpiqiUSs/49mXr7UaLSKAy6WF4hwNZM2YAAGo//gQ/njkB/nXrsPuX1+uDXp2pZMQEKoEvnlTWhQPAe7+qd4or5d7a66yAORCW1SmN8WrViu8iT8Y5giCIzggJkRRlvWMwAMARrLQeYBIiVYpVIwliQT4AwL92rfWA79/WX2eo5fr9yWuORBiPO8Mz8XRUD57NjFloRLPgqHUqrh4O9RdcIwiCIDomJERSlD2efpbrtRL6RiHCZKDuSNJj2eprQhiJq08SixGpx8qyiZXguegkBKFXSC30KRVXpa7lprEbypTjDSkYknwOBEEQRIeFhEiK4k7SOyYQVWNGQnHFyap3JT2WmF+PEIm3fMSEST0WkVhqcQxHJIjuVUpwatoEvVbIT/nA9nwOE8sn4oTME5LPgSAIguiwkBBJUTxOh+V6f0QtMOavVP63pyn/V+5Meix7eTn4tLSE9Yeefhrwx1lSomrdEKMQEc1ziTlZRvfIAQD0OrIdApMhFhbCPWqkNu5wmjKyh7dH0rkRBEEQHRsSIilKhtuOILMlrJ/2zjTI0YhS3h0AClQXzpKFSYuPCR43un/wPrKvvNK0/uAjjyKyZ5t5sGYRqdTXyVEEcnRXkQPK+zx56UlY/9szMd6/AwDgGjIEUrdu2ji7mnnsCzdT51+CIAgi5SAhkqJ4JBEhizIw+3z7sO3w90pcCADkqZkrR35U+s8kQczKQtoZ4xPWhzZvMq+IWrhmWBTfTnpdW0zj/HDbBaQ7bHCzCM7YtAwA4B45EkJGBqSeigVkTXfFIpIhZdT7WQmCIIiOCwmRFEUSeYQhWG6r8+1XXggSkFGib/j+3/UeU8hIFASBn3aYV8RcM8baJA4vjgb1rJd0+MCpQa2Rgwch19UBPI+Mc5UsmvKXX8aBX8/AkkHKmOm9ptc7L4IgCKLjQkIkRZFEAWHVIvKPYb8zbdtbpfZxcWQAHkPZdGODOgsErzdhnW/ND+YVMddMSG9gh5+9gip/WFtM5/xanIis1gsRsrLACYpw4t1uVI7pj5CNw6iiUXDZXPXOiyAIgui4kBBJUSQbrwmR/mmlcNvc2rYb1z2idHlxZACubH2nBoSIMWA182c/AwDUbVSCXJndAwA4WqNm48RiUMbeDpScjEqDEAGAYd2U940JEd5tFhsRWQkQsXGJcS4EQRBE54GESIoiibzWbyah1geAQwKvdODlDO4b0Z4wzghn6CMj5KhCwh+CHAV8UITEgaPVkGUGBFWLiKSIlyp/GN/KZQCAg85uuG9qf2V/nxKIyrt0oQQAYVkRLjaBhAhBEERnhoRIimJ0zWgBpAbm5uUCdjdQPkpfKUcbPK5z4EAAgPeCCwDVlRIN8ahiSt2SCm4ntvxpJhCqBWPAric/wM77HsBjSzfj/0K/wqrCS5B7xevI9kjKW/pUi4jL2iIiclTanSAIojNDQiRFcRhcM4iGE7avd0iAzaWIkas+VVYmSd81Uvb3F3DCyi9gKyzUYkaiQR6Vsl5A7YS9bwPBGvgP2VGz8nvUPvcsAGAvsrGm9y1Alp6im8w1QxYRgiAIAiAhkrLEW0Tie7UIjAE2VTzYVbdII4QIJ4pa9oyYlakcPsDjcFgyD9y5EnJEf09etbZkOM3CIplrRrOIULM7giCITg0JkRRFEnm9joiFa8bJGBALYBXU2JBow0LEiOBVhUiIRzWzLikfY/ABJbsmb8lbOPryKwAA2e/Hkb/9DYDimtlZvRPLdik1RbRgVZ4sIgRBEJ0ZehxNUSQbDz9TA1EtXDMhcLpFRFStGdGQ0oWXa1ynWyFTESKRoLUQYVH9OL/74i/Y78xEvv8o9gHIuOB8HHzkUYR3Klk3vNuNCW9OBgA8N+E5zTVDFhGCIIjODVlEUhRJFBCKNZeLhrBwxEIAwEUnXAQACPEcWKwHjGDIlrGwniRD0FwzAqrhTtguh82CJt9QbVX2+VD9wQf6sQypwRsObSCLCEEQBAGALCIpi1JZVXfNTOo6CcMLh0PgBbz2w2sAgIjNoUgVQ1M6FvYDgt2UqpuMmEUkGuJxkCVWXa2q6g1gn+W+cm0teEmPK/kk/J32+qGvH0K+Kx8AWUQIgiA6O2QRSVEcNiEhaybTkQk7r1s/QrG6IQaLyPUvrsTZjy9HJCo3+B5iTIgEeRxl5u68chTwbbQWIYAiRDiHLoDeqFlm2r6/TilDT0KEIAiic0NCJEUxBquySEBbbzeIjqCgWiR4HlBdIKu27Me3e6rx7R61MuqXfwLempNYdXXfegi8kvESCfAIwlwMLRqo/6sTra0FZ7CIVLusLTDkmiEIgujc0ONoiiKJAqqZUptDrjuqtb/jOR4igAiAkKjc5Pf59uGL9DScVXkETi4IMCAYUS0i7/1K+X/t34HbDwGCTUnz/csECDsiALIRDfEIwiwYoqH6hUjgu+8Q+OYbbXlXjvU4sogQBEF0bsgikqJINh4HmRcAINfsN22zMcX6EBaUm/yFb1+I2zM9+Is3HXeLSvGxYCSamG1Tqx4nUAWEfRAkRaxEgzx2BXOwYsiz2tBIwLrzb4wD996nvb5zOo+oYG0RqY71rCEIgiA6JSREUhS7wOMglABSVmOO1Yg5UUK8IhaqglUAgE+dTowUvoMNEfiCEUVwmFDFguqmEVUhEgza8Pt3H4f3V7fhX/JY7Ps6Azs/yTbvak/ex2Z/Jgen6MTYkrEJ27ZVb2vgkxIEQRAdGRIiKQrPc6jklGBS1B4wbdOESJLMmJ7cLhysDeHAgb3mDbHUXjXmRLArQkSQ9cBWvg44ullP5d2SWYxVeRWwX3Wd9US96TiUwcEu2NE3p2/C5pFdRlrvRxAEQXQKyEGfwlSJWQAA3hcnRBgADkhWMcSFAG5/awO4IXW41LghJkRUiwgnMsgcwDN9iBz3lfkhoxiPD7wQX84ch8OPP5zwXmz0EADLIAkSXKLeb+aBMQ/AzttxavGpjfikBEEQREeFLCIpTEhQLBNcpM603qYKBx+LoiZUo62P6QmJU2JD/rd6k/mAsV40qkWEy+pqEiEAwEfMX5kqtY+NZBNQ9MgjCXOM2hXh4hAc8Dq82voSTwnGlo6lYFWCIIhODgmRFEYWlbLrfFzq7X5VPVy1/jGc9spp2no1hhUO1Vbihc98wDiLCLNZ9JepMb9XtSpEHDYe6RMnIOeXZhdNxK7EqdgFO0Z1GaWtT7Ob65IQBEEQnZNWESLBYBADBw4Ex3FYu3Zta7xlpyAmFDgWMWXA+A2hISFZd9BERFU0QBmbwSURIqpFpDos4K6TZ5mGeEPmTJuYELELyldJzMoybd8XVcq+OwQHMh2ZuP/U+3HT4JtQml7auA9JEARBdGhaRYjcfPPN6NKlS2u8VefCEHOBcF3ycSqy+uf+g/0xiIjAi1rT9khILYymWkQ2Hg7j8y79TWMK5ahpucauzCFWMp5zmq0oyw5/CQCQ1MZ7k7pOwux+sxucK0EQBNE5aHEh8p///Af//e9/8eCDD7b0W3U6BJsdEab+CQ3umQeP+CzH/yCE8Iw3HQAwkNsCL2cWItW16rJqEQkwJf/m4UEXa2PCX+807RPlzF8hTjDXFwmqddAkQQJBEARBxNOiQmT//v244oor8MILL8DlcjU4PhgMorq62vSPSI5kE+CHeoM3WET6BwJJ9gCeyPQCAPK5SqTHuWYioSBQuQN46xoAQEBNBF5SOhTukSMsj7c1owv6Fxka4sUJk6i6SEKEIAiCsKLFhAhjDLNnz8bVV1+NIUOGNGqfxYsXIyMjQ/tXUlLSUtPrEEiioIkFo0XEHct+qYdu3J6EYNVQ0A98rFdE1Y7NcXCPGm1+b28Ys874DSodaXjq0pO09ZzNXArerU7FIThAEARBEPE0WYjMnz8fHMfV+2/jxo14/PHHUVNTgwULFjT62AsWLEBVVZX2b+fOnQ3v1InhOMCvuk8QUi0isgx3NNLgvjfZ/olxwhrTukgoAER0QcNBz93NmHKBaez6sm444FYCU4szdWuXZ8yp8JckNpYxNuMjCIIgiBhNLuJw0003Yfbs2fWO6datGz788EOsWLECkmQ2yQ8ZMgQzZszA888/n7CfJEkJ44nkfLzpIPz2ONeMHG7yH/UQS0cOVw1Wsx/Y8Lq2/oDaywYABK/XtM+88l9aHot3OLD6oZ9j058ewfCNMv53ohLE6hDJIkIQBEEk0mQhkpubi9zc3AbHPfbYY7j77ru15T179mDChAl45ZVXMGzYsKa+LWHBhYOL4V8f55qJJqunqqMWXtVYJVdgovAVuq4zBxQ/Hz1Te80ZysXbe3TH7y4YgNvf2oDrx/VMOL4/GsD7Q3gcmDwE/gOrAQA23pYwjiAIgiBarKxlaam5ToTH4wEAdO/eHcXFxS31tp2KeeN7Ytf6OIvIjx81uF+A4+BkitvlofCFKOf3JYx5NnMudu7Ntz5AJIpLh5ViTM9clGQlFj0LqFk3fbL7YLUqRIrT6G9OEARBJEKVVVMYjyTCxxSXRzSglnJ/dSYA4F+79uCR0x9B94zuCfv5eN268Xh0CkIs0Vrx7X494PXqMeZjsEgEHMehNNtlspTEiAmRdHs6bhx8I2b1mYVLKi5p4qcjCIIgOgOt1uijvLwcjLGGBxKNRhIF1ECxSET8VTBW8OgWjqC4y+l4bNWfEvar5XjkQO+oG7L4Gjg5XYj8cmwPAADvdkP2+eAafFLCeCPv/vQuACUu5LJ+lzX68xAEQRCdD+o4lsLYRR7VTCmxLtdVJWyf/qcv8H1tCaS8b8FkO1jUBd5WiWqBByKArDaf2cuyE/ZdIffRXjtsisQp/+drqPr3v5E9a1bC+BjLdi2DL6ykBctMTjqOIAiCIAASIimNwHPwcUrqrOyvTNj+9fajAE4FkyXIoUxIuUsBWyUOCQK22kRIsgNSlEdeWV9gt77fKvkE/MiKTO8DAFLXrsibO7feOS3ZvkR7TZkyBEEQRENQjEiK4+cViwgLJKtCyyN8dASivl5w8EoF1HVdh+O84i44uywHaxeeCXdhhWmPH+Vj7wtUG1LKxIuciCk9pxzzcQiCIIjOAQmRFCcgKNlI4qHvgENb6h1rgyJE/icoHXQjTIbDxkNIN6djGwuZje6RWJysPmrDihC5a/RdcIqJGTUEQRAEYYSESIoTEhSLiOPQBuCJwdr6hUV/ThgbEyI7anZo63xhH1xpmaZxRiFyzWmJWTf1EYsPcYvuJu1HEARBdE5IiKQ4dUKG5fova/MS1s048bSEdUcDR+FxmUWDMSNXEpv2FYkJEY/d06T9CIIgiM4JCZEUZ4M00HL9xv21puUuGQ5cM2xSwri9vr3gxbqE9TEkUUi6zYg/4serm17F1qqtAACXreFuywRBEARBQiTFsdls+DA60HLbqB56Wm5OmnUPn8v/ezmuWX4OjvL6V8HompFsjfuKvPj9i7jri7u0ZY+NLCIEQRBEw5AQSXHW7apCHaxFRs+8NDzxs0EozHDgtrP6WI6JsV7Su+O+Hx2qvW6sa2b9wfWmZbeNYkQIgiCIhiEhkuJ0z3UjCLvltuJMJ84e0AUrFozDyV2zAAAlaSWWY52GqrdLZD3otbGume5ec1ArCRGCIAiiMZAQSXHuv3AAeFhXMC3NSozTeOT0RyzHRmMvCvrj7AF6HZHGWkR4zjyOUncJgiCIxkBCJMXJS3PApssIE6XZiULkhMwTsH7Wetw4+EbT+oBbrSXScwKuU3vLAEoZ+cYQioa01xWZFfWMJAiCIAgdKvGe4rglETZETOtqmGKNKMlMnrmS7TT3lwmMuw0IRoF+F6KnIGFQqRc2gYfL3jjXTEjWhcgT455o7PQJgiCITg4JkRTHLQkJQuTnofnqtuR/3hyHuWLq7mgdMOhyAIAA4I1rRgIAOGNRkXoIRpVuvXMGzkGBu6BR+xAEQRAEuWZSHEkUIMa5Ztawnpg3vme9+/XP7W9afmT1Iyb3CsdxjRYhgO6asQvWgbMEQRAEYQUJkQ7A36JnJqy7ekz9pdnT7Gk4r/t5pnVHA0ePeQ41oRoAgCRYpxITBEEQhBUkRDoA/5MHw890S0SvgjQ4bA3Hdtw16i70z9EtIzH3SlP5Yu8X+GjnRwAAG287pmMQBEEQnRMSIh2ESaHF+CQ6ABcFF+LMvo2L0eA4TuuWCwB1keSl3uvjiv9eob0miwhBEATRFEiIdABevWoEtrFCzArPx1esF7zOxlslakMGIRI+NiFihGJECIIgiKZAQqQDcHLXLPx8RJm27KknWyaeMSVjtNf+iP+Y3p+DHtRKQoQgCIJoCiREOgjGVF2Po/FC5FdDfqW9PlbXjEN0aK+ZoVQ8QRAEQTQECZEOgtEK0hSLiNvmxqguowAA//npPwnb99buxbJdyxotMI5VzBAEQRCdExIiHYSCdN0q0RSLCADsqt0FAFiyfQnCcti0bfKbkzFn6Rws27XMcl/GGAKRgLZsdNMQBEEQREOQEOkgFGfqTeaaYhEBgO3V27XXu2p2mbZFZKVq64q9Kyz3DUQDYFCsJQNyB+DM8sSaJgRBEASRDBIiHYQigxBpbH+YGIPyBmmvt1ZutRyTzNKxo3qH9vqFSS9Q+i5BEATRJEiIdBCMrplsd9PEwKLRi7TX8z6eh6U7lgIwd9S1ojJQiQvfvlBb5jn6OhEEQRBNg+4cHQRR4PH5/LFY9uvT4WyiRaQkrQQXn3Cxtjzvo3l4c/ObGPz3waZxNaEa3PflfXh367uQmYxTXjmlWeZOEARBdF6o+24HoovX2fCgJKTZ00zLCz9faFr2hX1YvHIx3t76NtLsaRhdNPqY34sgCIIgYpAQIQAAHrun3u3VoWr8WPkjAMUy8lPVT6btvxv5uxabG0EQBNFxISFCAADS7en1bt9Tu8eUUbPu4Drt9Tc//wYcR2m7BEEQRNMhIUIASHTNxPP9ke9NyzEhUppWSiKEIAiCOGYoWJUAAHhs9btm4lmyfQkApTIrQRAEQRwrJEQIAECGlNGocScXnGxabii2hCAIgiDqg4QIAQDo4e3RqHFTek6ByOsePbKIEARBEMcDCRECAOCyuSzXZ0gZ+Oc5/9SWy9LL8Mcz/qgtN9WlQxAEQRBGWlSIvPvuuxg2bBicTicyMzNx/vnnt+TbEcfJa+e8Bqeo1yJ5evzTWH7JcnTzdtPWZTmy4JW82nJjXToEQRAEYUWLZc28/vrruOKKK3DPPfdg7NixiEQi2LBhQ0u9HdEM9Mrqhcv7XY4n1j4BQGliBwA23oZ7Rt+D2nAtuni64JD/kLZPkaeoTeZKEARBdAxaRIhEIhHMnTsXDzzwAC6//HJtfZ8+fVri7Yhm5OzuZ2P57uWY0XuGKaX3nO7naK+NVhCyiBAEQRDHQ4u4ZlavXo3du3eD53kMGjQIhYWFmDRpEllEUoAiTxFemPwCJnadmHSMjbdprysyK1pjWgRBEEQHpUUsIlu3Kq3kf/vb3+Lhhx9GeXk5HnroIZx22mn44YcfkJWVZblfMBhEMBjUlqurq1tiekQz8PzE53Gg7gAqskiIEARBEMdOkywi8+fPB8dx9f7buHEjZFkGANx6662YOnUqBg8ejOeeew4cx+G1115LevzFixcjIyND+1dSUnJ8n45oMU7KP6leqwlBEARBNIYmWURuuukmzJ49u94x3bp1w969ewGYY0IkSUK3bt2wY8eOpPsuWLAAN954o7ZcXV1NYoQgCIIgOjBNEiK5ubnIzc1tcNzgwYMhSRI2bdqE0aOVdvHhcBjbtm1DWVlZ0v0kSYIkSU2ZEkEQBEEQKUyLxIikp6fj6quvxh133IGSkhKUlZXhgQceAABcdNFFLfGWBEEQBEGkIC1WR+SBBx6AKIqYOXMm/H4/hg0bhg8//BCZmZkt9ZYEQRAEQaQYHGOMtfUkklFdXY2MjAxUVVUhPT29radDEEQz4QMQaw5QC4A6FhFEx6Ip92/qNUMQBEEQRJtBQoQgCIIgiDaDhAhBEARBEG0GCRGCIAiCINoMEiIEQRAEQbQZJEQIgiAIgmgzSIgQBEEQBNFmkBAhCIIgCKLNaLHKqs1BrNZadXV1G8+EIIjmxGd4XQ0g2lYTIQiiRYjdtxtTM7VdC5GamhoAoA68BNGB6dLWEyAIosWoqalBRkZGvWPadYl3WZaxZ88epKWlgeO4Zj12dXU1SkpKsHPnTiof34LQeW4d6Dy3HnSuWwc6z61DS51nxhhqamrQpUsX8Hz9USDt2iLC8zyKi4tb9D3S09PpS94K0HluHeg8tx50rlsHOs+tQ0uc54YsITEoWJUgCIIgiDaDhAhBEARBEG1GpxUikiThjjvugCRJbT2VDg2d59aBznPrQee6daDz3Dq0h/PcroNVCYIgCILo2HRaiwhBEARBEG0PCRGCIAiCINoMEiIEQRAEQbQZJEQIgiAIgmgzOqUQ+cMf/oDy8nI4HA4MGzYMX375ZVtPKaVYvHgxhg4dirS0NOTl5eH888/Hpk2bTGMCgQDmzJmD7OxseDweTJ06Ffv37zeN2bFjB8466yy4XC7k5eXh17/+NSKRSGt+lJTi3nvvBcdxmDdvnraOznPzsHv3blx66aXIzs6G0+lE//79sWrVKm07YwwLFy5EYWEhnE4nxo8fj82bN5uOceTIEcyYMQPp6enwer24/PLLUVtb29ofpV0TjUZx++23o2vXrnA6nejevTvuuusuUz8SOtdNZ9myZTjnnHPQpUsXcByHt956y7S9uc7pN998g1NOOQUOhwMlJSW4//77m+cDsE7Gyy+/zOx2O3v22WfZt99+y6644grm9XrZ/v3723pqKcOECRPYc889xzZs2MDWrl3LJk+ezEpLS1ltba025uqrr2YlJSVs6dKlbNWqVWz48OFs5MiR2vZIJML69evHxo8fz9asWcPee+89lpOTwxYsWNAWH6nd8+WXX7Ly8nI2YMAANnfuXG09nefj58iRI6ysrIzNnj2brVy5km3dupV98MEHbMuWLdqYe++9l2VkZLC33nqLrVu3jp177rmsa9euzO/3a2MmTpzITjzxRPbFF1+wTz/9lPXo0YNNnz69LT5Su2XRokUsOzubvfPOO+ynn35ir732GvN4POzRRx/VxtC5bjrvvfceu/XWW9kbb7zBALA333zTtL05zmlVVRXLz89nM2bMYBs2bGAvvfQSczqd7Jlnnjnu+Xc6IXLyySezOXPmaMvRaJR16dKFLV68uA1nldocOHCAAWCffPIJY4yxyspKZrPZ2GuvvaaN+f777xkAtmLFCsaY8sPheZ7t27dPG/PUU0+x9PR0FgwGW/cDtHNqampYz5492ZIlS9iYMWM0IULnuXm45ZZb2OjRo5Nul2WZFRQUsAceeEBbV1lZySRJYi+99BJjjLHvvvuOAWBfffWVNuY///kP4ziO7d69u+Umn2KcddZZ7Be/+IVp3ZQpU9iMGTMYY3Sum4N4IdJc5/TJJ59kmZmZpuvGLbfcwioqKo57zp3KNRMKhfD1119j/Pjx2jqe5zF+/HisWLGiDWeW2lRVVQEAsrKyAABff/01wuGw6Tz36tULpaWl2nlesWIF+vfvj/z8fG3MhAkTUF1djW+//bYVZ9/+mTNnDs466yzT+QToPDcX//73vzFkyBBcdNFFyMvLw6BBg/CnP/1J2/7TTz9h3759pvOckZGBYcOGmc6z1+vFkCFDtDHjx48Hz/NYuXJl632Yds7IkSOxdOlS/PDDDwCAdevWYfny5Zg0aRIAOtctQXOd0xUrVuDUU0+F3W7XxkyYMAGbNm3C0aNHj2uO7brpXXNz6NAhRKNR00UZAPLz87Fx48Y2mlVqI8sy5s2bh1GjRqFfv34AgH379sFut8Pr9ZrG5ufnY9++fdoYq79DbBuh8PLLL2P16tX46quvErbReW4etm7diqeeego33ngjfvOb3+Crr77C9ddfD7vdjlmzZmnnyeo8Gs9zXl6eabsoisjKyqLzbGD+/Pmorq5Gr169IAgCotEoFi1ahBkzZgAAnesWoLnO6b59+9C1a9eEY8S2ZWZmHvMcO5UQIZqfOXPmYMOGDVi+fHlbT6XDsXPnTsydOxdLliyBw+Fo6+l0WGRZxpAhQ3DPPfcAAAYNGoQNGzbg6aefxqxZs9p4dh2LV199FS+++CL+8Y9/oG/fvli7di3mzZuHLl260LnuxHQq10xOTg4EQUjIKti/fz8KCgraaFapy3XXXYd33nkHH330EYqLi7X1BQUFCIVCqKysNI03nueCggLLv0NsG6G4Xg4cOICTTjoJoihCFEV88skneOyxxyCKIvLz8+k8NwOFhYXo06ePaV3v3r2xY8cOAPp5qu+6UVBQgAMHDpi2RyIRHDlyhM6zgV//+teYP38+LrnkEvTv3x8zZ87EDTfcgMWLFwOgc90SNNc5bclrSacSIna7HYMHD8bSpUu1dbIsY+nSpRgxYkQbziy1YIzhuuuuw5tvvokPP/wwwVw3ePBg2Gw203netGkTduzYoZ3nESNGYP369aYv/5IlS5Cenp5wU+isjBs3DuvXr8fatWu1f0OGDMGMGTO013Sej59Ro0YlpJ//8MMPKCsrAwB07doVBQUFpvNcXV2NlStXms5zZWUlvv76a23Mhx9+CFmWMWzYsFb4FKlBXV0deN582xEEAbIsA6Bz3RI01zkdMWIEli1bhnA4rI1ZsmQJKioqjsstA6Bzpu9KksT++te/su+++45deeWVzOv1mrIKiPq55pprWEZGBvv444/Z3r17tX91dXXamKuvvpqVlpayDz/8kK1atYqNGDGCjRgxQtseSys988wz2dq1a9n777/PcnNzKa20AYxZM4zReW4OvvzySyaKIlu0aBHbvHkze/HFF5nL5WJ///vftTH33nsv83q97F//+hf75ptv2HnnnWeZ/jho0CC2cuVKtnz5ctazZ89OnVJqxaxZs1hRUZGWvvvGG2+wnJwcdvPNN2tj6Fw3nZqaGrZmzRq2Zs0aBoA9/PDDbM2aNWz79u2MseY5p5WVlSw/P5/NnDmTbdiwgb388svM5XJR+u6x8vjjj7PS0lJmt9vZySefzL744ou2nlJKAcDy33PPPaeN8fv97Nprr2WZmZnM5XKxCy64gO3du9d0nG3btrFJkyYxp9PJcnJy2E033cTC4XArf5rUIl6I0HluHt5++23Wr18/JkkS69WrF/vjH/9o2i7LMrv99ttZfn4+kySJjRs3jm3atMk05vDhw2z69OnM4/Gw9PR0dtlll7GamprW/BjtnurqajZ37lxWWlrKHA4H69atG7v11ltNKaF0rpvORx99ZHlNnjVrFmOs+c7punXr2OjRo5kkSayoqIjde++9zTJ/jjFDSTuCIAiCIIhWpFPFiBAEQRAE0b4gIUIQBEEQRJtBQoQgCIIgiDaDhAhBEARBEG0GCRGCIAiCINoMEiIEQRAEQbQZJEQIgiAIgmgzSIgQBEEQBNFmkBAhCIIgCKLNICFCEARBEESbQUKEIAiCIIg2g4QIQRAEQRBtxv8DrTgP4ZQsCqgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Loading and visualizing the data\n",
    "\n",
    "## Loading the dataset\n",
    "\n",
    "\n",
    "X_test = np.load(\"X_test.npy\")\n",
    "y_test = np.load(\"y_test.npy\")\n",
    "person_train_valid = np.load(\"person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"y_train_valid.npy\")\n",
    "person_test = np.load(\"person_test.npy\")\n",
    "\n",
    "## Adjusting the labels so that \n",
    "\n",
    "# Cue onset left - 0\n",
    "# Cue onset right - 1\n",
    "# Cue onset foot - 2\n",
    "# Cue onset tongue - 3\n",
    "\n",
    "y_train_valid -= 769\n",
    "y_test -= 769\n",
    "\n",
    "## Visualizing the data\n",
    "\n",
    "ch_data = X_train_valid[:,8,:] # extracts the 9th channel from the data\n",
    "\n",
    "\n",
    "class_0_ind = np.where(y_train_valid == 0) # finds the indices where the label is 0\n",
    "ch_data_class_0 = ch_data[class_0_ind] # finds the data where label is 0\n",
    "avg_ch_data_class_0 = np.mean(ch_data_class_0,axis=0) # finds the average representation of the 9th channel when label is 0\n",
    "\n",
    "\n",
    "class_1_ind = np.where(y_train_valid == 1)\n",
    "ch_data_class_1 = ch_data[class_1_ind]\n",
    "avg_ch_data_class_1 = np.mean(ch_data_class_1,axis=0)\n",
    "\n",
    "class_2_ind = np.where(y_train_valid == 2)\n",
    "ch_data_class_2 = ch_data[class_2_ind]\n",
    "avg_ch_data_class_2 = np.mean(ch_data_class_2,axis=0)\n",
    "\n",
    "class_3_ind = np.where(y_train_valid == 3)\n",
    "ch_data_class_3 = ch_data[class_3_ind]\n",
    "avg_ch_data_class_3 = np.mean(ch_data_class_3,axis=0)\n",
    "\n",
    "\n",
    "plt.plot(np.arange(1000),avg_ch_data_class_0)\n",
    "plt.plot(np.arange(1000),avg_ch_data_class_1)\n",
    "plt.plot(np.arange(1000),avg_ch_data_class_2)\n",
    "plt.plot(np.arange(1000),avg_ch_data_class_3)\n",
    "plt.axvline(x=500, label='line at t=500',c='cyan')\n",
    "\n",
    "plt.legend([\"Cue Onset left\", \"Cue Onset right\", \"Cue onset foot\", \"Cue onset tongue\"])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X after trimming: (1740, 22, 500)\n",
      "Shape of X after maxpooling: (1740, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (3480, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (6960, 22, 250)\n",
      "Shape of X after trimming: (375, 22, 500)\n",
      "Shape of X after maxpooling: (375, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (750, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (1500, 22, 250)\n",
      "Shape of X after trimming: (443, 22, 500)\n",
      "Shape of X after maxpooling: (443, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (886, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (1772, 22, 250)\n",
      "Shape of testing set: (1772, 22, 250)\n",
      "Shape of testing labels: (1772,)\n",
      "Shape of training set: (6960, 22, 250)\n",
      "Shape of validation set: (1500, 22, 250)\n",
      "Shape of training labels: (6960,)\n",
      "Shape of validation labels: (1500,)\n",
      "Shape of training labels after categorical conversion: (6960, 4)\n",
      "Shape of validation labels after categorical conversion: (1500, 4)\n",
      "Shape of test labels after categorical conversion: (1772, 4)\n",
      "Shape of training set after adding width info: (6960, 22, 250, 1)\n",
      "Shape of validation set after adding width info: (1500, 22, 250, 1)\n",
      "Shape of test set after adding width info: (1772, 22, 250, 1)\n",
      "Shape of training set after dimension reshaping: (6960, 250, 1, 22)\n",
      "Shape of validation set after dimension reshaping: (1500, 250, 1, 22)\n",
      "Shape of test set after dimension reshaping: (1772, 250, 1, 22)\n"
     ]
    }
   ],
   "source": [
    "## Random splitting and reshaping the data\n",
    "# First generating the training and validation indices using random splitting\n",
    "\n",
    "ind_valid = np.random.choice(2115, 375, replace=False)\n",
    "ind_train = np.array(list(set(range(2115)).difference(set(ind_valid))))\n",
    "\n",
    "# Creating the training and validation sets using the generated indices\n",
    "(X_train, X_valid) = X_train_valid[ind_train], X_train_valid[ind_valid] \n",
    "(y_train, y_valid) = y_train_valid[ind_train], y_train_valid[ind_valid]\n",
    "\n",
    "\n",
    "## Preprocessing the dataset\n",
    "x_train,y_train = data_prep(X_train,y_train,2,2,True)\n",
    "x_valid,y_valid = data_prep(X_valid,y_valid,2,2,True)\n",
    "X_test_prep,y_test_prep = data_prep(X_test,y_test,2,2,True)\n",
    "\n",
    "\n",
    "print('Shape of testing set:',X_test_prep.shape)\n",
    "print('Shape of testing labels:',y_test_prep.shape)\n",
    "\n",
    "print('Shape of training set:',x_train.shape)\n",
    "print('Shape of validation set:',x_valid.shape)\n",
    "print('Shape of training labels:',y_train.shape)\n",
    "print('Shape of validation labels:',y_valid.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Converting the labels to categorical variables for multiclass classification\n",
    "y_train = to_categorical(y_train, 4)\n",
    "y_valid = to_categorical(y_valid, 4)\n",
    "y_test = to_categorical(y_test_prep, 4)\n",
    "print('Shape of training labels after categorical conversion:',y_train.shape)\n",
    "print('Shape of validation labels after categorical conversion:',y_valid.shape)\n",
    "print('Shape of test labels after categorical conversion:',y_test.shape)\n",
    "\n",
    "# Adding width of the segment to be 1\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)\n",
    "x_test = X_test_prep.reshape(X_test_prep.shape[0], X_test_prep.shape[1], X_test_prep.shape[2], 1)\n",
    "print('Shape of training set after adding width info:',x_train.shape)\n",
    "print('Shape of validation set after adding width info:',x_valid.shape)\n",
    "print('Shape of test set after adding width info:',x_test.shape)\n",
    "\n",
    "\n",
    "# Reshaping the training and validation dataset\n",
    "x_train = np.swapaxes(x_train, 1,3)\n",
    "x_train = np.swapaxes(x_train, 1,2)\n",
    "x_valid = np.swapaxes(x_valid, 1,3)\n",
    "x_valid = np.swapaxes(x_valid, 1,2)\n",
    "x_test = np.swapaxes(x_test, 1,3)\n",
    "x_test = np.swapaxes(x_test, 1,2)\n",
    "print('Shape of training set after dimension reshaping:',x_train.shape)\n",
    "print('Shape of validation set after dimension reshaping:',x_valid.shape)\n",
    "print('Shape of test set after dimension reshaping:',x_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_25 (Conv2D)          (None, 250, 1, 25)        5525      \n",
      "                                                                 \n",
      " max_pooling2d_24 (MaxPoolin  (None, 84, 1, 25)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 84, 1, 25)        100       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 84, 1, 25)         0         \n",
      "                                                                 \n",
      " conv2d_26 (Conv2D)          (None, 84, 1, 50)         12550     \n",
      "                                                                 \n",
      " max_pooling2d_25 (MaxPoolin  (None, 28, 1, 50)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_25 (Bat  (None, 28, 1, 50)        200       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 28, 1, 50)         0         \n",
      "                                                                 \n",
      " conv2d_27 (Conv2D)          (None, 28, 1, 100)        50100     \n",
      "                                                                 \n",
      " max_pooling2d_26 (MaxPoolin  (None, 10, 1, 100)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_26 (Bat  (None, 10, 1, 100)       400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 10, 1, 100)        0         \n",
      "                                                                 \n",
      " conv2d_28 (Conv2D)          (None, 10, 1, 200)        200200    \n",
      "                                                                 \n",
      " max_pooling2d_27 (MaxPoolin  (None, 4, 1, 200)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_27 (Bat  (None, 4, 1, 200)        800       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 4, 1, 200)         0         \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 800)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 4)                 3204      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 273,079\n",
      "Trainable params: 272,329\n",
      "Non-trainable params: 750\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Building the CNN model using sequential class\n",
    "basic_cnn_model = Sequential()\n",
    "\n",
    "# Conv. block 1\n",
    "basic_cnn_model.add(Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu', input_shape=(250,1,22)))\n",
    "basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same')) # Read the keras documentation\n",
    "basic_cnn_model.add(BatchNormalization())\n",
    "basic_cnn_model.add(Dropout(0.5))\n",
    "\n",
    "# Conv. block 2\n",
    "basic_cnn_model.add(Conv2D(filters=50, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "basic_cnn_model.add(BatchNormalization())\n",
    "basic_cnn_model.add(Dropout(0.5))\n",
    "\n",
    "# Conv. block 3\n",
    "basic_cnn_model.add(Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "basic_cnn_model.add(BatchNormalization())\n",
    "basic_cnn_model.add(Dropout(0.5))\n",
    "\n",
    "# Conv. block 4\n",
    "basic_cnn_model.add(Conv2D(filters=200, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "basic_cnn_model.add(BatchNormalization())\n",
    "basic_cnn_model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer with Softmax activation\n",
    "basic_cnn_model.add(Flatten()) # Flattens the input\n",
    "basic_cnn_model.add(Dense(4, activation='softmax')) # Output FC layer with softmax activation\n",
    "\n",
    "\n",
    "# Printing the model summary\n",
    "basic_cnn_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karimsaraipour/opt/anaconda3/envs/ee147/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "learning_rate = 1e-3\n",
    "epochs = 50\n",
    "cnn_optimizer = keras.optimizers.Adam(lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "109/109 [==============================] - 5s 37ms/step - loss: 1.9355 - accuracy: 0.3345 - val_loss: 1.3113 - val_accuracy: 0.4260\n",
      "Epoch 2/50\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 1.5008 - accuracy: 0.3888 - val_loss: 1.1781 - val_accuracy: 0.4873\n",
      "Epoch 3/50\n",
      "109/109 [==============================] - 4s 36ms/step - loss: 1.3097 - accuracy: 0.4484 - val_loss: 1.1315 - val_accuracy: 0.5087\n",
      "Epoch 4/50\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 1.1879 - accuracy: 0.5052 - val_loss: 1.1120 - val_accuracy: 0.5240\n",
      "Epoch 5/50\n",
      "109/109 [==============================] - 4s 32ms/step - loss: 1.1011 - accuracy: 0.5458 - val_loss: 1.0589 - val_accuracy: 0.5613\n",
      "Epoch 6/50\n",
      "109/109 [==============================] - 4s 36ms/step - loss: 1.0233 - accuracy: 0.5764 - val_loss: 1.0651 - val_accuracy: 0.5460\n",
      "Epoch 7/50\n",
      "109/109 [==============================] - 6s 49ms/step - loss: 0.9778 - accuracy: 0.5957 - val_loss: 0.9786 - val_accuracy: 0.6207\n",
      "Epoch 8/50\n",
      "109/109 [==============================] - 4s 37ms/step - loss: 0.9270 - accuracy: 0.6257 - val_loss: 0.9681 - val_accuracy: 0.6007\n",
      "Epoch 9/50\n",
      "109/109 [==============================] - 4s 33ms/step - loss: 0.8890 - accuracy: 0.6424 - val_loss: 0.9942 - val_accuracy: 0.5707\n",
      "Epoch 10/50\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 0.8410 - accuracy: 0.6634 - val_loss: 0.9531 - val_accuracy: 0.6060\n",
      "Epoch 11/50\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 0.8039 - accuracy: 0.6769 - val_loss: 1.0240 - val_accuracy: 0.5993\n",
      "Epoch 12/50\n",
      "109/109 [==============================] - 4s 34ms/step - loss: 0.7719 - accuracy: 0.6955 - val_loss: 0.9344 - val_accuracy: 0.6147\n",
      "Epoch 13/50\n",
      "109/109 [==============================] - 4s 33ms/step - loss: 0.7456 - accuracy: 0.7046 - val_loss: 0.9770 - val_accuracy: 0.6013\n",
      "Epoch 14/50\n",
      "109/109 [==============================] - 3s 31ms/step - loss: 0.7281 - accuracy: 0.7171 - val_loss: 1.0032 - val_accuracy: 0.5813\n",
      "Epoch 15/50\n",
      "109/109 [==============================] - 4s 34ms/step - loss: 0.7080 - accuracy: 0.7251 - val_loss: 1.1018 - val_accuracy: 0.6033\n",
      "Epoch 16/50\n",
      "109/109 [==============================] - 4s 32ms/step - loss: 0.6760 - accuracy: 0.7343 - val_loss: 0.9028 - val_accuracy: 0.6393\n",
      "Epoch 17/50\n",
      "109/109 [==============================] - 5s 47ms/step - loss: 0.6408 - accuracy: 0.7514 - val_loss: 1.0225 - val_accuracy: 0.6153\n",
      "Epoch 18/50\n",
      "109/109 [==============================] - 4s 37ms/step - loss: 0.6267 - accuracy: 0.7506 - val_loss: 0.9509 - val_accuracy: 0.6293\n",
      "Epoch 19/50\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 0.6104 - accuracy: 0.7598 - val_loss: 1.0215 - val_accuracy: 0.6007\n",
      "Epoch 20/50\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 0.5761 - accuracy: 0.7753 - val_loss: 0.9551 - val_accuracy: 0.6300\n",
      "Epoch 21/50\n",
      "109/109 [==============================] - 4s 32ms/step - loss: 0.5641 - accuracy: 0.7894 - val_loss: 0.9855 - val_accuracy: 0.6413\n",
      "Epoch 22/50\n",
      "109/109 [==============================] - 3s 32ms/step - loss: 0.5599 - accuracy: 0.7770 - val_loss: 0.9649 - val_accuracy: 0.6487\n",
      "Epoch 23/50\n",
      "109/109 [==============================] - 4s 37ms/step - loss: 0.5300 - accuracy: 0.7922 - val_loss: 0.9072 - val_accuracy: 0.6693\n",
      "Epoch 24/50\n",
      "109/109 [==============================] - 4s 37ms/step - loss: 0.5019 - accuracy: 0.8049 - val_loss: 0.9683 - val_accuracy: 0.6507\n",
      "Epoch 25/50\n",
      "109/109 [==============================] - 4s 35ms/step - loss: 0.5066 - accuracy: 0.8024 - val_loss: 0.9694 - val_accuracy: 0.6540\n",
      "Epoch 26/50\n",
      "109/109 [==============================] - 4s 36ms/step - loss: 0.4987 - accuracy: 0.8068 - val_loss: 0.9603 - val_accuracy: 0.6587\n",
      "Epoch 27/50\n",
      "109/109 [==============================] - 4s 35ms/step - loss: 0.4819 - accuracy: 0.8114 - val_loss: 0.9532 - val_accuracy: 0.6613\n",
      "Epoch 28/50\n",
      "109/109 [==============================] - 4s 34ms/step - loss: 0.4609 - accuracy: 0.8204 - val_loss: 0.9882 - val_accuracy: 0.6653\n",
      "Epoch 29/50\n",
      "109/109 [==============================] - 4s 33ms/step - loss: 0.4586 - accuracy: 0.8172 - val_loss: 0.9940 - val_accuracy: 0.6507\n",
      "Epoch 30/50\n",
      "109/109 [==============================] - 4s 34ms/step - loss: 0.4266 - accuracy: 0.8397 - val_loss: 0.9880 - val_accuracy: 0.6587\n",
      "Epoch 31/50\n",
      "109/109 [==============================] - 4s 34ms/step - loss: 0.4288 - accuracy: 0.8336 - val_loss: 1.0618 - val_accuracy: 0.6393\n",
      "Epoch 32/50\n",
      "109/109 [==============================] - 4s 36ms/step - loss: 0.4134 - accuracy: 0.8407 - val_loss: 0.9987 - val_accuracy: 0.6640\n",
      "Epoch 33/50\n",
      "109/109 [==============================] - 4s 36ms/step - loss: 0.4090 - accuracy: 0.8457 - val_loss: 1.0302 - val_accuracy: 0.6567\n",
      "Epoch 34/50\n",
      "109/109 [==============================] - 4s 34ms/step - loss: 0.4016 - accuracy: 0.8463 - val_loss: 0.9653 - val_accuracy: 0.6913\n",
      "Epoch 35/50\n",
      "109/109 [==============================] - 4s 34ms/step - loss: 0.3862 - accuracy: 0.8534 - val_loss: 0.9508 - val_accuracy: 0.6700\n",
      "Epoch 36/50\n",
      "109/109 [==============================] - 4s 33ms/step - loss: 0.3670 - accuracy: 0.8572 - val_loss: 0.9881 - val_accuracy: 0.6620\n",
      "Epoch 37/50\n",
      "109/109 [==============================] - 4s 37ms/step - loss: 0.3778 - accuracy: 0.8532 - val_loss: 1.0114 - val_accuracy: 0.6620\n",
      "Epoch 38/50\n",
      "109/109 [==============================] - 4s 37ms/step - loss: 0.3731 - accuracy: 0.8579 - val_loss: 1.0402 - val_accuracy: 0.6693\n",
      "Epoch 39/50\n",
      "109/109 [==============================] - 4s 37ms/step - loss: 0.3651 - accuracy: 0.8598 - val_loss: 1.0131 - val_accuracy: 0.6633\n",
      "Epoch 40/50\n",
      "109/109 [==============================] - 4s 36ms/step - loss: 0.3548 - accuracy: 0.8648 - val_loss: 1.0044 - val_accuracy: 0.6660\n",
      "Epoch 41/50\n",
      "109/109 [==============================] - 4s 35ms/step - loss: 0.3460 - accuracy: 0.8697 - val_loss: 1.0168 - val_accuracy: 0.6707\n",
      "Epoch 42/50\n",
      "109/109 [==============================] - 4s 35ms/step - loss: 0.3532 - accuracy: 0.8662 - val_loss: 1.0051 - val_accuracy: 0.6680\n",
      "Epoch 43/50\n",
      "109/109 [==============================] - 5s 42ms/step - loss: 0.3292 - accuracy: 0.8731 - val_loss: 1.0193 - val_accuracy: 0.6800\n",
      "Epoch 44/50\n",
      "109/109 [==============================] - 4s 36ms/step - loss: 0.3186 - accuracy: 0.8773 - val_loss: 0.9889 - val_accuracy: 0.6707\n",
      "Epoch 45/50\n",
      "109/109 [==============================] - 5s 41ms/step - loss: 0.3214 - accuracy: 0.8753 - val_loss: 1.0248 - val_accuracy: 0.6753\n",
      "Epoch 46/50\n",
      "109/109 [==============================] - 4s 35ms/step - loss: 0.3024 - accuracy: 0.8830 - val_loss: 1.0229 - val_accuracy: 0.6833\n",
      "Epoch 47/50\n",
      "109/109 [==============================] - 4s 34ms/step - loss: 0.3115 - accuracy: 0.8858 - val_loss: 1.0591 - val_accuracy: 0.6673\n",
      "Epoch 48/50\n",
      "109/109 [==============================] - 4s 34ms/step - loss: 0.3087 - accuracy: 0.8807 - val_loss: 1.1036 - val_accuracy: 0.6793\n",
      "Epoch 49/50\n",
      "109/109 [==============================] - 4s 34ms/step - loss: 0.2988 - accuracy: 0.8881 - val_loss: 1.0653 - val_accuracy: 0.6693\n",
      "Epoch 50/50\n",
      "109/109 [==============================] - 4s 35ms/step - loss: 0.2858 - accuracy: 0.8925 - val_loss: 1.1572 - val_accuracy: 0.6460\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model\n",
    "basic_cnn_model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=cnn_optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Training and validating the model\n",
    "basic_cnn_model_results = basic_cnn_model.fit(x_train,\n",
    "             y_train,\n",
    "             batch_size=64,\n",
    "             epochs=epochs,\n",
    "             validation_data=(x_valid, y_valid), verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of the basic CNN model: 0.697516918182373\n"
     ]
    }
   ],
   "source": [
    "cnn_score = basic_cnn_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test accuracy of the basic CNN model:',cnn_score[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X after trimming: (1740, 22, 500)\n",
      "Shape of X after maxpooling: (1740, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (3480, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (6960, 22, 250)\n",
      "Shape of X after trimming: (375, 22, 500)\n",
      "Shape of X after maxpooling: (375, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (750, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (1500, 22, 250)\n",
      "Shape of X after trimming: (443, 22, 500)\n",
      "Shape of X after maxpooling: (443, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (886, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (1772, 22, 250)\n",
      "Shape of training set: (6960, 22, 250)\n",
      "Shape of validation set: (1500, 22, 250)\n",
      "Shape of training labels: (6960,)\n",
      "Shape of validation labels: (1500,)\n",
      "Shape of testing set: (1772, 22, 250)\n",
      "Shape of testing labels: (1772,)\n",
      "Shape of training labels after categorical conversion: (6960, 4)\n",
      "Shape of validation labels after categorical conversion: (1500, 4)\n",
      "Shape of test labels after categorical conversion: (1772, 4)\n",
      "Shape of training set after adding width info: (6960, 22, 250, 1)\n",
      "Shape of validation set after adding width info: (1500, 22, 250, 1)\n",
      "Shape of test set after adding width info: (1772, 22, 250, 1)\n",
      "Shape of training set after dimension reshaping: (6960, 250, 1, 22)\n",
      "Shape of validation set after dimension reshaping: (1500, 250, 1, 22)\n",
      "Shape of test set after dimension reshaping: (1772, 250, 1, 22)\n"
     ]
    }
   ],
   "source": [
    "## Loading the dataset\n",
    "\n",
    "\n",
    "X_test = np.load(\"X_test.npy\")\n",
    "y_test = np.load(\"y_test.npy\")\n",
    "person_train_valid = np.load(\"person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"y_train_valid.npy\")\n",
    "person_test = np.load(\"person_test.npy\")\n",
    "\n",
    "## Adjusting the labels so that \n",
    "\n",
    "# Cue onset left - 0\n",
    "# Cue onset right - 1\n",
    "# Cue onset foot - 2\n",
    "# Cue onset tongue - 3\n",
    "\n",
    "y_train_valid -= 769\n",
    "y_test -= 769\n",
    "\n",
    "\n",
    "## Random splitting and reshaping the data\n",
    "# First generating the training and validation indices using random splitting\n",
    "\n",
    "ind_valid = np.random.choice(2115, 375, replace=False)\n",
    "ind_train = np.array(list(set(range(2115)).difference(set(ind_valid))))\n",
    "\n",
    "# Creating the training and validation sets using the generated indices\n",
    "(X_train, X_valid) = X_train_valid[ind_train], X_train_valid[ind_valid] \n",
    "(y_train, y_valid) = y_train_valid[ind_train], y_train_valid[ind_valid]\n",
    "\n",
    "\n",
    "## Preprocessing the dataset\n",
    "x_train,y_train = data_prep(X_train,y_train,2,2,True)\n",
    "x_valid,y_valid = data_prep(X_valid,y_valid,2,2,True)\n",
    "X_test_prep,y_test_prep = data_prep(X_test,y_test,2,2,True)\n",
    "\n",
    "\n",
    "print('Shape of training set:',x_train.shape)\n",
    "print('Shape of validation set:',x_valid.shape)\n",
    "print('Shape of training labels:',y_train.shape)\n",
    "print('Shape of validation labels:',y_valid.shape)\n",
    "print('Shape of testing set:',X_test_prep.shape)\n",
    "print('Shape of testing labels:',y_test_prep.shape)\n",
    "\n",
    "\n",
    "# Converting the labels to categorical variables for multiclass classification\n",
    "y_train = to_categorical(y_train, 4)\n",
    "y_valid = to_categorical(y_valid, 4)\n",
    "y_test = to_categorical(y_test_prep, 4)\n",
    "print('Shape of training labels after categorical conversion:',y_train.shape)\n",
    "print('Shape of validation labels after categorical conversion:',y_valid.shape)\n",
    "print('Shape of test labels after categorical conversion:',y_test.shape)\n",
    "\n",
    "# Adding width of the segment to be 1\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)\n",
    "x_test = X_test_prep.reshape(X_test_prep.shape[0], X_test_prep.shape[1], X_test_prep.shape[2], 1)\n",
    "print('Shape of training set after adding width info:',x_train.shape)\n",
    "print('Shape of validation set after adding width info:',x_valid.shape)\n",
    "print('Shape of test set after adding width info:',x_test.shape)\n",
    "\n",
    "\n",
    "# Reshaping the training and validation dataset\n",
    "x_train = np.swapaxes(x_train, 1,3)\n",
    "x_train = np.swapaxes(x_train, 1,2)\n",
    "x_valid = np.swapaxes(x_valid, 1,3)\n",
    "x_valid = np.swapaxes(x_valid, 1,2)\n",
    "x_test = np.swapaxes(x_test, 1,3)\n",
    "x_test = np.swapaxes(x_test, 1,2)\n",
    "print('Shape of training set after dimension reshaping:',x_train.shape)\n",
    "print('Shape of validation set after dimension reshaping:',x_valid.shape)\n",
    "print('Shape of test set after dimension reshaping:',x_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_29 (Conv2D)          (None, 250, 1, 25)        5525      \n",
      "                                                                 \n",
      " max_pooling2d_28 (MaxPoolin  (None, 84, 1, 25)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_28 (Bat  (None, 84, 1, 25)        100       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 84, 1, 25)         0         \n",
      "                                                                 \n",
      " conv2d_30 (Conv2D)          (None, 84, 1, 50)         12550     \n",
      "                                                                 \n",
      " max_pooling2d_29 (MaxPoolin  (None, 28, 1, 50)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_29 (Bat  (None, 28, 1, 50)        200       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 28, 1, 50)         0         \n",
      "                                                                 \n",
      " conv2d_31 (Conv2D)          (None, 28, 1, 100)        50100     \n",
      "                                                                 \n",
      " max_pooling2d_30 (MaxPoolin  (None, 10, 1, 100)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_30 (Bat  (None, 10, 1, 100)       400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 10, 1, 100)        0         \n",
      "                                                                 \n",
      " conv2d_32 (Conv2D)          (None, 10, 1, 200)        200200    \n",
      "                                                                 \n",
      " max_pooling2d_31 (MaxPoolin  (None, 4, 1, 200)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_31 (Bat  (None, 4, 1, 200)        800       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 4, 1, 200)         0         \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 800)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 100)               80100     \n",
      "                                                                 \n",
      " reshape_3 (Reshape)         (None, 100, 1)            0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 10)                480       \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 4)                 44        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 350,499\n",
      "Trainable params: 349,749\n",
      "Non-trainable params: 750\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building the CNN model using sequential class\n",
    "hybrid_cnn_lstm_model = Sequential()\n",
    "\n",
    "# Conv. block 1\n",
    "hybrid_cnn_lstm_model.add(Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu', input_shape=(250,1,22)))\n",
    "hybrid_cnn_lstm_model.add(MaxPooling2D(pool_size=(3,1), padding='same')) # Read the keras documentation\n",
    "hybrid_cnn_lstm_model.add(BatchNormalization())\n",
    "hybrid_cnn_lstm_model.add(Dropout(0.5))\n",
    "\n",
    "# Conv. block 2\n",
    "hybrid_cnn_lstm_model.add(Conv2D(filters=50, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "hybrid_cnn_lstm_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "hybrid_cnn_lstm_model.add(BatchNormalization())\n",
    "hybrid_cnn_lstm_model.add(Dropout(0.5))\n",
    "\n",
    "# Conv. block 3\n",
    "hybrid_cnn_lstm_model.add(Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "hybrid_cnn_lstm_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "hybrid_cnn_lstm_model.add(BatchNormalization())\n",
    "hybrid_cnn_lstm_model.add(Dropout(0.5))\n",
    "\n",
    "# Conv. block 4\n",
    "hybrid_cnn_lstm_model.add(Conv2D(filters=200, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "hybrid_cnn_lstm_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "hybrid_cnn_lstm_model.add(BatchNormalization())\n",
    "hybrid_cnn_lstm_model.add(Dropout(0.5))\n",
    "\n",
    "# FC+LSTM layers\n",
    "hybrid_cnn_lstm_model.add(Flatten()) # Adding a flattening operation to the output of CNN block\n",
    "hybrid_cnn_lstm_model.add(Dense((100))) # FC layer with 100 units\n",
    "hybrid_cnn_lstm_model.add(Reshape((100,1))) # Reshape my output of FC layer so that it's compatible\n",
    "hybrid_cnn_lstm_model.add(LSTM(10, dropout=0.6, recurrent_dropout=0.1, input_shape=(100,1), return_sequences=False))\n",
    "\n",
    "\n",
    "# Output layer with Softmax activation \n",
    "hybrid_cnn_lstm_model.add(Dense(4, activation='softmax')) # Output FC layer with softmax activation\n",
    "\n",
    "\n",
    "# Printing the model summary\n",
    "hybrid_cnn_lstm_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "learning_rate = 1e-3\n",
    "epochs = 50\n",
    "hybrid_cnn_lstm_optimizer = keras.optimizers.Adam(lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "109/109 [==============================] - 11s 83ms/step - loss: 1.3732 - accuracy: 0.3029 - val_loss: 1.3364 - val_accuracy: 0.3713\n",
      "Epoch 2/50\n",
      "109/109 [==============================] - 9s 80ms/step - loss: 1.3001 - accuracy: 0.3819 - val_loss: 1.2494 - val_accuracy: 0.4547\n",
      "Epoch 3/50\n",
      "109/109 [==============================] - 9s 84ms/step - loss: 1.2391 - accuracy: 0.4394 - val_loss: 1.1863 - val_accuracy: 0.4960\n",
      "Epoch 4/50\n",
      "109/109 [==============================] - 9s 84ms/step - loss: 1.2157 - accuracy: 0.4496 - val_loss: 1.1806 - val_accuracy: 0.4880\n",
      "Epoch 5/50\n",
      "109/109 [==============================] - 9s 83ms/step - loss: 1.1756 - accuracy: 0.4776 - val_loss: 1.1527 - val_accuracy: 0.5273\n",
      "Epoch 6/50\n",
      "109/109 [==============================] - 9s 84ms/step - loss: 1.1516 - accuracy: 0.4884 - val_loss: 1.1558 - val_accuracy: 0.5120\n",
      "Epoch 7/50\n",
      "109/109 [==============================] - 10s 92ms/step - loss: 1.1213 - accuracy: 0.5013 - val_loss: 1.1212 - val_accuracy: 0.5153\n",
      "Epoch 8/50\n",
      "109/109 [==============================] - 9s 84ms/step - loss: 1.0979 - accuracy: 0.5102 - val_loss: 1.1181 - val_accuracy: 0.5273\n",
      "Epoch 9/50\n",
      "109/109 [==============================] - 10s 93ms/step - loss: 1.0663 - accuracy: 0.5266 - val_loss: 1.1118 - val_accuracy: 0.4973\n",
      "Epoch 10/50\n",
      "109/109 [==============================] - 9s 84ms/step - loss: 1.0360 - accuracy: 0.5455 - val_loss: 1.0979 - val_accuracy: 0.5160\n",
      "Epoch 11/50\n",
      "109/109 [==============================] - 9s 84ms/step - loss: 1.0123 - accuracy: 0.5688 - val_loss: 1.0684 - val_accuracy: 0.5493\n",
      "Epoch 12/50\n",
      "109/109 [==============================] - 9s 86ms/step - loss: 0.9945 - accuracy: 0.5705 - val_loss: 1.1130 - val_accuracy: 0.4953\n",
      "Epoch 13/50\n",
      "109/109 [==============================] - 9s 81ms/step - loss: 0.9856 - accuracy: 0.5772 - val_loss: 1.0776 - val_accuracy: 0.5593\n",
      "Epoch 14/50\n",
      "109/109 [==============================] - 9s 83ms/step - loss: 0.9608 - accuracy: 0.5855 - val_loss: 1.1129 - val_accuracy: 0.4987\n",
      "Epoch 15/50\n",
      "109/109 [==============================] - 10s 89ms/step - loss: 0.9397 - accuracy: 0.5976 - val_loss: 1.0983 - val_accuracy: 0.4940\n",
      "Epoch 16/50\n",
      "109/109 [==============================] - 10s 88ms/step - loss: 0.9113 - accuracy: 0.6114 - val_loss: 1.0493 - val_accuracy: 0.5600\n",
      "Epoch 17/50\n",
      "109/109 [==============================] - 9s 87ms/step - loss: 0.9077 - accuracy: 0.6157 - val_loss: 1.0589 - val_accuracy: 0.5487\n",
      "Epoch 18/50\n",
      "109/109 [==============================] - 9s 85ms/step - loss: 0.8754 - accuracy: 0.6322 - val_loss: 1.1738 - val_accuracy: 0.4713\n",
      "Epoch 19/50\n",
      "109/109 [==============================] - 9s 85ms/step - loss: 0.8790 - accuracy: 0.6280 - val_loss: 1.1651 - val_accuracy: 0.5073\n",
      "Epoch 20/50\n",
      "109/109 [==============================] - 9s 86ms/step - loss: 0.8380 - accuracy: 0.6507 - val_loss: 1.0910 - val_accuracy: 0.5147\n",
      "Epoch 21/50\n",
      "109/109 [==============================] - 9s 82ms/step - loss: 0.8264 - accuracy: 0.6490 - val_loss: 1.1576 - val_accuracy: 0.4993\n",
      "Epoch 22/50\n",
      "109/109 [==============================] - 9s 81ms/step - loss: 0.8151 - accuracy: 0.6631 - val_loss: 1.1880 - val_accuracy: 0.5060\n",
      "Epoch 23/50\n",
      "109/109 [==============================] - 9s 81ms/step - loss: 0.7887 - accuracy: 0.6739 - val_loss: 1.1564 - val_accuracy: 0.5220\n",
      "Epoch 24/50\n",
      "109/109 [==============================] - 9s 82ms/step - loss: 0.7905 - accuracy: 0.6763 - val_loss: 1.1710 - val_accuracy: 0.5067\n",
      "Epoch 25/50\n",
      "109/109 [==============================] - 9s 81ms/step - loss: 0.7566 - accuracy: 0.6957 - val_loss: 1.1450 - val_accuracy: 0.5400\n",
      "Epoch 26/50\n",
      "109/109 [==============================] - 9s 82ms/step - loss: 0.7651 - accuracy: 0.6868 - val_loss: 1.1282 - val_accuracy: 0.5580\n",
      "Epoch 27/50\n",
      "109/109 [==============================] - 9s 85ms/step - loss: 0.7360 - accuracy: 0.7047 - val_loss: 1.1986 - val_accuracy: 0.5300\n",
      "Epoch 28/50\n",
      "109/109 [==============================] - 9s 82ms/step - loss: 0.7348 - accuracy: 0.7060 - val_loss: 1.1610 - val_accuracy: 0.5493\n",
      "Epoch 29/50\n",
      "109/109 [==============================] - 9s 84ms/step - loss: 0.7033 - accuracy: 0.7211 - val_loss: 1.2557 - val_accuracy: 0.5227\n",
      "Epoch 30/50\n",
      "109/109 [==============================] - 10s 89ms/step - loss: 0.7055 - accuracy: 0.7227 - val_loss: 1.2054 - val_accuracy: 0.5427\n",
      "Epoch 31/50\n",
      "109/109 [==============================] - 10s 90ms/step - loss: 0.6798 - accuracy: 0.7343 - val_loss: 1.1162 - val_accuracy: 0.5733\n",
      "Epoch 32/50\n",
      "109/109 [==============================] - 9s 86ms/step - loss: 0.6741 - accuracy: 0.7364 - val_loss: 1.1483 - val_accuracy: 0.5827\n",
      "Epoch 33/50\n",
      "109/109 [==============================] - 9s 82ms/step - loss: 0.6600 - accuracy: 0.7460 - val_loss: 1.2238 - val_accuracy: 0.5473\n",
      "Epoch 34/50\n",
      "109/109 [==============================] - 9s 80ms/step - loss: 0.6365 - accuracy: 0.7511 - val_loss: 1.1669 - val_accuracy: 0.5673\n",
      "Epoch 35/50\n",
      "109/109 [==============================] - 9s 84ms/step - loss: 0.6186 - accuracy: 0.7632 - val_loss: 1.2083 - val_accuracy: 0.5573\n",
      "Epoch 36/50\n",
      "109/109 [==============================] - 9s 81ms/step - loss: 0.6082 - accuracy: 0.7694 - val_loss: 1.1394 - val_accuracy: 0.5853\n",
      "Epoch 37/50\n",
      "109/109 [==============================] - 9s 80ms/step - loss: 0.5987 - accuracy: 0.7744 - val_loss: 1.1071 - val_accuracy: 0.6120\n",
      "Epoch 38/50\n",
      "109/109 [==============================] - 9s 81ms/step - loss: 0.5907 - accuracy: 0.7739 - val_loss: 1.1166 - val_accuracy: 0.6007\n",
      "Epoch 39/50\n",
      "109/109 [==============================] - 9s 82ms/step - loss: 0.5756 - accuracy: 0.7796 - val_loss: 1.0716 - val_accuracy: 0.6287\n",
      "Epoch 40/50\n",
      "109/109 [==============================] - 9s 82ms/step - loss: 0.5571 - accuracy: 0.7894 - val_loss: 1.0756 - val_accuracy: 0.6287\n",
      "Epoch 41/50\n",
      "109/109 [==============================] - 9s 83ms/step - loss: 0.5535 - accuracy: 0.7911 - val_loss: 1.1712 - val_accuracy: 0.5920\n",
      "Epoch 42/50\n",
      "109/109 [==============================] - 9s 83ms/step - loss: 0.5398 - accuracy: 0.8052 - val_loss: 1.0864 - val_accuracy: 0.6380\n",
      "Epoch 43/50\n",
      "109/109 [==============================] - 9s 84ms/step - loss: 0.5375 - accuracy: 0.8011 - val_loss: 1.0894 - val_accuracy: 0.6407\n",
      "Epoch 44/50\n",
      "109/109 [==============================] - 9s 84ms/step - loss: 0.5165 - accuracy: 0.8088 - val_loss: 1.0297 - val_accuracy: 0.6607\n",
      "Epoch 45/50\n",
      "109/109 [==============================] - 9s 85ms/step - loss: 0.5263 - accuracy: 0.8043 - val_loss: 1.2336 - val_accuracy: 0.6053\n",
      "Epoch 46/50\n",
      "109/109 [==============================] - 9s 86ms/step - loss: 0.4961 - accuracy: 0.8162 - val_loss: 1.1148 - val_accuracy: 0.6320\n",
      "Epoch 47/50\n",
      "109/109 [==============================] - 10s 88ms/step - loss: 0.5186 - accuracy: 0.8083 - val_loss: 1.0337 - val_accuracy: 0.6527\n",
      "Epoch 48/50\n",
      "109/109 [==============================] - 9s 82ms/step - loss: 0.4828 - accuracy: 0.8231 - val_loss: 1.0685 - val_accuracy: 0.6467\n",
      "Epoch 49/50\n",
      "109/109 [==============================] - 9s 81ms/step - loss: 0.4915 - accuracy: 0.8181 - val_loss: 1.1036 - val_accuracy: 0.6387\n",
      "Epoch 50/50\n",
      "109/109 [==============================] - 9s 81ms/step - loss: 0.4726 - accuracy: 0.8287 - val_loss: 1.0991 - val_accuracy: 0.6380\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model\n",
    "hybrid_cnn_lstm_model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=hybrid_cnn_lstm_optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Training and validating the model\n",
    "hybrid_cnn_lstm_model_results = hybrid_cnn_lstm_model.fit(x_train,\n",
    "             y_train,\n",
    "             batch_size=64,\n",
    "             epochs=epochs,\n",
    "             validation_data=(x_valid, y_valid), verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of the hybrid CNN-LSTM model: 0.6399548649787903\n"
     ]
    }
   ],
   "source": [
    "## Testing the hybrid CNN-LSTM model\n",
    "\n",
    "hybrid_cnn_lstm_score = hybrid_cnn_lstm_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test accuracy of the hybrid CNN-LSTM model:',hybrid_cnn_lstm_score[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Optimize the classification accuracy for subject 1. Does it help to train across all subjects?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test Shape for Subject 0: (50, 22, 1000)\n",
      "y_test Shape for Subject 0: (50,)\n",
      "X_train_valid Shape for Subject 0: (237, 22, 1000)\n",
      "y_train_valid Shape for Subject 0: (237,)\n"
     ]
    }
   ],
   "source": [
    "X_test = np.load(\"X_test.npy\")\n",
    "y_test = np.load(\"y_test.npy\")\n",
    "person_train_valid = np.load(\"person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"y_train_valid.npy\")\n",
    "person_test = np.load(\"person_test.npy\")\n",
    "\n",
    "## Adjusting the labels so that \n",
    "\n",
    "# Cue onset left - 0\n",
    "# Cue onset right - 1\n",
    "# Cue onset foot - 2\n",
    "# Cue onset tongue - 3\n",
    "\n",
    "y_train_valid -= 769\n",
    "y_test -= 769\n",
    "\n",
    "subject = 0\n",
    "subject_test_idx = np.where(person_test==subject)[0]\n",
    "subject_valid_idx = np.where(person_train_valid==subject)[0]\n",
    "\n",
    "\n",
    "subject_X_test = X_test[subject_test_idx]\n",
    "suject_y_test = y_test[subject_test_idx]\n",
    "suject_X_train_valid = X_train_valid[subject_valid_idx]\n",
    "suject_y_train_valid = y_train_valid[subject_valid_idx]\n",
    "\n",
    "print(f'X_test Shape for Subject {subject}: {subject_X_test.shape}')\n",
    "print(f'y_test Shape for Subject {subject}: {suject_y_test.shape}')\n",
    "print(f'X_train_valid Shape for Subject {subject}: {suject_X_train_valid.shape}')\n",
    "print(f'y_train_valid Shape for Subject {subject}: {suject_y_train_valid.shape}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X after trimming: (190, 22, 500)\n",
      "Shape of X after maxpooling: (190, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (380, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (760, 22, 250)\n",
      "Shape of X after trimming: (47, 22, 500)\n",
      "Shape of X after maxpooling: (47, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (94, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (188, 22, 250)\n",
      "Shape of X after trimming: (50, 22, 500)\n",
      "Shape of X after maxpooling: (50, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (100, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (200, 22, 250)\n",
      "Shape of training set: (760, 22, 250)\n",
      "Shape of validation set: (188, 22, 250)\n",
      "Shape of training labels: (760,)\n",
      "Shape of validation labels: (188,)\n",
      "Shape of testing set: (200, 22, 250)\n",
      "Shape of testing labels: (200,)\n",
      "Shape of training labels after categorical conversion: (760, 4)\n",
      "Shape of validation labels after categorical conversion: (188, 4)\n",
      "Shape of test labels after categorical conversion: (200, 4)\n",
      "Shape of training set after adding width info: (760, 22, 250, 1)\n",
      "Shape of validation set after adding width info: (188, 22, 250, 1)\n",
      "Shape of test set after adding width info: (200, 22, 250, 1)\n",
      "Shape of training set after dimension reshaping: (760, 250, 1, 22)\n",
      "Shape of validation set after dimension reshaping: (188, 250, 1, 22)\n",
      "Shape of test set after dimension reshaping: (200, 250, 1, 22)\n"
     ]
    }
   ],
   "source": [
    "# shuffle with 5 fold\n",
    "indicies_valid = np.random.choice(suject_X_train_valid.shape[0], suject_X_train_valid.shape[0] // 5, replace=False)\n",
    "indicies_train = np.array(list(set(range(suject_X_train_valid.shape[0])).difference(set(indicies_valid))))\n",
    "\n",
    "# Creating the training and validation sets using the generated indices\n",
    "X_train, X_valid = suject_X_train_valid[indicies_train], suject_X_train_valid[indicies_valid] \n",
    "y_train, y_valid = suject_y_train_valid[indicies_train], suject_y_train_valid[indicies_valid]\n",
    "\n",
    "\n",
    "# Preprocessing the dataset\n",
    "x_train,y_train = data_prep(X_train,y_train,2,2,True)\n",
    "x_valid,y_valid = data_prep(X_valid,y_valid,2,2,True)\n",
    "X_test_prep,y_test_prep = data_prep(subject_X_test,suject_y_test,2,2,True)\n",
    "\n",
    "\n",
    "\n",
    "print('Shape of training set:',x_train.shape)\n",
    "print('Shape of validation set:',x_valid.shape)\n",
    "print('Shape of training labels:',y_train.shape)\n",
    "print('Shape of validation labels:',y_valid.shape)\n",
    "print('Shape of testing set:',X_test_prep.shape)\n",
    "print('Shape of testing labels:',y_test_prep.shape)\n",
    "\n",
    "\n",
    "# Converting the labels to categorical variables for multiclass classification\n",
    "y_train = to_categorical(y_train, 4)\n",
    "y_valid = to_categorical(y_valid, 4)\n",
    "y_test = to_categorical(y_test_prep, 4)\n",
    "print('Shape of training labels after categorical conversion:',y_train.shape)\n",
    "print('Shape of validation labels after categorical conversion:',y_valid.shape)\n",
    "print('Shape of test labels after categorical conversion:',y_test.shape)\n",
    "\n",
    "# Adding width of the segment to be 1\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)\n",
    "x_test = X_test_prep.reshape(X_test_prep.shape[0], X_test_prep.shape[1], X_test_prep.shape[2], 1)\n",
    "print('Shape of training set after adding width info:',x_train.shape)\n",
    "print('Shape of validation set after adding width info:',x_valid.shape)\n",
    "print('Shape of test set after adding width info:',x_test.shape)\n",
    "\n",
    "\n",
    "# Reshaping the training and validation dataset\n",
    "x_train = np.swapaxes(x_train, 1,3)\n",
    "x_train = np.swapaxes(x_train, 1,2)\n",
    "x_valid = np.swapaxes(x_valid, 1,3)\n",
    "x_valid = np.swapaxes(x_valid, 1,2)\n",
    "x_test = np.swapaxes(x_test, 1,3)\n",
    "x_test = np.swapaxes(x_test, 1,2)\n",
    "print('Shape of training set after dimension reshaping:',x_train.shape)\n",
    "print('Shape of validation set after dimension reshaping:',x_valid.shape)\n",
    "print('Shape of test set after dimension reshaping:',x_test.shape)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_5 (Conv2D)           (None, 250, 1, 10)        1110      \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 84, 1, 10)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 84, 1, 10)        40        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 84, 1, 10)         0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 84, 1, 10)         1510      \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 28, 1, 10)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 28, 1, 10)        40        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 28, 1, 10)         0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 280)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 1124      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,824\n",
      "Trainable params: 3,784\n",
      "Non-trainable params: 40\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Building the CNN model using sequential class\n",
    "cnn_subject_model = Sequential()\n",
    "\n",
    "# Conv. block 1\n",
    "cnn_subject_model.add(Conv2D(filters=10, kernel_size=(5,1), padding='same', activation='elu', input_shape=(250,1,22)))\n",
    "cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same')) \n",
    "cnn_subject_model.add(BatchNormalization())\n",
    "cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "# Conv. block 2\n",
    "cnn_subject_model.add(Conv2D(filters=10, kernel_size=(15,1), padding='same', activation='elu'))\n",
    "cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "cnn_subject_model.add(BatchNormalization())\n",
    "cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "\n",
    "# # Conv. block 4\n",
    "# cnn_subject_model.add(Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "# cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "# cnn_subject_model.add(BatchNormalization())\n",
    "# cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "# # Conv. block 5\n",
    "# cnn_subject_model.add(Conv2D(filters=100, kernel_size=(50,1), padding='same', activation='elu'))\n",
    "# cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "# cnn_subject_model.add(BatchNormalization())\n",
    "# cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "# # Conv. block 6\n",
    "# cnn_subject_model.add(Conv2D(filters=50, kernel_size=(50,1), padding='same', activation='elu'))\n",
    "# cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "# cnn_subject_model.add(BatchNormalization())\n",
    "# cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "# # Conv. block 7\n",
    "# cnn_subject_model.add(Conv2D(filters=25, kernel_size=(50,1), padding='same', activation='elu'))\n",
    "# cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "# cnn_subject_model.add(BatchNormalization())\n",
    "# cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer with Softmax activation\n",
    "cnn_subject_model.add(Flatten()) # Flattens the input\n",
    "cnn_subject_model.add(Dense(4, activation='softmax')) # Output FC layer with softmax activation\n",
    "\n",
    "\n",
    "# Printing the model summary\n",
    "cnn_subject_model.summary()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 100\n",
    "cnn_subject_model_optimizer = keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 26ms/step - loss: 0.1424 - accuracy: 0.9500 - val_loss: 1.3554 - val_accuracy: 0.6543\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.1654 - accuracy: 0.9434 - val_loss: 1.2711 - val_accuracy: 0.6543\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.1715 - accuracy: 0.9408 - val_loss: 1.4166 - val_accuracy: 0.6011\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1713 - accuracy: 0.9355 - val_loss: 1.3082 - val_accuracy: 0.6649\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1564 - accuracy: 0.9382 - val_loss: 1.3320 - val_accuracy: 0.6489\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1449 - accuracy: 0.9474 - val_loss: 1.3556 - val_accuracy: 0.6489\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1455 - accuracy: 0.9513 - val_loss: 1.3389 - val_accuracy: 0.6543\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1560 - accuracy: 0.9316 - val_loss: 1.3372 - val_accuracy: 0.6330\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1334 - accuracy: 0.9605 - val_loss: 1.3336 - val_accuracy: 0.6543\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1406 - accuracy: 0.9526 - val_loss: 1.3054 - val_accuracy: 0.6596\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1612 - accuracy: 0.9368 - val_loss: 1.3412 - val_accuracy: 0.6489\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1283 - accuracy: 0.9513 - val_loss: 1.3061 - val_accuracy: 0.6596\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1320 - accuracy: 0.9474 - val_loss: 1.3198 - val_accuracy: 0.6436\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1484 - accuracy: 0.9474 - val_loss: 1.4087 - val_accuracy: 0.6543\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1476 - accuracy: 0.9474 - val_loss: 1.3360 - val_accuracy: 0.6543\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1372 - accuracy: 0.9421 - val_loss: 1.4072 - val_accuracy: 0.6489\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1358 - accuracy: 0.9487 - val_loss: 1.4081 - val_accuracy: 0.6383\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1470 - accuracy: 0.9395 - val_loss: 1.3537 - val_accuracy: 0.6436\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1502 - accuracy: 0.9408 - val_loss: 1.3917 - val_accuracy: 0.6489\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1347 - accuracy: 0.9553 - val_loss: 1.3019 - val_accuracy: 0.6702\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1501 - accuracy: 0.9355 - val_loss: 1.3829 - val_accuracy: 0.6383\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1176 - accuracy: 0.9592 - val_loss: 1.3740 - val_accuracy: 0.6543\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1334 - accuracy: 0.9461 - val_loss: 1.3935 - val_accuracy: 0.6223\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1399 - accuracy: 0.9474 - val_loss: 1.4126 - val_accuracy: 0.6383\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1127 - accuracy: 0.9618 - val_loss: 1.3388 - val_accuracy: 0.6489\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0941 - accuracy: 0.9711 - val_loss: 1.3718 - val_accuracy: 0.6489\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1289 - accuracy: 0.9487 - val_loss: 1.3385 - val_accuracy: 0.6543\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1241 - accuracy: 0.9592 - val_loss: 1.3857 - val_accuracy: 0.6223\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1286 - accuracy: 0.9487 - val_loss: 1.3494 - val_accuracy: 0.6489\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1388 - accuracy: 0.9487 - val_loss: 1.4356 - val_accuracy: 0.6489\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1059 - accuracy: 0.9539 - val_loss: 1.4280 - val_accuracy: 0.6277\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1374 - accuracy: 0.9421 - val_loss: 1.4361 - val_accuracy: 0.6117\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1429 - accuracy: 0.9461 - val_loss: 1.3452 - val_accuracy: 0.6489\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1369 - accuracy: 0.9447 - val_loss: 1.4848 - val_accuracy: 0.6277\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1284 - accuracy: 0.9500 - val_loss: 1.3179 - val_accuracy: 0.6436\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1253 - accuracy: 0.9579 - val_loss: 1.3639 - val_accuracy: 0.6489\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1274 - accuracy: 0.9566 - val_loss: 1.3197 - val_accuracy: 0.6543\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1064 - accuracy: 0.9645 - val_loss: 1.3163 - val_accuracy: 0.6436\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1078 - accuracy: 0.9579 - val_loss: 1.3145 - val_accuracy: 0.6543\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1185 - accuracy: 0.9526 - val_loss: 1.3581 - val_accuracy: 0.6543\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1129 - accuracy: 0.9605 - val_loss: 1.3278 - val_accuracy: 0.6543\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1386 - accuracy: 0.9461 - val_loss: 1.3329 - val_accuracy: 0.6543\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1179 - accuracy: 0.9566 - val_loss: 1.3884 - val_accuracy: 0.6436\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1149 - accuracy: 0.9566 - val_loss: 1.3237 - val_accuracy: 0.6543\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1274 - accuracy: 0.9632 - val_loss: 1.3458 - val_accuracy: 0.6543\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1106 - accuracy: 0.9539 - val_loss: 1.3637 - val_accuracy: 0.6543\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1166 - accuracy: 0.9579 - val_loss: 1.3271 - val_accuracy: 0.6543\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0803 - accuracy: 0.9750 - val_loss: 1.3241 - val_accuracy: 0.6543\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.1052 - accuracy: 0.9592 - val_loss: 1.4477 - val_accuracy: 0.6489\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0964 - accuracy: 0.9605 - val_loss: 1.3309 - val_accuracy: 0.6543\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.1372 - accuracy: 0.9526 - val_loss: 1.4586 - val_accuracy: 0.6489\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1100 - accuracy: 0.9684 - val_loss: 1.4342 - val_accuracy: 0.6543\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0992 - accuracy: 0.9658 - val_loss: 1.3931 - val_accuracy: 0.6543\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1018 - accuracy: 0.9632 - val_loss: 1.4343 - val_accuracy: 0.6489\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1269 - accuracy: 0.9579 - val_loss: 1.3907 - val_accuracy: 0.6489\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0885 - accuracy: 0.9671 - val_loss: 1.4127 - val_accuracy: 0.6596\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1149 - accuracy: 0.9632 - val_loss: 1.4805 - val_accuracy: 0.6543\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0869 - accuracy: 0.9684 - val_loss: 1.5288 - val_accuracy: 0.6436\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1234 - accuracy: 0.9605 - val_loss: 1.4784 - val_accuracy: 0.6489\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0962 - accuracy: 0.9645 - val_loss: 1.4667 - val_accuracy: 0.6489\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0853 - accuracy: 0.9671 - val_loss: 1.4470 - val_accuracy: 0.6436\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0777 - accuracy: 0.9737 - val_loss: 1.5285 - val_accuracy: 0.6543\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0973 - accuracy: 0.9645 - val_loss: 1.5629 - val_accuracy: 0.6383\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1167 - accuracy: 0.9539 - val_loss: 1.5026 - val_accuracy: 0.6596\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0965 - accuracy: 0.9618 - val_loss: 1.5399 - val_accuracy: 0.6436\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1258 - accuracy: 0.9553 - val_loss: 1.4837 - val_accuracy: 0.6543\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0977 - accuracy: 0.9618 - val_loss: 1.4881 - val_accuracy: 0.6330\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1001 - accuracy: 0.9605 - val_loss: 1.4282 - val_accuracy: 0.6596\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0783 - accuracy: 0.9737 - val_loss: 1.4080 - val_accuracy: 0.6596\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1006 - accuracy: 0.9697 - val_loss: 1.4347 - val_accuracy: 0.6489\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1283 - accuracy: 0.9500 - val_loss: 1.3951 - val_accuracy: 0.6543\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0830 - accuracy: 0.9711 - val_loss: 1.4463 - val_accuracy: 0.6489\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0852 - accuracy: 0.9658 - val_loss: 1.3878 - val_accuracy: 0.6489\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0874 - accuracy: 0.9671 - val_loss: 1.2818 - val_accuracy: 0.6543\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1139 - accuracy: 0.9632 - val_loss: 1.3534 - val_accuracy: 0.6489\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0880 - accuracy: 0.9711 - val_loss: 1.2789 - val_accuracy: 0.6489\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1072 - accuracy: 0.9605 - val_loss: 1.5180 - val_accuracy: 0.6383\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1019 - accuracy: 0.9605 - val_loss: 1.4189 - val_accuracy: 0.6809\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0903 - accuracy: 0.9697 - val_loss: 1.4417 - val_accuracy: 0.6543\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0930 - accuracy: 0.9671 - val_loss: 1.4489 - val_accuracy: 0.6489\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0974 - accuracy: 0.9671 - val_loss: 1.3728 - val_accuracy: 0.6596\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0989 - accuracy: 0.9711 - val_loss: 1.4323 - val_accuracy: 0.6436\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0970 - accuracy: 0.9618 - val_loss: 1.4502 - val_accuracy: 0.6543\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0861 - accuracy: 0.9697 - val_loss: 1.3111 - val_accuracy: 0.6489\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0912 - accuracy: 0.9724 - val_loss: 1.3440 - val_accuracy: 0.6596\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0864 - accuracy: 0.9658 - val_loss: 1.4451 - val_accuracy: 0.6543\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0855 - accuracy: 0.9645 - val_loss: 1.4018 - val_accuracy: 0.6543\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0921 - accuracy: 0.9671 - val_loss: 1.3915 - val_accuracy: 0.6543\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1023 - accuracy: 0.9618 - val_loss: 1.3188 - val_accuracy: 0.6543\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0785 - accuracy: 0.9724 - val_loss: 1.3980 - val_accuracy: 0.6383\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0814 - accuracy: 0.9684 - val_loss: 1.3639 - val_accuracy: 0.6596\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0700 - accuracy: 0.9750 - val_loss: 1.3305 - val_accuracy: 0.6702\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0764 - accuracy: 0.9776 - val_loss: 1.4176 - val_accuracy: 0.6596\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0695 - accuracy: 0.9724 - val_loss: 1.4272 - val_accuracy: 0.6543\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0839 - accuracy: 0.9737 - val_loss: 1.5298 - val_accuracy: 0.6543\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0808 - accuracy: 0.9684 - val_loss: 1.4706 - val_accuracy: 0.6596\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1032 - accuracy: 0.9658 - val_loss: 1.3960 - val_accuracy: 0.6649\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0990 - accuracy: 0.9671 - val_loss: 1.5750 - val_accuracy: 0.6330\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.0786 - accuracy: 0.9737 - val_loss: 1.4349 - val_accuracy: 0.6702\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0933 - accuracy: 0.9658 - val_loss: 1.4216 - val_accuracy: 0.6702\n"
     ]
    }
   ],
   "source": [
    "cnn_subject_model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=cnn_subject_model_optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "cnn_subject_model_results = cnn_subject_model.fit(x_train,\n",
    "             y_train,\n",
    "             batch_size=64,\n",
    "             epochs=epochs,\n",
    "             validation_data=(x_valid, y_valid), verbose=True\n",
    "             )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of the CNN model for subject 0: 0.7850000262260437\n"
     ]
    }
   ],
   "source": [
    "cnn_subject_model_score = cnn_subject_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Test accuracy of the CNN model for subject {subject}:',cnn_subject_model_score[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Training across all subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test Shape for Subject 0: (50, 22, 1000)\n",
      "y_test Shape for Subject 0: (50,)\n",
      "X_train_valid Shape for Subject 0: (237, 22, 1000)\n",
      "y_train_valid Shape for Subject 0: (237,)\n",
      "Shape of X after trimming: (1692, 22, 500)\n",
      "Shape of X after maxpooling: (1692, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (3384, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (6768, 22, 250)\n",
      "Shape of X after trimming: (423, 22, 500)\n",
      "Shape of X after maxpooling: (423, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (846, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (1692, 22, 250)\n",
      "Shape of X after trimming: (50, 22, 500)\n",
      "Shape of X after maxpooling: (50, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (100, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (200, 22, 250)\n",
      "Shape of training set: (6768, 22, 250)\n",
      "Shape of validation set: (1692, 22, 250)\n",
      "Shape of training labels: (6768,)\n",
      "Shape of validation labels: (1692,)\n",
      "Shape of testing set: (200, 22, 250)\n",
      "Shape of testing labels: (200,)\n",
      "Shape of training labels after categorical conversion: (6768, 4)\n",
      "Shape of validation labels after categorical conversion: (1692, 4)\n",
      "Shape of test labels after categorical conversion: (200, 4)\n",
      "Shape of training set after adding width info: (6768, 22, 250, 1)\n",
      "Shape of validation set after adding width info: (1692, 22, 250, 1)\n",
      "Shape of test set after adding width info: (200, 22, 250, 1)\n",
      "Shape of training set after dimension reshaping: (6768, 250, 1, 22)\n",
      "Shape of validation set after dimension reshaping: (1692, 250, 1, 22)\n",
      "Shape of test set after dimension reshaping: (200, 250, 1, 22)\n"
     ]
    }
   ],
   "source": [
    "X_test = np.load(\"X_test.npy\")\n",
    "y_test = np.load(\"y_test.npy\")\n",
    "person_train_valid = np.load(\"person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"y_train_valid.npy\")\n",
    "person_test = np.load(\"person_test.npy\")\n",
    "\n",
    "## Adjusting the labels so that \n",
    "\n",
    "# Cue onset left - 0\n",
    "# Cue onset right - 1\n",
    "# Cue onset foot - 2\n",
    "# Cue onset tongue - 3\n",
    "\n",
    "y_train_valid -= 769\n",
    "y_test -= 769\n",
    "\n",
    "subject = 0\n",
    "subject_test_idx = np.where(person_test==subject)[0]\n",
    "subject_valid_idx = np.where(person_train_valid==subject)[0]\n",
    "\n",
    "\n",
    "subject_X_test = X_test[subject_test_idx]\n",
    "suject_y_test = y_test[subject_test_idx]\n",
    "suject_X_train_valid = X_train_valid[subject_valid_idx]\n",
    "suject_y_train_valid = y_train_valid[subject_valid_idx]\n",
    "\n",
    "print(f'X_test Shape for Subject {subject}: {subject_X_test.shape}')\n",
    "print(f'y_test Shape for Subject {subject}: {suject_y_test.shape}')\n",
    "print(f'X_train_valid Shape for Subject {subject}: {suject_X_train_valid.shape}')\n",
    "print(f'y_train_valid Shape for Subject {subject}: {suject_y_train_valid.shape}')\n",
    "\n",
    "# shuffle with 5 fold\n",
    "indicies_valid = np.random.choice(X_train_valid.shape[0], X_train_valid.shape[0] // 5, replace=False)\n",
    "indicies_train = np.array(list(set(range(X_train_valid.shape[0])).difference(set(indicies_valid))))\n",
    "\n",
    "# Creating the training and validation sets using the generated indices\n",
    "X_train, X_valid = X_train_valid[indicies_train], X_train_valid[indicies_valid] \n",
    "y_train, y_valid = y_train_valid[indicies_train], y_train_valid[indicies_valid]\n",
    "\n",
    "\n",
    "# Preprocessing the dataset\n",
    "x_train,y_train = data_prep(X_train,y_train,2,2,True)\n",
    "x_valid,y_valid = data_prep(X_valid,y_valid,2,2,True)\n",
    "X_test_prep,y_test_prep = data_prep(subject_X_test,suject_y_test,2,2,True)\n",
    "\n",
    "\n",
    "\n",
    "print('Shape of training set:',x_train.shape)\n",
    "print('Shape of validation set:',x_valid.shape)\n",
    "print('Shape of training labels:',y_train.shape)\n",
    "print('Shape of validation labels:',y_valid.shape)\n",
    "print('Shape of testing set:',X_test_prep.shape)\n",
    "print('Shape of testing labels:',y_test_prep.shape)\n",
    "\n",
    "\n",
    "# Converting the labels to categorical variables for multiclass classification\n",
    "y_train = to_categorical(y_train, 4)\n",
    "y_valid = to_categorical(y_valid, 4)\n",
    "y_test = to_categorical(y_test_prep, 4)\n",
    "print('Shape of training labels after categorical conversion:',y_train.shape)\n",
    "print('Shape of validation labels after categorical conversion:',y_valid.shape)\n",
    "print('Shape of test labels after categorical conversion:',y_test.shape)\n",
    "\n",
    "# Adding width of the segment to be 1\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)\n",
    "x_test = X_test_prep.reshape(X_test_prep.shape[0], X_test_prep.shape[1], X_test_prep.shape[2], 1)\n",
    "print('Shape of training set after adding width info:',x_train.shape)\n",
    "print('Shape of validation set after adding width info:',x_valid.shape)\n",
    "print('Shape of test set after adding width info:',x_test.shape)\n",
    "\n",
    "\n",
    "# Reshaping the training and validation dataset\n",
    "x_train = np.swapaxes(x_train, 1,3)\n",
    "x_train = np.swapaxes(x_train, 1,2)\n",
    "x_valid = np.swapaxes(x_valid, 1,3)\n",
    "x_valid = np.swapaxes(x_valid, 1,2)\n",
    "x_test = np.swapaxes(x_test, 1,3)\n",
    "x_test = np.swapaxes(x_test, 1,2)\n",
    "print('Shape of training set after dimension reshaping:',x_train.shape)\n",
    "print('Shape of validation set after dimension reshaping:',x_valid.shape)\n",
    "print('Shape of test set after dimension reshaping:',x_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_15 (Conv2D)          (None, 250, 1, 10)        1110      \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPoolin  (None, 84, 1, 10)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 84, 1, 10)        40        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 84, 1, 10)         0         \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 84, 1, 10)         1510      \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPoolin  (None, 28, 1, 10)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 28, 1, 10)        40        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 28, 1, 10)         0         \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 280)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 4)                 1124      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,824\n",
      "Trainable params: 3,784\n",
      "Non-trainable params: 40\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Building the CNN model using sequential class\n",
    "cnn_subject_model = Sequential()\n",
    "\n",
    "# Conv. block 1\n",
    "cnn_subject_model.add(Conv2D(filters=10, kernel_size=(5,1), padding='same', activation='elu', input_shape=(250,1,22)))\n",
    "cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same')) \n",
    "cnn_subject_model.add(BatchNormalization())\n",
    "cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "# Conv. block 2\n",
    "cnn_subject_model.add(Conv2D(filters=10, kernel_size=(15,1), padding='same', activation='elu'))\n",
    "cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "cnn_subject_model.add(BatchNormalization())\n",
    "cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "\n",
    "# # Conv. block 4\n",
    "# cnn_subject_model.add(Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "# cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "# cnn_subject_model.add(BatchNormalization())\n",
    "# cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "# # Conv. block 5\n",
    "# cnn_subject_model.add(Conv2D(filters=100, kernel_size=(50,1), padding='same', activation='elu'))\n",
    "# cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "# cnn_subject_model.add(BatchNormalization())\n",
    "# cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "# # Conv. block 6\n",
    "# cnn_subject_model.add(Conv2D(filters=50, kernel_size=(50,1), padding='same', activation='elu'))\n",
    "# cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "# cnn_subject_model.add(BatchNormalization())\n",
    "# cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "# # Conv. block 7\n",
    "# cnn_subject_model.add(Conv2D(filters=25, kernel_size=(50,1), padding='same', activation='elu'))\n",
    "# cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "# cnn_subject_model.add(BatchNormalization())\n",
    "# cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer with Softmax activation\n",
    "cnn_subject_model.add(Flatten()) # Flattens the input\n",
    "cnn_subject_model.add(Dense(4, activation='softmax')) # Output FC layer with softmax activation\n",
    "\n",
    "\n",
    "# Printing the model summary\n",
    "cnn_subject_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 100\n",
    "cnn_subject_model_optimizer = keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "106/106 [==============================] - 2s 11ms/step - loss: 1.9760 - accuracy: 0.2850 - val_loss: 1.3613 - val_accuracy: 0.4084\n",
      "Epoch 2/100\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 1.5502 - accuracy: 0.3527 - val_loss: 1.2148 - val_accuracy: 0.4326\n",
      "Epoch 3/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.3634 - accuracy: 0.3973 - val_loss: 1.1623 - val_accuracy: 0.4823\n",
      "Epoch 4/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.2364 - accuracy: 0.4465 - val_loss: 1.1127 - val_accuracy: 0.5171\n",
      "Epoch 5/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.1904 - accuracy: 0.4818 - val_loss: 1.0909 - val_accuracy: 0.5177\n",
      "Epoch 6/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.1424 - accuracy: 0.5132 - val_loss: 1.0685 - val_accuracy: 0.5437\n",
      "Epoch 7/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 1.1179 - accuracy: 0.5142 - val_loss: 1.0436 - val_accuracy: 0.5774\n",
      "Epoch 8/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 1.0872 - accuracy: 0.5380 - val_loss: 1.0152 - val_accuracy: 0.5792\n",
      "Epoch 9/100\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 1.0540 - accuracy: 0.5604 - val_loss: 0.9979 - val_accuracy: 0.5863\n",
      "Epoch 10/100\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 1.0393 - accuracy: 0.5622 - val_loss: 0.9958 - val_accuracy: 0.5881\n",
      "Epoch 11/100\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 1.0235 - accuracy: 0.5758 - val_loss: 0.9685 - val_accuracy: 0.6123\n",
      "Epoch 12/100\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.9936 - accuracy: 0.5839 - val_loss: 0.9650 - val_accuracy: 0.5934\n",
      "Epoch 13/100\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.9760 - accuracy: 0.5977 - val_loss: 0.9458 - val_accuracy: 0.6223\n",
      "Epoch 14/100\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.9603 - accuracy: 0.6068 - val_loss: 0.9299 - val_accuracy: 0.6348\n",
      "Epoch 15/100\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.9401 - accuracy: 0.6170 - val_loss: 0.9107 - val_accuracy: 0.6312\n",
      "Epoch 16/100\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.9376 - accuracy: 0.6139 - val_loss: 0.9132 - val_accuracy: 0.6277\n",
      "Epoch 17/100\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.9267 - accuracy: 0.6170 - val_loss: 0.8749 - val_accuracy: 0.6472\n",
      "Epoch 18/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.9302 - accuracy: 0.6147 - val_loss: 0.8955 - val_accuracy: 0.6377\n",
      "Epoch 19/100\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.9130 - accuracy: 0.6215 - val_loss: 0.8663 - val_accuracy: 0.6613\n",
      "Epoch 20/100\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.8926 - accuracy: 0.6374 - val_loss: 0.8797 - val_accuracy: 0.6472\n",
      "Epoch 21/100\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.8963 - accuracy: 0.6331 - val_loss: 0.8448 - val_accuracy: 0.6495\n",
      "Epoch 22/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8899 - accuracy: 0.6390 - val_loss: 0.8273 - val_accuracy: 0.6809\n",
      "Epoch 23/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8713 - accuracy: 0.6480 - val_loss: 0.8268 - val_accuracy: 0.6791\n",
      "Epoch 24/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8624 - accuracy: 0.6501 - val_loss: 0.7943 - val_accuracy: 0.7045\n",
      "Epoch 25/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8718 - accuracy: 0.6460 - val_loss: 0.8038 - val_accuracy: 0.6856\n",
      "Epoch 26/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8537 - accuracy: 0.6556 - val_loss: 0.7978 - val_accuracy: 0.6968\n",
      "Epoch 27/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8653 - accuracy: 0.6569 - val_loss: 0.8181 - val_accuracy: 0.6980\n",
      "Epoch 28/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.8324 - accuracy: 0.6680 - val_loss: 0.8139 - val_accuracy: 0.6826\n",
      "Epoch 29/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8536 - accuracy: 0.6529 - val_loss: 0.7881 - val_accuracy: 0.6927\n",
      "Epoch 30/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.8305 - accuracy: 0.6702 - val_loss: 0.8084 - val_accuracy: 0.6803\n",
      "Epoch 31/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8270 - accuracy: 0.6676 - val_loss: 0.8316 - val_accuracy: 0.6732\n",
      "Epoch 32/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8309 - accuracy: 0.6647 - val_loss: 0.7958 - val_accuracy: 0.6915\n",
      "Epoch 33/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.8281 - accuracy: 0.6645 - val_loss: 0.7902 - val_accuracy: 0.6980\n",
      "Epoch 34/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8155 - accuracy: 0.6659 - val_loss: 0.7842 - val_accuracy: 0.6832\n",
      "Epoch 35/100\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.8185 - accuracy: 0.6712 - val_loss: 0.7766 - val_accuracy: 0.7039\n",
      "Epoch 36/100\n",
      "106/106 [==============================] - 1s 8ms/step - loss: 0.8086 - accuracy: 0.6746 - val_loss: 0.7903 - val_accuracy: 0.7039\n",
      "Epoch 37/100\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 0.7915 - accuracy: 0.6823 - val_loss: 0.7853 - val_accuracy: 0.6950\n",
      "Epoch 38/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8087 - accuracy: 0.6773 - val_loss: 0.7521 - val_accuracy: 0.7216\n",
      "Epoch 39/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8024 - accuracy: 0.6772 - val_loss: 0.7810 - val_accuracy: 0.7098\n",
      "Epoch 40/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8091 - accuracy: 0.6736 - val_loss: 0.7938 - val_accuracy: 0.6939\n",
      "Epoch 41/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7926 - accuracy: 0.6814 - val_loss: 0.7908 - val_accuracy: 0.7039\n",
      "Epoch 42/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.8130 - accuracy: 0.6764 - val_loss: 0.8064 - val_accuracy: 0.6909\n",
      "Epoch 43/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7990 - accuracy: 0.6689 - val_loss: 0.7948 - val_accuracy: 0.6944\n",
      "Epoch 44/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7911 - accuracy: 0.6869 - val_loss: 0.7777 - val_accuracy: 0.7104\n",
      "Epoch 45/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7887 - accuracy: 0.6884 - val_loss: 0.7913 - val_accuracy: 0.7033\n",
      "Epoch 46/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7744 - accuracy: 0.6950 - val_loss: 0.7664 - val_accuracy: 0.7122\n",
      "Epoch 47/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7809 - accuracy: 0.6878 - val_loss: 0.7764 - val_accuracy: 0.7045\n",
      "Epoch 48/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7827 - accuracy: 0.6851 - val_loss: 0.7789 - val_accuracy: 0.7092\n",
      "Epoch 49/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7805 - accuracy: 0.6943 - val_loss: 0.7708 - val_accuracy: 0.6998\n",
      "Epoch 50/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7809 - accuracy: 0.6859 - val_loss: 0.7800 - val_accuracy: 0.7045\n",
      "Epoch 51/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7976 - accuracy: 0.6838 - val_loss: 0.7692 - val_accuracy: 0.7092\n",
      "Epoch 52/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7823 - accuracy: 0.6928 - val_loss: 0.7858 - val_accuracy: 0.6856\n",
      "Epoch 53/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7700 - accuracy: 0.6980 - val_loss: 0.7931 - val_accuracy: 0.6927\n",
      "Epoch 54/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7719 - accuracy: 0.6955 - val_loss: 0.7710 - val_accuracy: 0.7027\n",
      "Epoch 55/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.7673 - accuracy: 0.6990 - val_loss: 0.7611 - val_accuracy: 0.7051\n",
      "Epoch 56/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7900 - accuracy: 0.6848 - val_loss: 0.7669 - val_accuracy: 0.7009\n",
      "Epoch 57/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.7712 - accuracy: 0.6930 - val_loss: 0.7746 - val_accuracy: 0.7033\n",
      "Epoch 58/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.7667 - accuracy: 0.6934 - val_loss: 0.7708 - val_accuracy: 0.7074\n",
      "Epoch 59/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7711 - accuracy: 0.6921 - val_loss: 0.7698 - val_accuracy: 0.7086\n",
      "Epoch 60/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7736 - accuracy: 0.6884 - val_loss: 0.7607 - val_accuracy: 0.7181\n",
      "Epoch 61/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7623 - accuracy: 0.6965 - val_loss: 0.7542 - val_accuracy: 0.7098\n",
      "Epoch 62/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7548 - accuracy: 0.7017 - val_loss: 0.7444 - val_accuracy: 0.7157\n",
      "Epoch 63/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7737 - accuracy: 0.6959 - val_loss: 0.7416 - val_accuracy: 0.7275\n",
      "Epoch 64/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7614 - accuracy: 0.6890 - val_loss: 0.7497 - val_accuracy: 0.7157\n",
      "Epoch 65/100\n",
      "106/106 [==============================] - 1s 13ms/step - loss: 0.7601 - accuracy: 0.6993 - val_loss: 0.7628 - val_accuracy: 0.6980\n",
      "Epoch 66/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.7513 - accuracy: 0.7045 - val_loss: 0.7400 - val_accuracy: 0.7181\n",
      "Epoch 67/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7606 - accuracy: 0.6950 - val_loss: 0.7510 - val_accuracy: 0.7098\n",
      "Epoch 68/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7675 - accuracy: 0.6912 - val_loss: 0.7625 - val_accuracy: 0.6998\n",
      "Epoch 69/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7371 - accuracy: 0.7045 - val_loss: 0.7714 - val_accuracy: 0.6950\n",
      "Epoch 70/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7482 - accuracy: 0.7029 - val_loss: 0.7617 - val_accuracy: 0.7039\n",
      "Epoch 71/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.7535 - accuracy: 0.7004 - val_loss: 0.7489 - val_accuracy: 0.7169\n",
      "Epoch 72/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7603 - accuracy: 0.6947 - val_loss: 0.7588 - val_accuracy: 0.7252\n",
      "Epoch 73/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7531 - accuracy: 0.6984 - val_loss: 0.7537 - val_accuracy: 0.7145\n",
      "Epoch 74/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.7348 - accuracy: 0.7076 - val_loss: 0.7546 - val_accuracy: 0.7074\n",
      "Epoch 75/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.7487 - accuracy: 0.7009 - val_loss: 0.7821 - val_accuracy: 0.6950\n",
      "Epoch 76/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7570 - accuracy: 0.6989 - val_loss: 0.7414 - val_accuracy: 0.7193\n",
      "Epoch 77/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7437 - accuracy: 0.7067 - val_loss: 0.7511 - val_accuracy: 0.7187\n",
      "Epoch 78/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7422 - accuracy: 0.7036 - val_loss: 0.7354 - val_accuracy: 0.7139\n",
      "Epoch 79/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7556 - accuracy: 0.7009 - val_loss: 0.7474 - val_accuracy: 0.7216\n",
      "Epoch 80/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7371 - accuracy: 0.7048 - val_loss: 0.7493 - val_accuracy: 0.7145\n",
      "Epoch 81/100\n",
      "106/106 [==============================] - 1s 9ms/step - loss: 0.7528 - accuracy: 0.7070 - val_loss: 0.7472 - val_accuracy: 0.7193\n",
      "Epoch 82/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.7489 - accuracy: 0.7054 - val_loss: 0.7386 - val_accuracy: 0.7281\n",
      "Epoch 83/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.7552 - accuracy: 0.6998 - val_loss: 0.7404 - val_accuracy: 0.7193\n",
      "Epoch 84/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.7394 - accuracy: 0.7128 - val_loss: 0.7460 - val_accuracy: 0.7086\n",
      "Epoch 85/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7429 - accuracy: 0.7035 - val_loss: 0.7459 - val_accuracy: 0.7193\n",
      "Epoch 86/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.7415 - accuracy: 0.7024 - val_loss: 0.7378 - val_accuracy: 0.7275\n",
      "Epoch 87/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.7302 - accuracy: 0.7045 - val_loss: 0.7577 - val_accuracy: 0.7163\n",
      "Epoch 88/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7554 - accuracy: 0.7002 - val_loss: 0.7554 - val_accuracy: 0.7104\n",
      "Epoch 89/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.7396 - accuracy: 0.7063 - val_loss: 0.7444 - val_accuracy: 0.7116\n",
      "Epoch 90/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7288 - accuracy: 0.7097 - val_loss: 0.7528 - val_accuracy: 0.7134\n",
      "Epoch 91/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.7282 - accuracy: 0.7052 - val_loss: 0.7505 - val_accuracy: 0.7116\n",
      "Epoch 92/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.7388 - accuracy: 0.7103 - val_loss: 0.7509 - val_accuracy: 0.7027\n",
      "Epoch 93/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7415 - accuracy: 0.7094 - val_loss: 0.7348 - val_accuracy: 0.7210\n",
      "Epoch 94/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7383 - accuracy: 0.7055 - val_loss: 0.7356 - val_accuracy: 0.7134\n",
      "Epoch 95/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7371 - accuracy: 0.7070 - val_loss: 0.7288 - val_accuracy: 0.7264\n",
      "Epoch 96/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7379 - accuracy: 0.7049 - val_loss: 0.7433 - val_accuracy: 0.7210\n",
      "Epoch 97/100\n",
      "106/106 [==============================] - 1s 10ms/step - loss: 0.7250 - accuracy: 0.7128 - val_loss: 0.7490 - val_accuracy: 0.7086\n",
      "Epoch 98/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.7383 - accuracy: 0.7061 - val_loss: 0.7316 - val_accuracy: 0.7252\n",
      "Epoch 99/100\n",
      "106/106 [==============================] - 1s 12ms/step - loss: 0.7258 - accuracy: 0.7120 - val_loss: 0.7382 - val_accuracy: 0.7181\n",
      "Epoch 100/100\n",
      "106/106 [==============================] - 1s 11ms/step - loss: 0.7414 - accuracy: 0.7080 - val_loss: 0.7446 - val_accuracy: 0.7216\n"
     ]
    }
   ],
   "source": [
    "cnn_subject_model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=cnn_subject_model_optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "cnn_subject_model_results = cnn_subject_model.fit(x_train,\n",
    "             y_train,\n",
    "             batch_size=64,\n",
    "             epochs=epochs,\n",
    "             validation_data=(x_valid, y_valid), verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of the CNN model for subject 0: 0.7049999833106995\n"
     ]
    }
   ],
   "source": [
    "cnn_subject_model_score = cnn_subject_model.evaluate(x_test, y_test, verbose=False)\n",
    "print(f'Test accuracy of the CNN model for subject {subject}:',cnn_subject_model_score[1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Optimize the classification accuracy across all subjects. How does the classifier do? Do you notice any interesting trends?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_subjects(subject):\n",
    "    X_test = np.load(\"X_test.npy\")\n",
    "    y_test = np.load(\"y_test.npy\")\n",
    "    person_train_valid = np.load(\"person_train_valid.npy\")\n",
    "    X_train_valid = np.load(\"X_train_valid.npy\")\n",
    "    y_train_valid = np.load(\"y_train_valid.npy\")\n",
    "    person_test = np.load(\"person_test.npy\")\n",
    "\n",
    "    ## Adjusting the labels so that \n",
    "\n",
    "    # Cue onset left - 0\n",
    "    # Cue onset right - 1\n",
    "    # Cue onset foot - 2\n",
    "    # Cue onset tongue - 3\n",
    "\n",
    "    y_train_valid -= 769\n",
    "    y_test -= 769\n",
    "\n",
    "\n",
    "    subject_test_idx = np.where(person_test==subject)[0]\n",
    "    subject_valid_idx = np.where(person_train_valid==subject)[0]\n",
    "\n",
    "\n",
    "    subject_X_test = X_test[subject_test_idx]\n",
    "    suject_y_test = y_test[subject_test_idx]\n",
    "    suject_X_train_valid = X_train_valid[subject_valid_idx]\n",
    "    suject_y_train_valid = y_train_valid[subject_valid_idx]\n",
    "\n",
    "    # print(f'X_test Shape for Subject {subject}: {subject_X_test.shape}')\n",
    "    # print(f'y_test Shape for Subject {subject}: {suject_y_test.shape}')\n",
    "    # print(f'X_train_valid Shape for Subject {subject}: {suject_X_train_valid.shape}')\n",
    "    # print(f'y_train_valid Shape for Subject {subject}: {suject_y_train_valid.shape}')\n",
    "\n",
    "    # shuffle with 5 fold\n",
    "    indicies_valid = np.random.choice(suject_X_train_valid.shape[0], suject_X_train_valid.shape[0] // 5, replace=False)\n",
    "    indicies_train = np.array(list(set(range(suject_X_train_valid.shape[0])).difference(set(indicies_valid))))\n",
    "\n",
    "    # Creating the training and validation sets using the generated indices\n",
    "    X_train, X_valid = suject_X_train_valid[indicies_train], suject_X_train_valid[indicies_valid] \n",
    "    y_train, y_valid = suject_y_train_valid[indicies_train], suject_y_train_valid[indicies_valid]\n",
    "\n",
    "\n",
    "    # Preprocessing the dataset\n",
    "    x_train,y_train = data_prep(X_train,y_train,2,2,True)\n",
    "    x_valid,y_valid = data_prep(X_valid,y_valid,2,2,True)\n",
    "    X_test_prep,y_test_prep = data_prep(subject_X_test,suject_y_test,2,2,True)\n",
    "\n",
    "\n",
    "\n",
    "    # print('Shape of training set:',x_train.shape)\n",
    "    # print('Shape of validation set:',x_valid.shape)\n",
    "    # print('Shape of training labels:',y_train.shape)\n",
    "    # print('Shape of validation labels:',y_valid.shape)\n",
    "    # print('Shape of testing set:',X_test_prep.shape)\n",
    "    # print('Shape of testing labels:',y_test_prep.shape)\n",
    "\n",
    "\n",
    "    # Converting the labels to categorical variables for multiclass classification\n",
    "    y_train = to_categorical(y_train, 4)\n",
    "    y_valid = to_categorical(y_valid, 4)\n",
    "    y_test = to_categorical(y_test_prep, 4)\n",
    "    # print('Shape of training labels after categorical conversion:',y_train.shape)\n",
    "    # print('Shape of validation labels after categorical conversion:',y_valid.shape)\n",
    "    # print('Shape of test labels after categorical conversion:',y_test.shape)\n",
    "\n",
    "    # Adding width of the segment to be 1\n",
    "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "    x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)\n",
    "    x_test = X_test_prep.reshape(X_test_prep.shape[0], X_test_prep.shape[1], X_test_prep.shape[2], 1)\n",
    "    # print('Shape of training set after adding width info:',x_train.shape)\n",
    "    # print('Shape of validation set after adding width info:',x_valid.shape)\n",
    "    # print('Shape of test set after adding width info:',x_test.shape)\n",
    "\n",
    "\n",
    "    # Reshaping the training and validation dataset\n",
    "    x_train = np.swapaxes(x_train, 1,3)\n",
    "    x_train = np.swapaxes(x_train, 1,2)\n",
    "    x_valid = np.swapaxes(x_valid, 1,3)\n",
    "    x_valid = np.swapaxes(x_valid, 1,2)\n",
    "    x_test = np.swapaxes(x_test, 1,3)\n",
    "    x_test = np.swapaxes(x_test, 1,2)\n",
    "    # print('Shape of training set after dimension reshaping:',x_train.shape)\n",
    "    # print('Shape of validation set after dimension reshaping:',x_valid.shape)\n",
    "    # print('Shape of test set after dimension reshaping:',x_test.shape)\n",
    "\n",
    "    return (x_train, y_train, x_valid, y_valid, x_test, y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 250, 1, 10)        1110      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 84, 1, 10)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 84, 1, 10)        40        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 84, 1, 10)         0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 84, 1, 10)         1510      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 28, 1, 10)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 28, 1, 10)        40        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 28, 1, 10)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 280)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 1124      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,824\n",
      "Trainable params: 3,784\n",
      "Non-trainable params: 40\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Building the CNN model using sequential class\n",
    "cnn_subject_model = Sequential()\n",
    "\n",
    "# Conv. block 1\n",
    "cnn_subject_model.add(Conv2D(filters=10, kernel_size=(5,1), padding='same', activation='elu', input_shape=(250,1,22)))\n",
    "cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same')) \n",
    "cnn_subject_model.add(BatchNormalization())\n",
    "cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "# Conv. block 2\n",
    "cnn_subject_model.add(Conv2D(filters=10, kernel_size=(15,1), padding='same', activation='elu'))\n",
    "cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "cnn_subject_model.add(BatchNormalization())\n",
    "cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "\n",
    "# # Conv. block 4\n",
    "# cnn_subject_model.add(Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "# cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "# cnn_subject_model.add(BatchNormalization())\n",
    "# cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "# # Conv. block 5\n",
    "# cnn_subject_model.add(Conv2D(filters=100, kernel_size=(50,1), padding='same', activation='elu'))\n",
    "# cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "# cnn_subject_model.add(BatchNormalization())\n",
    "# cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "# # Conv. block 6\n",
    "# cnn_subject_model.add(Conv2D(filters=50, kernel_size=(50,1), padding='same', activation='elu'))\n",
    "# cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "# cnn_subject_model.add(BatchNormalization())\n",
    "# cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "# # Conv. block 7\n",
    "# cnn_subject_model.add(Conv2D(filters=25, kernel_size=(50,1), padding='same', activation='elu'))\n",
    "# cnn_subject_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "# cnn_subject_model.add(BatchNormalization())\n",
    "# cnn_subject_model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer with Softmax activation\n",
    "cnn_subject_model.add(Flatten()) # Flattens the input\n",
    "cnn_subject_model.add(Dense(4, activation='softmax')) # Output FC layer with softmax activation\n",
    "\n",
    "\n",
    "# Printing the model summary\n",
    "cnn_subject_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training / Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X after trimming: (190, 22, 500)\n",
      "Shape of X after maxpooling: (190, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (380, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (760, 22, 250)\n",
      "Shape of X after trimming: (47, 22, 500)\n",
      "Shape of X after maxpooling: (47, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (94, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (188, 22, 250)\n",
      "Shape of X after trimming: (50, 22, 500)\n",
      "Shape of X after maxpooling: (50, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (100, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (200, 22, 250)\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 20ms/step - loss: 2.2608 - accuracy: 0.2421 - val_loss: 3.8544 - val_accuracy: 0.1809\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 2.0252 - accuracy: 0.3066 - val_loss: 2.7890 - val_accuracy: 0.2287\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.7620 - accuracy: 0.3526 - val_loss: 2.3857 - val_accuracy: 0.2660\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 1.6310 - accuracy: 0.3842 - val_loss: 2.1062 - val_accuracy: 0.2926\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 1.5334 - accuracy: 0.4079 - val_loss: 1.8671 - val_accuracy: 0.3457\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.5021 - accuracy: 0.4224 - val_loss: 1.6769 - val_accuracy: 0.3617\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.3855 - accuracy: 0.4671 - val_loss: 1.5236 - val_accuracy: 0.3830\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.3233 - accuracy: 0.4737 - val_loss: 1.4998 - val_accuracy: 0.3830\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.3846 - accuracy: 0.4697 - val_loss: 1.4044 - val_accuracy: 0.3883\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.2286 - accuracy: 0.5053 - val_loss: 1.4378 - val_accuracy: 0.3351\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 1.1827 - accuracy: 0.5316 - val_loss: 1.3506 - val_accuracy: 0.3883\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.0994 - accuracy: 0.5684 - val_loss: 1.2993 - val_accuracy: 0.4202\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 1.0441 - accuracy: 0.5618 - val_loss: 1.2278 - val_accuracy: 0.4628\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.0636 - accuracy: 0.5868 - val_loss: 1.1994 - val_accuracy: 0.4681\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.9542 - accuracy: 0.6039 - val_loss: 1.1935 - val_accuracy: 0.4840\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.9053 - accuracy: 0.6066 - val_loss: 1.1481 - val_accuracy: 0.4840\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.9266 - accuracy: 0.6211 - val_loss: 1.1085 - val_accuracy: 0.5106\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.8387 - accuracy: 0.6592 - val_loss: 1.0938 - val_accuracy: 0.5000\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.8024 - accuracy: 0.6658 - val_loss: 1.0819 - val_accuracy: 0.5266\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.7337 - accuracy: 0.6947 - val_loss: 1.0661 - val_accuracy: 0.4947\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.7391 - accuracy: 0.7145 - val_loss: 1.0061 - val_accuracy: 0.5426\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.7017 - accuracy: 0.7145 - val_loss: 1.0412 - val_accuracy: 0.5851\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.6994 - accuracy: 0.7066 - val_loss: 0.9599 - val_accuracy: 0.5691\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.6932 - accuracy: 0.7316 - val_loss: 1.0107 - val_accuracy: 0.5479\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.6350 - accuracy: 0.7500 - val_loss: 0.9729 - val_accuracy: 0.5638\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5958 - accuracy: 0.7684 - val_loss: 0.9770 - val_accuracy: 0.5691\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5768 - accuracy: 0.7684 - val_loss: 0.9340 - val_accuracy: 0.5904\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5654 - accuracy: 0.7737 - val_loss: 0.9629 - val_accuracy: 0.6011\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4939 - accuracy: 0.8066 - val_loss: 0.9744 - val_accuracy: 0.5745\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5394 - accuracy: 0.7711 - val_loss: 0.9781 - val_accuracy: 0.5851\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4974 - accuracy: 0.8224 - val_loss: 0.9219 - val_accuracy: 0.6383\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5042 - accuracy: 0.8105 - val_loss: 0.9350 - val_accuracy: 0.6383\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4797 - accuracy: 0.8224 - val_loss: 0.9343 - val_accuracy: 0.6489\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4952 - accuracy: 0.7947 - val_loss: 0.8784 - val_accuracy: 0.6596\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4763 - accuracy: 0.8316 - val_loss: 0.9162 - val_accuracy: 0.6596\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4591 - accuracy: 0.8118 - val_loss: 0.9288 - val_accuracy: 0.6702\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4675 - accuracy: 0.8092 - val_loss: 0.8956 - val_accuracy: 0.6170\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4208 - accuracy: 0.8342 - val_loss: 0.8661 - val_accuracy: 0.6915\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4195 - accuracy: 0.8368 - val_loss: 0.9154 - val_accuracy: 0.6755\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4178 - accuracy: 0.8474 - val_loss: 0.9492 - val_accuracy: 0.6489\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4184 - accuracy: 0.8408 - val_loss: 0.8980 - val_accuracy: 0.7021\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3791 - accuracy: 0.8526 - val_loss: 0.9294 - val_accuracy: 0.6862\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3735 - accuracy: 0.8500 - val_loss: 0.9310 - val_accuracy: 0.6702\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3636 - accuracy: 0.8711 - val_loss: 0.9080 - val_accuracy: 0.6649\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3876 - accuracy: 0.8539 - val_loss: 0.9079 - val_accuracy: 0.6755\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3028 - accuracy: 0.8829 - val_loss: 0.9535 - val_accuracy: 0.6809\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3593 - accuracy: 0.8658 - val_loss: 0.9459 - val_accuracy: 0.6809\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3576 - accuracy: 0.8697 - val_loss: 0.9236 - val_accuracy: 0.6862\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3516 - accuracy: 0.8579 - val_loss: 0.9251 - val_accuracy: 0.7021\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3330 - accuracy: 0.8776 - val_loss: 0.8438 - val_accuracy: 0.7021\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3291 - accuracy: 0.8882 - val_loss: 0.8662 - val_accuracy: 0.6862\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2888 - accuracy: 0.8842 - val_loss: 0.9194 - val_accuracy: 0.6809\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2965 - accuracy: 0.8961 - val_loss: 0.9258 - val_accuracy: 0.6596\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.2934 - accuracy: 0.8868 - val_loss: 0.8423 - val_accuracy: 0.7074\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.2744 - accuracy: 0.9013 - val_loss: 0.8636 - val_accuracy: 0.7234\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2972 - accuracy: 0.8803 - val_loss: 0.8921 - val_accuracy: 0.7074\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.2947 - accuracy: 0.8829 - val_loss: 0.8316 - val_accuracy: 0.7128\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.2879 - accuracy: 0.8947 - val_loss: 0.8861 - val_accuracy: 0.7074\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.2854 - accuracy: 0.9000 - val_loss: 0.8953 - val_accuracy: 0.6862\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3127 - accuracy: 0.8868 - val_loss: 0.8786 - val_accuracy: 0.7074\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2965 - accuracy: 0.8908 - val_loss: 0.9122 - val_accuracy: 0.7074\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2882 - accuracy: 0.9039 - val_loss: 0.8704 - val_accuracy: 0.6968\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2756 - accuracy: 0.8921 - val_loss: 0.9399 - val_accuracy: 0.6809\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2296 - accuracy: 0.9224 - val_loss: 0.8973 - val_accuracy: 0.7128\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2663 - accuracy: 0.9039 - val_loss: 0.8960 - val_accuracy: 0.7021\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2087 - accuracy: 0.9237 - val_loss: 0.8882 - val_accuracy: 0.6968\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2186 - accuracy: 0.9237 - val_loss: 0.9003 - val_accuracy: 0.6915\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2478 - accuracy: 0.9118 - val_loss: 0.8904 - val_accuracy: 0.6755\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2330 - accuracy: 0.9132 - val_loss: 0.8926 - val_accuracy: 0.6968\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2363 - accuracy: 0.9145 - val_loss: 0.8762 - val_accuracy: 0.7074\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2144 - accuracy: 0.9211 - val_loss: 0.8392 - val_accuracy: 0.7074\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2277 - accuracy: 0.9224 - val_loss: 0.9048 - val_accuracy: 0.6862\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2210 - accuracy: 0.9092 - val_loss: 0.9097 - val_accuracy: 0.6862\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2326 - accuracy: 0.9105 - val_loss: 0.8395 - val_accuracy: 0.6862\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2192 - accuracy: 0.9224 - val_loss: 0.8532 - val_accuracy: 0.7128\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2276 - accuracy: 0.9184 - val_loss: 0.8972 - val_accuracy: 0.7074\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2171 - accuracy: 0.9145 - val_loss: 0.9081 - val_accuracy: 0.7181\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2230 - accuracy: 0.9237 - val_loss: 0.8985 - val_accuracy: 0.6862\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2301 - accuracy: 0.9171 - val_loss: 0.8859 - val_accuracy: 0.6862\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2022 - accuracy: 0.9211 - val_loss: 0.8776 - val_accuracy: 0.7074\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2188 - accuracy: 0.9197 - val_loss: 0.8446 - val_accuracy: 0.7181\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2085 - accuracy: 0.9197 - val_loss: 0.7903 - val_accuracy: 0.7660\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1973 - accuracy: 0.9211 - val_loss: 0.7788 - val_accuracy: 0.7500\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1757 - accuracy: 0.9474 - val_loss: 0.8298 - val_accuracy: 0.7234\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1978 - accuracy: 0.9316 - val_loss: 0.7902 - val_accuracy: 0.7447\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1874 - accuracy: 0.9316 - val_loss: 0.8131 - val_accuracy: 0.7340\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2030 - accuracy: 0.9303 - val_loss: 0.8759 - val_accuracy: 0.6915\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1718 - accuracy: 0.9355 - val_loss: 0.8119 - val_accuracy: 0.7234\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1894 - accuracy: 0.9276 - val_loss: 0.8061 - val_accuracy: 0.7447\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2032 - accuracy: 0.9211 - val_loss: 0.7953 - val_accuracy: 0.7340\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2006 - accuracy: 0.9316 - val_loss: 0.8132 - val_accuracy: 0.7181\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1555 - accuracy: 0.9474 - val_loss: 0.8495 - val_accuracy: 0.7074\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1690 - accuracy: 0.9368 - val_loss: 0.8513 - val_accuracy: 0.7128\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.2057 - accuracy: 0.9263 - val_loss: 0.8436 - val_accuracy: 0.7394\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1951 - accuracy: 0.9434 - val_loss: 0.7739 - val_accuracy: 0.7553\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1410 - accuracy: 0.9487 - val_loss: 0.8048 - val_accuracy: 0.7021\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2096 - accuracy: 0.9211 - val_loss: 0.8910 - val_accuracy: 0.7021\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1923 - accuracy: 0.9250 - val_loss: 0.8204 - val_accuracy: 0.7234\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1550 - accuracy: 0.9434 - val_loss: 0.7881 - val_accuracy: 0.7606\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1495 - accuracy: 0.9500 - val_loss: 0.8973 - val_accuracy: 0.7234\n",
      "Test accuracy of the CNN model for subject 0: 0.7099999785423279\n",
      "Shape of X after trimming: (189, 22, 500)\n",
      "Shape of X after maxpooling: (189, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (378, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (756, 22, 250)\n",
      "Shape of X after trimming: (47, 22, 500)\n",
      "Shape of X after maxpooling: (47, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (94, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (188, 22, 250)\n",
      "Shape of X after trimming: (50, 22, 500)\n",
      "Shape of X after maxpooling: (50, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (100, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (200, 22, 250)\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 18ms/step - loss: 2.7795 - accuracy: 0.3426 - val_loss: 2.1800 - val_accuracy: 0.2766\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 2.3468 - accuracy: 0.3730 - val_loss: 1.7827 - val_accuracy: 0.3138\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 1.9413 - accuracy: 0.4312 - val_loss: 1.6574 - val_accuracy: 0.3245\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.7405 - accuracy: 0.4392 - val_loss: 1.5697 - val_accuracy: 0.3404\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.6860 - accuracy: 0.4365 - val_loss: 1.5683 - val_accuracy: 0.3511\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.5249 - accuracy: 0.4894 - val_loss: 1.5138 - val_accuracy: 0.3564\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.4292 - accuracy: 0.4974 - val_loss: 1.4146 - val_accuracy: 0.3989\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.2470 - accuracy: 0.5384 - val_loss: 1.3531 - val_accuracy: 0.4043\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.1331 - accuracy: 0.5886 - val_loss: 1.2261 - val_accuracy: 0.4734\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.1113 - accuracy: 0.6019 - val_loss: 1.2932 - val_accuracy: 0.4521\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.1164 - accuracy: 0.5701 - val_loss: 1.3419 - val_accuracy: 0.4415\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.0251 - accuracy: 0.5952 - val_loss: 1.2715 - val_accuracy: 0.4202\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.9779 - accuracy: 0.6310 - val_loss: 1.2676 - val_accuracy: 0.4255\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.9165 - accuracy: 0.6455 - val_loss: 1.3165 - val_accuracy: 0.4255\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.9171 - accuracy: 0.6442 - val_loss: 1.2143 - val_accuracy: 0.4840\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.8295 - accuracy: 0.6892 - val_loss: 1.2221 - val_accuracy: 0.4468\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.8141 - accuracy: 0.6706 - val_loss: 1.2202 - val_accuracy: 0.4362\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.8142 - accuracy: 0.6865 - val_loss: 1.2282 - val_accuracy: 0.4574\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.8082 - accuracy: 0.6772 - val_loss: 1.2455 - val_accuracy: 0.4468\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.7144 - accuracy: 0.7130 - val_loss: 1.2100 - val_accuracy: 0.4681\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.6921 - accuracy: 0.7249 - val_loss: 1.2163 - val_accuracy: 0.4521\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.7086 - accuracy: 0.7235 - val_loss: 1.2196 - val_accuracy: 0.4574\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.7104 - accuracy: 0.7235 - val_loss: 1.2002 - val_accuracy: 0.4734\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.6952 - accuracy: 0.7116 - val_loss: 1.1838 - val_accuracy: 0.4468\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.6711 - accuracy: 0.7407 - val_loss: 1.1876 - val_accuracy: 0.4574\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.6364 - accuracy: 0.7421 - val_loss: 1.1824 - val_accuracy: 0.4947\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.6335 - accuracy: 0.7632 - val_loss: 1.1452 - val_accuracy: 0.4947\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.6392 - accuracy: 0.7487 - val_loss: 1.1908 - val_accuracy: 0.4787\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.6123 - accuracy: 0.7619 - val_loss: 1.1643 - val_accuracy: 0.4574\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.6249 - accuracy: 0.7526 - val_loss: 1.1255 - val_accuracy: 0.4255\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5552 - accuracy: 0.7712 - val_loss: 1.1150 - val_accuracy: 0.4468\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5784 - accuracy: 0.7672 - val_loss: 1.1337 - val_accuracy: 0.4521\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.6479 - accuracy: 0.7566 - val_loss: 1.1375 - val_accuracy: 0.4574\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5698 - accuracy: 0.7751 - val_loss: 1.2078 - val_accuracy: 0.4309\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5415 - accuracy: 0.7884 - val_loss: 1.2275 - val_accuracy: 0.4521\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.5297 - accuracy: 0.8095 - val_loss: 1.2234 - val_accuracy: 0.4415\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5844 - accuracy: 0.7817 - val_loss: 1.1605 - val_accuracy: 0.4894\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5627 - accuracy: 0.7712 - val_loss: 1.2176 - val_accuracy: 0.4628\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5291 - accuracy: 0.8042 - val_loss: 1.1295 - val_accuracy: 0.4628\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.5190 - accuracy: 0.7897 - val_loss: 1.2071 - val_accuracy: 0.4628\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.5516 - accuracy: 0.7725 - val_loss: 1.1093 - val_accuracy: 0.4787\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4967 - accuracy: 0.7963 - val_loss: 1.1548 - val_accuracy: 0.4574\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5330 - accuracy: 0.7897 - val_loss: 1.0821 - val_accuracy: 0.4787\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4868 - accuracy: 0.8082 - val_loss: 1.0764 - val_accuracy: 0.4840\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4516 - accuracy: 0.8267 - val_loss: 1.1765 - val_accuracy: 0.4521\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4394 - accuracy: 0.8307 - val_loss: 1.1049 - val_accuracy: 0.4415\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4424 - accuracy: 0.8307 - val_loss: 1.0617 - val_accuracy: 0.4468\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4853 - accuracy: 0.8175 - val_loss: 1.1226 - val_accuracy: 0.4787\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4390 - accuracy: 0.8466 - val_loss: 1.1133 - val_accuracy: 0.4628\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.4533 - accuracy: 0.8201 - val_loss: 1.1086 - val_accuracy: 0.4574\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4870 - accuracy: 0.8042 - val_loss: 1.1013 - val_accuracy: 0.4255\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4991 - accuracy: 0.8003 - val_loss: 1.0919 - val_accuracy: 0.4415\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.4092 - accuracy: 0.8413 - val_loss: 1.0890 - val_accuracy: 0.4947\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4075 - accuracy: 0.8505 - val_loss: 1.0739 - val_accuracy: 0.5160\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4330 - accuracy: 0.8241 - val_loss: 1.0427 - val_accuracy: 0.4574\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.4652 - accuracy: 0.8135 - val_loss: 1.1156 - val_accuracy: 0.4574\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4225 - accuracy: 0.8399 - val_loss: 1.1252 - val_accuracy: 0.4628\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4010 - accuracy: 0.8386 - val_loss: 1.0633 - val_accuracy: 0.4734\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4260 - accuracy: 0.8347 - val_loss: 1.0493 - val_accuracy: 0.5106\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3904 - accuracy: 0.8624 - val_loss: 1.0691 - val_accuracy: 0.4681\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4308 - accuracy: 0.8347 - val_loss: 1.0248 - val_accuracy: 0.5160\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3940 - accuracy: 0.8466 - val_loss: 1.0542 - val_accuracy: 0.4734\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3898 - accuracy: 0.8426 - val_loss: 1.0382 - val_accuracy: 0.4628\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3715 - accuracy: 0.8757 - val_loss: 1.0686 - val_accuracy: 0.5000\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3922 - accuracy: 0.8519 - val_loss: 1.0265 - val_accuracy: 0.4840\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4298 - accuracy: 0.8439 - val_loss: 1.1542 - val_accuracy: 0.4468\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4189 - accuracy: 0.8439 - val_loss: 1.0676 - val_accuracy: 0.4947\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3705 - accuracy: 0.8638 - val_loss: 1.0324 - val_accuracy: 0.4787\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3987 - accuracy: 0.8452 - val_loss: 1.1381 - val_accuracy: 0.4681\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3540 - accuracy: 0.8690 - val_loss: 1.0611 - val_accuracy: 0.4574\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3576 - accuracy: 0.8585 - val_loss: 1.1134 - val_accuracy: 0.4574\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3552 - accuracy: 0.8704 - val_loss: 1.1220 - val_accuracy: 0.4521\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3181 - accuracy: 0.8783 - val_loss: 1.1844 - val_accuracy: 0.4681\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3798 - accuracy: 0.8479 - val_loss: 1.0593 - val_accuracy: 0.4840\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3460 - accuracy: 0.8876 - val_loss: 1.1552 - val_accuracy: 0.4947\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3481 - accuracy: 0.8690 - val_loss: 1.1067 - val_accuracy: 0.4734\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.3123 - accuracy: 0.8849 - val_loss: 1.1493 - val_accuracy: 0.4415\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.3626 - accuracy: 0.8585 - val_loss: 1.1016 - val_accuracy: 0.4894\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3629 - accuracy: 0.8624 - val_loss: 1.1614 - val_accuracy: 0.5000\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3486 - accuracy: 0.8677 - val_loss: 1.0527 - val_accuracy: 0.4894\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3453 - accuracy: 0.8704 - val_loss: 1.1627 - val_accuracy: 0.4362\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2969 - accuracy: 0.8889 - val_loss: 1.0952 - val_accuracy: 0.4787\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3177 - accuracy: 0.8757 - val_loss: 1.1221 - val_accuracy: 0.5000\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3311 - accuracy: 0.8942 - val_loss: 1.1349 - val_accuracy: 0.4787\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2771 - accuracy: 0.9087 - val_loss: 1.0794 - val_accuracy: 0.5053\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3459 - accuracy: 0.8704 - val_loss: 1.1531 - val_accuracy: 0.4840\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3180 - accuracy: 0.8889 - val_loss: 1.1425 - val_accuracy: 0.4894\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3013 - accuracy: 0.8981 - val_loss: 1.0076 - val_accuracy: 0.5319\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3212 - accuracy: 0.8717 - val_loss: 1.1915 - val_accuracy: 0.4628\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3161 - accuracy: 0.8915 - val_loss: 1.0921 - val_accuracy: 0.5053\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2912 - accuracy: 0.8889 - val_loss: 1.0687 - val_accuracy: 0.5160\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2954 - accuracy: 0.8889 - val_loss: 1.1297 - val_accuracy: 0.5160\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3158 - accuracy: 0.8955 - val_loss: 1.1394 - val_accuracy: 0.5266\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2909 - accuracy: 0.8929 - val_loss: 1.0756 - val_accuracy: 0.5585\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3388 - accuracy: 0.8757 - val_loss: 1.0842 - val_accuracy: 0.5160\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3032 - accuracy: 0.8955 - val_loss: 1.1377 - val_accuracy: 0.5266\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2592 - accuracy: 0.9048 - val_loss: 1.0835 - val_accuracy: 0.5479\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2938 - accuracy: 0.8862 - val_loss: 1.1480 - val_accuracy: 0.5106\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2827 - accuracy: 0.8823 - val_loss: 1.1118 - val_accuracy: 0.5053\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2799 - accuracy: 0.9021 - val_loss: 1.1886 - val_accuracy: 0.5213\n",
      "Test accuracy of the CNN model for subject 1: 0.5849999785423279\n",
      "Shape of X after trimming: (189, 22, 500)\n",
      "Shape of X after maxpooling: (189, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (378, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (756, 22, 250)\n",
      "Shape of X after trimming: (47, 22, 500)\n",
      "Shape of X after maxpooling: (47, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (94, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (188, 22, 250)\n",
      "Shape of X after trimming: (50, 22, 500)\n",
      "Shape of X after maxpooling: (50, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (100, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (200, 22, 250)\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 18ms/step - loss: 2.0367 - accuracy: 0.4193 - val_loss: 1.8556 - val_accuracy: 0.4362\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.5664 - accuracy: 0.4669 - val_loss: 1.6034 - val_accuracy: 0.4894\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.4107 - accuracy: 0.5265 - val_loss: 1.4087 - val_accuracy: 0.5319\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.2193 - accuracy: 0.5767 - val_loss: 1.2746 - val_accuracy: 0.5745\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.1084 - accuracy: 0.6177 - val_loss: 1.1591 - val_accuracy: 0.6117\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.0343 - accuracy: 0.5939 - val_loss: 1.0947 - val_accuracy: 0.6383\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.8907 - accuracy: 0.6614 - val_loss: 1.0042 - val_accuracy: 0.6596\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.8263 - accuracy: 0.6997 - val_loss: 0.9618 - val_accuracy: 0.6755\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.7864 - accuracy: 0.6918 - val_loss: 0.9695 - val_accuracy: 0.6862\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.7698 - accuracy: 0.6772 - val_loss: 0.9344 - val_accuracy: 0.6702\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.7193 - accuracy: 0.7077 - val_loss: 0.9154 - val_accuracy: 0.7021\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.5951 - accuracy: 0.7831 - val_loss: 0.9052 - val_accuracy: 0.7021\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.6191 - accuracy: 0.7579 - val_loss: 0.8927 - val_accuracy: 0.7287\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5724 - accuracy: 0.7765 - val_loss: 0.9056 - val_accuracy: 0.6968\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5240 - accuracy: 0.8069 - val_loss: 0.9055 - val_accuracy: 0.7074\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5194 - accuracy: 0.8003 - val_loss: 0.8875 - val_accuracy: 0.7128\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5033 - accuracy: 0.7844 - val_loss: 0.8624 - val_accuracy: 0.7234\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.4789 - accuracy: 0.8042 - val_loss: 0.8373 - val_accuracy: 0.7234\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.4282 - accuracy: 0.8360 - val_loss: 0.8527 - val_accuracy: 0.7234\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4695 - accuracy: 0.8056 - val_loss: 0.8588 - val_accuracy: 0.7287\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4151 - accuracy: 0.8413 - val_loss: 0.8567 - val_accuracy: 0.7181\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4268 - accuracy: 0.8505 - val_loss: 0.8616 - val_accuracy: 0.7021\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4088 - accuracy: 0.8413 - val_loss: 0.8521 - val_accuracy: 0.7128\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3483 - accuracy: 0.8598 - val_loss: 0.8170 - val_accuracy: 0.7181\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3517 - accuracy: 0.8624 - val_loss: 0.8128 - val_accuracy: 0.7181\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3998 - accuracy: 0.8532 - val_loss: 0.8274 - val_accuracy: 0.7287\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3558 - accuracy: 0.8690 - val_loss: 0.8252 - val_accuracy: 0.7234\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3645 - accuracy: 0.8585 - val_loss: 0.8193 - val_accuracy: 0.7128\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.3100 - accuracy: 0.8823 - val_loss: 0.8157 - val_accuracy: 0.7181\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.3332 - accuracy: 0.8796 - val_loss: 0.7658 - val_accuracy: 0.7447\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3342 - accuracy: 0.8849 - val_loss: 0.7346 - val_accuracy: 0.7287\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3345 - accuracy: 0.8677 - val_loss: 0.7380 - val_accuracy: 0.7340\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3078 - accuracy: 0.8955 - val_loss: 0.7386 - val_accuracy: 0.7447\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3005 - accuracy: 0.8810 - val_loss: 0.7674 - val_accuracy: 0.7340\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2743 - accuracy: 0.9140 - val_loss: 0.7638 - val_accuracy: 0.7287\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2922 - accuracy: 0.8968 - val_loss: 0.7588 - val_accuracy: 0.7287\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3023 - accuracy: 0.8929 - val_loss: 0.7701 - val_accuracy: 0.7447\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2782 - accuracy: 0.8955 - val_loss: 0.7419 - val_accuracy: 0.7394\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3012 - accuracy: 0.8902 - val_loss: 0.7203 - val_accuracy: 0.7660\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2849 - accuracy: 0.8889 - val_loss: 0.7335 - val_accuracy: 0.7606\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2639 - accuracy: 0.9061 - val_loss: 0.7179 - val_accuracy: 0.7606\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2682 - accuracy: 0.8968 - val_loss: 0.7255 - val_accuracy: 0.7606\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2610 - accuracy: 0.8955 - val_loss: 0.6991 - val_accuracy: 0.7660\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2274 - accuracy: 0.9167 - val_loss: 0.7032 - val_accuracy: 0.7553\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2216 - accuracy: 0.9167 - val_loss: 0.6974 - val_accuracy: 0.7713\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2401 - accuracy: 0.9127 - val_loss: 0.6904 - val_accuracy: 0.7606\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2734 - accuracy: 0.8968 - val_loss: 0.7022 - val_accuracy: 0.7553\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2499 - accuracy: 0.9101 - val_loss: 0.7287 - val_accuracy: 0.7181\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2421 - accuracy: 0.9087 - val_loss: 0.6825 - val_accuracy: 0.7606\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2487 - accuracy: 0.9101 - val_loss: 0.6777 - val_accuracy: 0.7394\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1966 - accuracy: 0.9206 - val_loss: 0.6598 - val_accuracy: 0.7500\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2298 - accuracy: 0.9246 - val_loss: 0.6496 - val_accuracy: 0.7660\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2056 - accuracy: 0.9233 - val_loss: 0.6423 - val_accuracy: 0.7447\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2135 - accuracy: 0.9206 - val_loss: 0.6502 - val_accuracy: 0.7660\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2087 - accuracy: 0.9180 - val_loss: 0.6704 - val_accuracy: 0.7500\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1950 - accuracy: 0.9325 - val_loss: 0.6525 - val_accuracy: 0.7606\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2036 - accuracy: 0.9325 - val_loss: 0.6115 - val_accuracy: 0.7819\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.1917 - accuracy: 0.9220 - val_loss: 0.6464 - val_accuracy: 0.7766\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2106 - accuracy: 0.9259 - val_loss: 0.6580 - val_accuracy: 0.7500\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2308 - accuracy: 0.9114 - val_loss: 0.6124 - val_accuracy: 0.7660\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1779 - accuracy: 0.9444 - val_loss: 0.5994 - val_accuracy: 0.7660\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1940 - accuracy: 0.9206 - val_loss: 0.6291 - val_accuracy: 0.7766\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1940 - accuracy: 0.9233 - val_loss: 0.7018 - val_accuracy: 0.7394\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1883 - accuracy: 0.9339 - val_loss: 0.6782 - val_accuracy: 0.7553\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1822 - accuracy: 0.9339 - val_loss: 0.6471 - val_accuracy: 0.7766\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1495 - accuracy: 0.9471 - val_loss: 0.6774 - val_accuracy: 0.7660\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1697 - accuracy: 0.9365 - val_loss: 0.6392 - val_accuracy: 0.7660\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1753 - accuracy: 0.9312 - val_loss: 0.6012 - val_accuracy: 0.7713\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1433 - accuracy: 0.9418 - val_loss: 0.6329 - val_accuracy: 0.7713\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1891 - accuracy: 0.9220 - val_loss: 0.6181 - val_accuracy: 0.7819\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1846 - accuracy: 0.9339 - val_loss: 0.6155 - val_accuracy: 0.7872\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1972 - accuracy: 0.9246 - val_loss: 0.6183 - val_accuracy: 0.7819\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1469 - accuracy: 0.9471 - val_loss: 0.6328 - val_accuracy: 0.7713\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1492 - accuracy: 0.9444 - val_loss: 0.6065 - val_accuracy: 0.7713\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1447 - accuracy: 0.9471 - val_loss: 0.5838 - val_accuracy: 0.7979\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1566 - accuracy: 0.9418 - val_loss: 0.6398 - val_accuracy: 0.7713\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1581 - accuracy: 0.9511 - val_loss: 0.5961 - val_accuracy: 0.7713\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1512 - accuracy: 0.9471 - val_loss: 0.6202 - val_accuracy: 0.7872\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1557 - accuracy: 0.9392 - val_loss: 0.5695 - val_accuracy: 0.7979\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1918 - accuracy: 0.9339 - val_loss: 0.5780 - val_accuracy: 0.7872\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1475 - accuracy: 0.9418 - val_loss: 0.6220 - val_accuracy: 0.7660\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1517 - accuracy: 0.9458 - val_loss: 0.6440 - val_accuracy: 0.7660\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1689 - accuracy: 0.9392 - val_loss: 0.5653 - val_accuracy: 0.7979\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1614 - accuracy: 0.9365 - val_loss: 0.5491 - val_accuracy: 0.8032\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1425 - accuracy: 0.9471 - val_loss: 0.5404 - val_accuracy: 0.8191\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1287 - accuracy: 0.9458 - val_loss: 0.5380 - val_accuracy: 0.8138\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1321 - accuracy: 0.9550 - val_loss: 0.5304 - val_accuracy: 0.8085\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1280 - accuracy: 0.9643 - val_loss: 0.5584 - val_accuracy: 0.7926\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1306 - accuracy: 0.9497 - val_loss: 0.5500 - val_accuracy: 0.8351\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1505 - accuracy: 0.9392 - val_loss: 0.5658 - val_accuracy: 0.8191\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1458 - accuracy: 0.9484 - val_loss: 0.5541 - val_accuracy: 0.8245\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1524 - accuracy: 0.9444 - val_loss: 0.5440 - val_accuracy: 0.8298\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1327 - accuracy: 0.9524 - val_loss: 0.5493 - val_accuracy: 0.8032\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1367 - accuracy: 0.9484 - val_loss: 0.5767 - val_accuracy: 0.8085\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.1361 - accuracy: 0.9484 - val_loss: 0.5418 - val_accuracy: 0.8245\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1241 - accuracy: 0.9563 - val_loss: 0.5020 - val_accuracy: 0.8245\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1234 - accuracy: 0.9577 - val_loss: 0.4923 - val_accuracy: 0.8298\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.1376 - accuracy: 0.9497 - val_loss: 0.5057 - val_accuracy: 0.8191\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1630 - accuracy: 0.9365 - val_loss: 0.5198 - val_accuracy: 0.8085\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1400 - accuracy: 0.9484 - val_loss: 0.5195 - val_accuracy: 0.8032\n",
      "Test accuracy of the CNN model for subject 2: 0.800000011920929\n",
      "Shape of X after trimming: (188, 22, 500)\n",
      "Shape of X after maxpooling: (188, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (376, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (752, 22, 250)\n",
      "Shape of X after trimming: (46, 22, 500)\n",
      "Shape of X after maxpooling: (46, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (92, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (184, 22, 250)\n",
      "Shape of X after trimming: (50, 22, 500)\n",
      "Shape of X after maxpooling: (50, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (100, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (200, 22, 250)\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 18ms/step - loss: 2.4069 - accuracy: 0.4335 - val_loss: 2.3173 - val_accuracy: 0.4293\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 1.9612 - accuracy: 0.5040 - val_loss: 1.9748 - val_accuracy: 0.4891\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.5550 - accuracy: 0.5505 - val_loss: 1.8511 - val_accuracy: 0.4511\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 1.4396 - accuracy: 0.5798 - val_loss: 1.9225 - val_accuracy: 0.4891\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.2574 - accuracy: 0.5957 - val_loss: 1.5658 - val_accuracy: 0.4891\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.1245 - accuracy: 0.6210 - val_loss: 1.5089 - val_accuracy: 0.4946\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.0916 - accuracy: 0.6343 - val_loss: 1.7233 - val_accuracy: 0.4565\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.9322 - accuracy: 0.6822 - val_loss: 1.4735 - val_accuracy: 0.5109\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.8907 - accuracy: 0.6755 - val_loss: 1.3757 - val_accuracy: 0.5380\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.8748 - accuracy: 0.6835 - val_loss: 1.4347 - val_accuracy: 0.5272\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.8021 - accuracy: 0.7048 - val_loss: 1.2701 - val_accuracy: 0.5380\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.7741 - accuracy: 0.7128 - val_loss: 1.2575 - val_accuracy: 0.5435\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.7145 - accuracy: 0.7420 - val_loss: 1.1397 - val_accuracy: 0.5870\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.7023 - accuracy: 0.7314 - val_loss: 1.1595 - val_accuracy: 0.5707\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.6130 - accuracy: 0.7779 - val_loss: 1.1355 - val_accuracy: 0.5870\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.6421 - accuracy: 0.7473 - val_loss: 1.0726 - val_accuracy: 0.5815\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5860 - accuracy: 0.7846 - val_loss: 1.0921 - val_accuracy: 0.5815\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5486 - accuracy: 0.7912 - val_loss: 1.1003 - val_accuracy: 0.5652\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5123 - accuracy: 0.8005 - val_loss: 1.0566 - val_accuracy: 0.5978\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5112 - accuracy: 0.8085 - val_loss: 1.0756 - val_accuracy: 0.5978\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4822 - accuracy: 0.8271 - val_loss: 1.0476 - val_accuracy: 0.6033\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4587 - accuracy: 0.8218 - val_loss: 1.0814 - val_accuracy: 0.5489\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4741 - accuracy: 0.8152 - val_loss: 1.0143 - val_accuracy: 0.6250\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4462 - accuracy: 0.8484 - val_loss: 1.0195 - val_accuracy: 0.6033\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4606 - accuracy: 0.8324 - val_loss: 1.0407 - val_accuracy: 0.5924\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4488 - accuracy: 0.8444 - val_loss: 1.0464 - val_accuracy: 0.5978\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4618 - accuracy: 0.8258 - val_loss: 1.0278 - val_accuracy: 0.5870\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4048 - accuracy: 0.8404 - val_loss: 1.0034 - val_accuracy: 0.5978\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4134 - accuracy: 0.8431 - val_loss: 1.0518 - val_accuracy: 0.5598\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3845 - accuracy: 0.8484 - val_loss: 0.9743 - val_accuracy: 0.6087\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3960 - accuracy: 0.8484 - val_loss: 1.0213 - val_accuracy: 0.6033\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3961 - accuracy: 0.8457 - val_loss: 0.9885 - val_accuracy: 0.5924\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.3427 - accuracy: 0.8723 - val_loss: 1.0306 - val_accuracy: 0.5924\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.3699 - accuracy: 0.8564 - val_loss: 1.0441 - val_accuracy: 0.5815\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3255 - accuracy: 0.8843 - val_loss: 1.0067 - val_accuracy: 0.6141\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3606 - accuracy: 0.8670 - val_loss: 1.0037 - val_accuracy: 0.6141\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2753 - accuracy: 0.8949 - val_loss: 0.9611 - val_accuracy: 0.6304\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3083 - accuracy: 0.8843 - val_loss: 0.9991 - val_accuracy: 0.6087\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3410 - accuracy: 0.8617 - val_loss: 0.9722 - val_accuracy: 0.6359\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3509 - accuracy: 0.8644 - val_loss: 0.9642 - val_accuracy: 0.5924\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3123 - accuracy: 0.8777 - val_loss: 0.9290 - val_accuracy: 0.6196\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3269 - accuracy: 0.8710 - val_loss: 0.9881 - val_accuracy: 0.5870\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2689 - accuracy: 0.8870 - val_loss: 0.9046 - val_accuracy: 0.6141\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3084 - accuracy: 0.8949 - val_loss: 0.9750 - val_accuracy: 0.5924\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2858 - accuracy: 0.8949 - val_loss: 0.9239 - val_accuracy: 0.6196\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3054 - accuracy: 0.8856 - val_loss: 0.9717 - val_accuracy: 0.6087\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3273 - accuracy: 0.8803 - val_loss: 0.9391 - val_accuracy: 0.6304\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3042 - accuracy: 0.8830 - val_loss: 0.9153 - val_accuracy: 0.5978\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2702 - accuracy: 0.9016 - val_loss: 0.9356 - val_accuracy: 0.6033\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2900 - accuracy: 0.9003 - val_loss: 0.8906 - val_accuracy: 0.6250\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.2832 - accuracy: 0.8976 - val_loss: 0.9717 - val_accuracy: 0.6087\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.3318 - accuracy: 0.8870 - val_loss: 0.9138 - val_accuracy: 0.6359\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3259 - accuracy: 0.8843 - val_loss: 0.9118 - val_accuracy: 0.6576\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2943 - accuracy: 0.8896 - val_loss: 0.9248 - val_accuracy: 0.6141\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2623 - accuracy: 0.9029 - val_loss: 0.9211 - val_accuracy: 0.6304\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2506 - accuracy: 0.9082 - val_loss: 0.8862 - val_accuracy: 0.6413\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2476 - accuracy: 0.9056 - val_loss: 0.9113 - val_accuracy: 0.6141\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2603 - accuracy: 0.9122 - val_loss: 0.9218 - val_accuracy: 0.5978\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2552 - accuracy: 0.9003 - val_loss: 0.8795 - val_accuracy: 0.6304\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2585 - accuracy: 0.9003 - val_loss: 0.8727 - val_accuracy: 0.6304\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2414 - accuracy: 0.9069 - val_loss: 0.9259 - val_accuracy: 0.6141\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2238 - accuracy: 0.9229 - val_loss: 0.9315 - val_accuracy: 0.6141\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2311 - accuracy: 0.9189 - val_loss: 0.8867 - val_accuracy: 0.6413\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2309 - accuracy: 0.9229 - val_loss: 0.9449 - val_accuracy: 0.6087\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2186 - accuracy: 0.9229 - val_loss: 0.8922 - val_accuracy: 0.6250\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2252 - accuracy: 0.9162 - val_loss: 0.8980 - val_accuracy: 0.6576\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2390 - accuracy: 0.9122 - val_loss: 0.8721 - val_accuracy: 0.6630\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2743 - accuracy: 0.8963 - val_loss: 0.9146 - val_accuracy: 0.6467\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2608 - accuracy: 0.9043 - val_loss: 0.9036 - val_accuracy: 0.6522\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.2390 - accuracy: 0.9096 - val_loss: 0.9244 - val_accuracy: 0.6467\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2292 - accuracy: 0.9136 - val_loss: 0.9352 - val_accuracy: 0.6196\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2014 - accuracy: 0.9269 - val_loss: 0.9439 - val_accuracy: 0.6250\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2180 - accuracy: 0.9202 - val_loss: 0.9495 - val_accuracy: 0.6359\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2435 - accuracy: 0.9202 - val_loss: 0.9221 - val_accuracy: 0.6467\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1933 - accuracy: 0.9348 - val_loss: 0.9143 - val_accuracy: 0.6304\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2202 - accuracy: 0.9242 - val_loss: 0.9353 - val_accuracy: 0.6467\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2080 - accuracy: 0.9229 - val_loss: 0.9014 - val_accuracy: 0.6467\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2245 - accuracy: 0.9202 - val_loss: 0.9365 - val_accuracy: 0.6467\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2233 - accuracy: 0.9215 - val_loss: 0.8978 - val_accuracy: 0.6467\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2487 - accuracy: 0.9242 - val_loss: 0.9758 - val_accuracy: 0.6250\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2211 - accuracy: 0.9149 - val_loss: 0.9880 - val_accuracy: 0.6033\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2078 - accuracy: 0.9242 - val_loss: 0.9181 - val_accuracy: 0.6359\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1941 - accuracy: 0.9322 - val_loss: 0.9257 - val_accuracy: 0.6196\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1925 - accuracy: 0.9295 - val_loss: 0.8747 - val_accuracy: 0.6576\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1847 - accuracy: 0.9375 - val_loss: 0.9378 - val_accuracy: 0.6196\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1927 - accuracy: 0.9375 - val_loss: 0.8489 - val_accuracy: 0.6467\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1799 - accuracy: 0.9348 - val_loss: 0.8902 - val_accuracy: 0.6467\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1884 - accuracy: 0.9309 - val_loss: 0.9113 - val_accuracy: 0.6576\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2003 - accuracy: 0.9269 - val_loss: 0.9769 - val_accuracy: 0.6467\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1820 - accuracy: 0.9335 - val_loss: 0.9135 - val_accuracy: 0.6196\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1712 - accuracy: 0.9322 - val_loss: 0.9651 - val_accuracy: 0.6467\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1655 - accuracy: 0.9362 - val_loss: 0.9229 - val_accuracy: 0.6359\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2197 - accuracy: 0.9096 - val_loss: 0.9793 - val_accuracy: 0.6359\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1897 - accuracy: 0.9282 - val_loss: 0.9989 - val_accuracy: 0.6250\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1912 - accuracy: 0.9255 - val_loss: 0.8736 - val_accuracy: 0.6250\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1724 - accuracy: 0.9295 - val_loss: 1.0630 - val_accuracy: 0.5924\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1971 - accuracy: 0.9309 - val_loss: 0.8776 - val_accuracy: 0.6576\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2308 - accuracy: 0.9189 - val_loss: 0.9492 - val_accuracy: 0.6467\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.2224 - accuracy: 0.9122 - val_loss: 1.0153 - val_accuracy: 0.6196\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.2029 - accuracy: 0.9269 - val_loss: 1.0181 - val_accuracy: 0.6304\n",
      "Test accuracy of the CNN model for subject 3: 0.800000011920929\n",
      "Shape of X after trimming: (188, 22, 500)\n",
      "Shape of X after maxpooling: (188, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (376, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (752, 22, 250)\n",
      "Shape of X after trimming: (47, 22, 500)\n",
      "Shape of X after maxpooling: (47, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (94, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (188, 22, 250)\n",
      "Shape of X after trimming: (47, 22, 500)\n",
      "Shape of X after maxpooling: (47, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (94, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (188, 22, 250)\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 19ms/step - loss: 2.2951 - accuracy: 0.4295 - val_loss: 3.0034 - val_accuracy: 0.2819\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 1.9637 - accuracy: 0.4761 - val_loss: 2.3515 - val_accuracy: 0.3138\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 1.6188 - accuracy: 0.5598 - val_loss: 2.3843 - val_accuracy: 0.3457\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.3645 - accuracy: 0.5838 - val_loss: 2.2512 - val_accuracy: 0.3298\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.2986 - accuracy: 0.5811 - val_loss: 2.2018 - val_accuracy: 0.3511\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.1481 - accuracy: 0.6396 - val_loss: 2.0477 - val_accuracy: 0.3723\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 1.0374 - accuracy: 0.6423 - val_loss: 1.9685 - val_accuracy: 0.3989\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.8767 - accuracy: 0.6995 - val_loss: 1.9688 - val_accuracy: 0.4202\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.8042 - accuracy: 0.7141 - val_loss: 1.9252 - val_accuracy: 0.4043\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.8261 - accuracy: 0.7194 - val_loss: 1.7798 - val_accuracy: 0.4521\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.7305 - accuracy: 0.7168 - val_loss: 1.6865 - val_accuracy: 0.4468\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.6640 - accuracy: 0.7473 - val_loss: 1.7587 - val_accuracy: 0.4628\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.6706 - accuracy: 0.7540 - val_loss: 1.6928 - val_accuracy: 0.4787\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.6079 - accuracy: 0.7646 - val_loss: 1.6049 - val_accuracy: 0.5000\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.5712 - accuracy: 0.7739 - val_loss: 1.5444 - val_accuracy: 0.4894\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.5451 - accuracy: 0.7965 - val_loss: 1.5696 - val_accuracy: 0.5053\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5534 - accuracy: 0.7832 - val_loss: 1.6817 - val_accuracy: 0.4787\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5193 - accuracy: 0.8059 - val_loss: 1.5256 - val_accuracy: 0.5000\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5079 - accuracy: 0.7979 - val_loss: 1.5375 - val_accuracy: 0.5106\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4964 - accuracy: 0.8059 - val_loss: 1.5132 - val_accuracy: 0.5053\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4336 - accuracy: 0.8351 - val_loss: 1.4563 - val_accuracy: 0.5426\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4982 - accuracy: 0.8218 - val_loss: 1.5116 - val_accuracy: 0.4947\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4617 - accuracy: 0.8165 - val_loss: 1.4478 - val_accuracy: 0.5213\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4339 - accuracy: 0.8351 - val_loss: 1.4063 - val_accuracy: 0.5372\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4156 - accuracy: 0.8351 - val_loss: 1.5129 - val_accuracy: 0.5319\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3932 - accuracy: 0.8551 - val_loss: 1.3820 - val_accuracy: 0.5479\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4102 - accuracy: 0.8524 - val_loss: 1.4186 - val_accuracy: 0.5479\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3841 - accuracy: 0.8537 - val_loss: 1.3324 - val_accuracy: 0.5479\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3703 - accuracy: 0.8564 - val_loss: 1.4277 - val_accuracy: 0.5372\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3872 - accuracy: 0.8537 - val_loss: 1.3483 - val_accuracy: 0.5372\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3572 - accuracy: 0.8697 - val_loss: 1.2772 - val_accuracy: 0.5691\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3478 - accuracy: 0.8684 - val_loss: 1.3917 - val_accuracy: 0.5585\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3269 - accuracy: 0.8803 - val_loss: 1.3333 - val_accuracy: 0.5851\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3024 - accuracy: 0.8830 - val_loss: 1.3013 - val_accuracy: 0.5851\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3397 - accuracy: 0.8697 - val_loss: 1.3597 - val_accuracy: 0.5745\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3100 - accuracy: 0.8803 - val_loss: 1.3066 - val_accuracy: 0.5798\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2864 - accuracy: 0.8989 - val_loss: 1.3285 - val_accuracy: 0.5745\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2937 - accuracy: 0.8803 - val_loss: 1.3110 - val_accuracy: 0.6064\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3078 - accuracy: 0.8896 - val_loss: 1.2816 - val_accuracy: 0.5585\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.3049 - accuracy: 0.8883 - val_loss: 1.3080 - val_accuracy: 0.5691\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.3021 - accuracy: 0.8816 - val_loss: 1.2234 - val_accuracy: 0.6011\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.2840 - accuracy: 0.8976 - val_loss: 1.2579 - val_accuracy: 0.6011\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.2934 - accuracy: 0.8976 - val_loss: 1.3311 - val_accuracy: 0.6064\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2347 - accuracy: 0.9109 - val_loss: 1.2706 - val_accuracy: 0.6223\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2681 - accuracy: 0.8989 - val_loss: 1.2233 - val_accuracy: 0.6277\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2803 - accuracy: 0.9082 - val_loss: 1.1814 - val_accuracy: 0.6170\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2678 - accuracy: 0.9029 - val_loss: 1.2710 - val_accuracy: 0.6330\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2606 - accuracy: 0.9029 - val_loss: 1.2826 - val_accuracy: 0.6277\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2320 - accuracy: 0.9202 - val_loss: 1.2484 - val_accuracy: 0.6223\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2298 - accuracy: 0.9109 - val_loss: 1.1940 - val_accuracy: 0.6383\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2217 - accuracy: 0.9189 - val_loss: 1.2773 - val_accuracy: 0.6170\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2368 - accuracy: 0.9109 - val_loss: 1.2148 - val_accuracy: 0.6277\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2005 - accuracy: 0.9322 - val_loss: 1.2227 - val_accuracy: 0.6436\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1938 - accuracy: 0.9415 - val_loss: 1.1395 - val_accuracy: 0.6383\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2225 - accuracy: 0.9082 - val_loss: 1.3242 - val_accuracy: 0.6170\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2071 - accuracy: 0.9269 - val_loss: 1.2557 - val_accuracy: 0.6117\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2215 - accuracy: 0.9202 - val_loss: 1.2230 - val_accuracy: 0.6223\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2104 - accuracy: 0.9348 - val_loss: 1.2135 - val_accuracy: 0.6277\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2062 - accuracy: 0.9229 - val_loss: 1.2232 - val_accuracy: 0.6170\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2336 - accuracy: 0.9176 - val_loss: 1.1186 - val_accuracy: 0.6223\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2123 - accuracy: 0.9162 - val_loss: 1.2441 - val_accuracy: 0.6223\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2481 - accuracy: 0.9109 - val_loss: 1.1697 - val_accuracy: 0.6383\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1907 - accuracy: 0.9229 - val_loss: 1.3599 - val_accuracy: 0.5957\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2045 - accuracy: 0.9229 - val_loss: 1.1480 - val_accuracy: 0.6383\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2004 - accuracy: 0.9242 - val_loss: 1.3083 - val_accuracy: 0.6117\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1722 - accuracy: 0.9375 - val_loss: 1.1894 - val_accuracy: 0.6277\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1837 - accuracy: 0.9388 - val_loss: 1.2266 - val_accuracy: 0.6223\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1981 - accuracy: 0.9229 - val_loss: 1.2605 - val_accuracy: 0.6383\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2126 - accuracy: 0.9322 - val_loss: 1.1339 - val_accuracy: 0.6064\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1931 - accuracy: 0.9255 - val_loss: 1.2375 - val_accuracy: 0.6011\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1857 - accuracy: 0.9388 - val_loss: 1.3202 - val_accuracy: 0.6170\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1781 - accuracy: 0.9362 - val_loss: 1.2774 - val_accuracy: 0.6330\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1928 - accuracy: 0.9322 - val_loss: 1.2078 - val_accuracy: 0.6436\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2038 - accuracy: 0.9215 - val_loss: 1.2063 - val_accuracy: 0.6330\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2044 - accuracy: 0.9242 - val_loss: 1.3385 - val_accuracy: 0.6064\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1977 - accuracy: 0.9335 - val_loss: 1.2617 - val_accuracy: 0.6223\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1952 - accuracy: 0.9269 - val_loss: 1.2441 - val_accuracy: 0.6277\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1678 - accuracy: 0.9455 - val_loss: 1.3305 - val_accuracy: 0.6011\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.2243 - accuracy: 0.9215 - val_loss: 1.2296 - val_accuracy: 0.6277\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1940 - accuracy: 0.9309 - val_loss: 1.3257 - val_accuracy: 0.5904\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2088 - accuracy: 0.9229 - val_loss: 1.1820 - val_accuracy: 0.6223\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1721 - accuracy: 0.9295 - val_loss: 1.4094 - val_accuracy: 0.5957\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1978 - accuracy: 0.9255 - val_loss: 1.2332 - val_accuracy: 0.6117\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1962 - accuracy: 0.9229 - val_loss: 1.3729 - val_accuracy: 0.6011\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2001 - accuracy: 0.9255 - val_loss: 1.1141 - val_accuracy: 0.6543\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1754 - accuracy: 0.9375 - val_loss: 1.4203 - val_accuracy: 0.6011\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1813 - accuracy: 0.9335 - val_loss: 1.2200 - val_accuracy: 0.6011\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1647 - accuracy: 0.9335 - val_loss: 1.2427 - val_accuracy: 0.6011\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1615 - accuracy: 0.9388 - val_loss: 1.1312 - val_accuracy: 0.6170\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1537 - accuracy: 0.9428 - val_loss: 1.2456 - val_accuracy: 0.5851\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1585 - accuracy: 0.9415 - val_loss: 1.2430 - val_accuracy: 0.6064\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1711 - accuracy: 0.9269 - val_loss: 1.2455 - val_accuracy: 0.6223\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1779 - accuracy: 0.9309 - val_loss: 1.2561 - val_accuracy: 0.6170\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1532 - accuracy: 0.9388 - val_loss: 1.1928 - val_accuracy: 0.6277\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1456 - accuracy: 0.9388 - val_loss: 1.2207 - val_accuracy: 0.6489\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1463 - accuracy: 0.9455 - val_loss: 1.2061 - val_accuracy: 0.5957\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1322 - accuracy: 0.9548 - val_loss: 1.2822 - val_accuracy: 0.6170\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1585 - accuracy: 0.9428 - val_loss: 1.2314 - val_accuracy: 0.5638\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1702 - accuracy: 0.9455 - val_loss: 1.1651 - val_accuracy: 0.6011\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1460 - accuracy: 0.9388 - val_loss: 1.2289 - val_accuracy: 0.5904\n",
      "Test accuracy of the CNN model for subject 4: 0.8510638475418091\n",
      "Shape of X after trimming: (189, 22, 500)\n",
      "Shape of X after maxpooling: (189, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (378, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (756, 22, 250)\n",
      "Shape of X after trimming: (47, 22, 500)\n",
      "Shape of X after maxpooling: (47, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (94, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (188, 22, 250)\n",
      "Shape of X after trimming: (49, 22, 500)\n",
      "Shape of X after maxpooling: (49, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (98, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (196, 22, 250)\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 18ms/step - loss: 2.3450 - accuracy: 0.4444 - val_loss: 3.9310 - val_accuracy: 0.3457\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 1.8350 - accuracy: 0.4987 - val_loss: 3.6779 - val_accuracy: 0.3457\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 1.6072 - accuracy: 0.5291 - val_loss: 2.5683 - val_accuracy: 0.3989\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 1.3995 - accuracy: 0.5728 - val_loss: 2.2535 - val_accuracy: 0.4362\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.4127 - accuracy: 0.5608 - val_loss: 1.7241 - val_accuracy: 0.4840\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 1.2677 - accuracy: 0.5714 - val_loss: 1.7822 - val_accuracy: 0.5000\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.0482 - accuracy: 0.6204 - val_loss: 1.3808 - val_accuracy: 0.5479\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.0609 - accuracy: 0.6389 - val_loss: 1.2873 - val_accuracy: 0.5851\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.9421 - accuracy: 0.6323 - val_loss: 1.1780 - val_accuracy: 0.5745\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.8861 - accuracy: 0.6495 - val_loss: 1.1696 - val_accuracy: 0.6117\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.8607 - accuracy: 0.6892 - val_loss: 1.0792 - val_accuracy: 0.6170\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.8121 - accuracy: 0.7077 - val_loss: 0.9833 - val_accuracy: 0.5957\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.7495 - accuracy: 0.7196 - val_loss: 0.9993 - val_accuracy: 0.6170\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7427 - accuracy: 0.7169 - val_loss: 0.9350 - val_accuracy: 0.5904\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.6963 - accuracy: 0.7394 - val_loss: 0.9544 - val_accuracy: 0.6277\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.6973 - accuracy: 0.7262 - val_loss: 0.9037 - val_accuracy: 0.6117\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.6319 - accuracy: 0.7646 - val_loss: 0.8229 - val_accuracy: 0.6330\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.6304 - accuracy: 0.7659 - val_loss: 0.8616 - val_accuracy: 0.6383\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.5963 - accuracy: 0.7579 - val_loss: 0.8471 - val_accuracy: 0.5957\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.6397 - accuracy: 0.7474 - val_loss: 0.8466 - val_accuracy: 0.6170\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.5749 - accuracy: 0.7526 - val_loss: 0.7948 - val_accuracy: 0.5904\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.5428 - accuracy: 0.7817 - val_loss: 0.7942 - val_accuracy: 0.6011\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5327 - accuracy: 0.7857 - val_loss: 0.7914 - val_accuracy: 0.5904\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4947 - accuracy: 0.8122 - val_loss: 0.7855 - val_accuracy: 0.6436\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.5228 - accuracy: 0.7963 - val_loss: 0.7512 - val_accuracy: 0.6117\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.5054 - accuracy: 0.8082 - val_loss: 0.7840 - val_accuracy: 0.6277\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.5067 - accuracy: 0.7831 - val_loss: 0.7340 - val_accuracy: 0.6117\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4745 - accuracy: 0.8254 - val_loss: 0.7607 - val_accuracy: 0.6330\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4957 - accuracy: 0.8095 - val_loss: 0.7272 - val_accuracy: 0.6436\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4448 - accuracy: 0.8320 - val_loss: 0.7452 - val_accuracy: 0.6330\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4233 - accuracy: 0.8373 - val_loss: 0.7152 - val_accuracy: 0.6543\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4514 - accuracy: 0.8320 - val_loss: 0.7257 - val_accuracy: 0.6702\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3857 - accuracy: 0.8426 - val_loss: 0.7572 - val_accuracy: 0.6596\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3966 - accuracy: 0.8452 - val_loss: 0.7352 - val_accuracy: 0.6809\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4151 - accuracy: 0.8386 - val_loss: 0.7128 - val_accuracy: 0.6436\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4097 - accuracy: 0.8386 - val_loss: 0.7716 - val_accuracy: 0.6862\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3668 - accuracy: 0.8571 - val_loss: 0.7149 - val_accuracy: 0.7234\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3904 - accuracy: 0.8532 - val_loss: 0.7049 - val_accuracy: 0.7181\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4393 - accuracy: 0.8360 - val_loss: 0.7178 - val_accuracy: 0.7128\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4161 - accuracy: 0.8439 - val_loss: 0.6968 - val_accuracy: 0.7553\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3920 - accuracy: 0.8624 - val_loss: 0.6857 - val_accuracy: 0.7340\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3573 - accuracy: 0.8677 - val_loss: 0.6776 - val_accuracy: 0.7553\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3699 - accuracy: 0.8585 - val_loss: 0.6801 - val_accuracy: 0.7553\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.3487 - accuracy: 0.8677 - val_loss: 0.6741 - val_accuracy: 0.7394\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.4045 - accuracy: 0.8373 - val_loss: 0.6773 - val_accuracy: 0.7713\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.3263 - accuracy: 0.8783 - val_loss: 0.6649 - val_accuracy: 0.7500\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.3361 - accuracy: 0.8690 - val_loss: 0.6804 - val_accuracy: 0.7340\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3110 - accuracy: 0.8849 - val_loss: 0.6502 - val_accuracy: 0.7234\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3465 - accuracy: 0.8743 - val_loss: 0.6717 - val_accuracy: 0.7447\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3330 - accuracy: 0.8743 - val_loss: 0.6618 - val_accuracy: 0.7394\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3418 - accuracy: 0.8730 - val_loss: 0.6199 - val_accuracy: 0.7660\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3123 - accuracy: 0.8783 - val_loss: 0.6481 - val_accuracy: 0.7553\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3049 - accuracy: 0.8889 - val_loss: 0.6581 - val_accuracy: 0.7606\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2587 - accuracy: 0.9061 - val_loss: 0.6533 - val_accuracy: 0.7181\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3071 - accuracy: 0.8783 - val_loss: 0.6625 - val_accuracy: 0.7394\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3057 - accuracy: 0.8902 - val_loss: 0.6795 - val_accuracy: 0.7394\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2932 - accuracy: 0.8849 - val_loss: 0.6804 - val_accuracy: 0.7447\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2888 - accuracy: 0.8995 - val_loss: 0.6516 - val_accuracy: 0.7447\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3537 - accuracy: 0.8651 - val_loss: 0.6823 - val_accuracy: 0.7394\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3012 - accuracy: 0.8836 - val_loss: 0.6365 - val_accuracy: 0.7500\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2937 - accuracy: 0.8876 - val_loss: 0.6444 - val_accuracy: 0.7447\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2942 - accuracy: 0.8968 - val_loss: 0.6413 - val_accuracy: 0.7447\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2670 - accuracy: 0.9061 - val_loss: 0.6250 - val_accuracy: 0.7447\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2777 - accuracy: 0.8955 - val_loss: 0.6654 - val_accuracy: 0.7340\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2711 - accuracy: 0.9101 - val_loss: 0.6633 - val_accuracy: 0.7447\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2612 - accuracy: 0.9061 - val_loss: 0.6684 - val_accuracy: 0.7553\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2878 - accuracy: 0.8889 - val_loss: 0.6689 - val_accuracy: 0.7447\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2689 - accuracy: 0.9034 - val_loss: 0.6763 - val_accuracy: 0.7447\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2731 - accuracy: 0.9061 - val_loss: 0.6905 - val_accuracy: 0.7234\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2657 - accuracy: 0.9153 - val_loss: 0.6792 - val_accuracy: 0.7181\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2547 - accuracy: 0.9127 - val_loss: 0.6847 - val_accuracy: 0.7287\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2678 - accuracy: 0.9087 - val_loss: 0.6650 - val_accuracy: 0.7394\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2213 - accuracy: 0.9233 - val_loss: 0.6661 - val_accuracy: 0.7234\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2597 - accuracy: 0.9048 - val_loss: 0.6801 - val_accuracy: 0.7287\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2429 - accuracy: 0.9180 - val_loss: 0.6793 - val_accuracy: 0.7074\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.2527 - accuracy: 0.9008 - val_loss: 0.6998 - val_accuracy: 0.7340\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.2475 - accuracy: 0.9021 - val_loss: 0.6895 - val_accuracy: 0.7500\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2422 - accuracy: 0.9087 - val_loss: 0.6877 - val_accuracy: 0.7447\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.2681 - accuracy: 0.9074 - val_loss: 0.6579 - val_accuracy: 0.7340\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2678 - accuracy: 0.9021 - val_loss: 0.7187 - val_accuracy: 0.7394\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2583 - accuracy: 0.9021 - val_loss: 0.6239 - val_accuracy: 0.7128\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2268 - accuracy: 0.9127 - val_loss: 0.6624 - val_accuracy: 0.7128\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2356 - accuracy: 0.9153 - val_loss: 0.7417 - val_accuracy: 0.7128\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2085 - accuracy: 0.9259 - val_loss: 0.6917 - val_accuracy: 0.7234\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2462 - accuracy: 0.9140 - val_loss: 0.6822 - val_accuracy: 0.7500\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2445 - accuracy: 0.9101 - val_loss: 0.6553 - val_accuracy: 0.7340\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2433 - accuracy: 0.9114 - val_loss: 0.6372 - val_accuracy: 0.7553\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2199 - accuracy: 0.9206 - val_loss: 0.6680 - val_accuracy: 0.7553\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2277 - accuracy: 0.9101 - val_loss: 0.6128 - val_accuracy: 0.7394\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2433 - accuracy: 0.9008 - val_loss: 0.5738 - val_accuracy: 0.7766\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2483 - accuracy: 0.9206 - val_loss: 0.6328 - val_accuracy: 0.7394\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2473 - accuracy: 0.9034 - val_loss: 0.5549 - val_accuracy: 0.7500\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2274 - accuracy: 0.9087 - val_loss: 0.6418 - val_accuracy: 0.7234\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2286 - accuracy: 0.9087 - val_loss: 0.5699 - val_accuracy: 0.7340\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2368 - accuracy: 0.9048 - val_loss: 0.6663 - val_accuracy: 0.7500\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1916 - accuracy: 0.9272 - val_loss: 0.6600 - val_accuracy: 0.7287\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1935 - accuracy: 0.9220 - val_loss: 0.6928 - val_accuracy: 0.7606\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2111 - accuracy: 0.9259 - val_loss: 0.6520 - val_accuracy: 0.7447\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1991 - accuracy: 0.9299 - val_loss: 0.6195 - val_accuracy: 0.7553\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1801 - accuracy: 0.9325 - val_loss: 0.6212 - val_accuracy: 0.7500\n",
      "Test accuracy of the CNN model for subject 5: 0.6224489808082581\n",
      "Shape of X after trimming: (191, 22, 500)\n",
      "Shape of X after maxpooling: (191, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (382, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (764, 22, 250)\n",
      "Shape of X after trimming: (47, 22, 500)\n",
      "Shape of X after maxpooling: (47, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (94, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (188, 22, 250)\n",
      "Shape of X after trimming: (50, 22, 500)\n",
      "Shape of X after maxpooling: (50, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (100, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (200, 22, 250)\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 25ms/step - loss: 1.9198 - accuracy: 0.4843 - val_loss: 1.9173 - val_accuracy: 0.4255\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 1.5510 - accuracy: 0.5105 - val_loss: 1.6381 - val_accuracy: 0.4734\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 1.2683 - accuracy: 0.5942 - val_loss: 1.4236 - val_accuracy: 0.4947\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 1.2243 - accuracy: 0.5903 - val_loss: 1.2098 - val_accuracy: 0.5532\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 1.0302 - accuracy: 0.6204 - val_loss: 1.0146 - val_accuracy: 0.6277\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.9308 - accuracy: 0.6741 - val_loss: 0.8982 - val_accuracy: 0.6702\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.8826 - accuracy: 0.6898 - val_loss: 0.8681 - val_accuracy: 0.6809\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.7614 - accuracy: 0.7003 - val_loss: 0.7778 - val_accuracy: 0.7234\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.7578 - accuracy: 0.7120 - val_loss: 0.7929 - val_accuracy: 0.7181\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.7039 - accuracy: 0.7421 - val_loss: 0.7427 - val_accuracy: 0.7340\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.5769 - accuracy: 0.7723 - val_loss: 0.7366 - val_accuracy: 0.7394\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.6329 - accuracy: 0.7565 - val_loss: 0.7340 - val_accuracy: 0.7394\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.5799 - accuracy: 0.7840 - val_loss: 0.6940 - val_accuracy: 0.7447\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.5373 - accuracy: 0.7945 - val_loss: 0.6951 - val_accuracy: 0.7447\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.5671 - accuracy: 0.7932 - val_loss: 0.6687 - val_accuracy: 0.7500\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.4722 - accuracy: 0.8076 - val_loss: 0.6329 - val_accuracy: 0.7500\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.4739 - accuracy: 0.8181 - val_loss: 0.6299 - val_accuracy: 0.7713\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4264 - accuracy: 0.8416 - val_loss: 0.6263 - val_accuracy: 0.7979\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4428 - accuracy: 0.8246 - val_loss: 0.6774 - val_accuracy: 0.7766\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4297 - accuracy: 0.8272 - val_loss: 0.6417 - val_accuracy: 0.7766\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4476 - accuracy: 0.8325 - val_loss: 0.6182 - val_accuracy: 0.7926\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4054 - accuracy: 0.8338 - val_loss: 0.6156 - val_accuracy: 0.8191\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3983 - accuracy: 0.8521 - val_loss: 0.6049 - val_accuracy: 0.8298\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4037 - accuracy: 0.8403 - val_loss: 0.6096 - val_accuracy: 0.8298\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3748 - accuracy: 0.8639 - val_loss: 0.6165 - val_accuracy: 0.8298\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3743 - accuracy: 0.8495 - val_loss: 0.6352 - val_accuracy: 0.8191\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3541 - accuracy: 0.8613 - val_loss: 0.6120 - val_accuracy: 0.8351\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3377 - accuracy: 0.8678 - val_loss: 0.5922 - val_accuracy: 0.8245\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3426 - accuracy: 0.8665 - val_loss: 0.5927 - val_accuracy: 0.8351\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3141 - accuracy: 0.8835 - val_loss: 0.6021 - val_accuracy: 0.8191\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3031 - accuracy: 0.8809 - val_loss: 0.5770 - val_accuracy: 0.8351\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2649 - accuracy: 0.8992 - val_loss: 0.5790 - val_accuracy: 0.8404\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2952 - accuracy: 0.8914 - val_loss: 0.5604 - val_accuracy: 0.8404\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3128 - accuracy: 0.8835 - val_loss: 0.5723 - val_accuracy: 0.8457\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3074 - accuracy: 0.8835 - val_loss: 0.5638 - val_accuracy: 0.8404\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.2640 - accuracy: 0.9031 - val_loss: 0.5855 - val_accuracy: 0.8404\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.2853 - accuracy: 0.8940 - val_loss: 0.5913 - val_accuracy: 0.8245\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2521 - accuracy: 0.9071 - val_loss: 0.5694 - val_accuracy: 0.8298\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.2828 - accuracy: 0.8796 - val_loss: 0.5618 - val_accuracy: 0.8298\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2366 - accuracy: 0.9097 - val_loss: 0.5452 - val_accuracy: 0.8351\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2633 - accuracy: 0.9045 - val_loss: 0.5588 - val_accuracy: 0.8245\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.2577 - accuracy: 0.9058 - val_loss: 0.5442 - val_accuracy: 0.8298\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.2123 - accuracy: 0.9267 - val_loss: 0.5390 - val_accuracy: 0.8351\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2315 - accuracy: 0.9110 - val_loss: 0.5482 - val_accuracy: 0.8351\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2190 - accuracy: 0.9228 - val_loss: 0.5418 - val_accuracy: 0.8351\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2159 - accuracy: 0.9162 - val_loss: 0.5596 - val_accuracy: 0.8245\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2000 - accuracy: 0.9280 - val_loss: 0.5545 - val_accuracy: 0.8191\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2184 - accuracy: 0.9123 - val_loss: 0.5250 - val_accuracy: 0.8351\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2247 - accuracy: 0.9175 - val_loss: 0.5505 - val_accuracy: 0.8245\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2233 - accuracy: 0.9241 - val_loss: 0.5531 - val_accuracy: 0.8298\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2095 - accuracy: 0.9280 - val_loss: 0.5204 - val_accuracy: 0.8351\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.2533 - accuracy: 0.9031 - val_loss: 0.5038 - val_accuracy: 0.8457\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2080 - accuracy: 0.9188 - val_loss: 0.5622 - val_accuracy: 0.8245\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2158 - accuracy: 0.9228 - val_loss: 0.5168 - val_accuracy: 0.8298\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1776 - accuracy: 0.9411 - val_loss: 0.5294 - val_accuracy: 0.8351\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2183 - accuracy: 0.9188 - val_loss: 0.5433 - val_accuracy: 0.8298\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1862 - accuracy: 0.9319 - val_loss: 0.5591 - val_accuracy: 0.8245\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1809 - accuracy: 0.9332 - val_loss: 0.5091 - val_accuracy: 0.8511\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1922 - accuracy: 0.9136 - val_loss: 0.5118 - val_accuracy: 0.8404\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1892 - accuracy: 0.9202 - val_loss: 0.5256 - val_accuracy: 0.8191\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1911 - accuracy: 0.9267 - val_loss: 0.5642 - val_accuracy: 0.8351\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1965 - accuracy: 0.9123 - val_loss: 0.5188 - val_accuracy: 0.8457\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1814 - accuracy: 0.9385 - val_loss: 0.4937 - val_accuracy: 0.8404\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1738 - accuracy: 0.9385 - val_loss: 0.5070 - val_accuracy: 0.8457\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1886 - accuracy: 0.9215 - val_loss: 0.5043 - val_accuracy: 0.8457\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1590 - accuracy: 0.9385 - val_loss: 0.5171 - val_accuracy: 0.8511\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1422 - accuracy: 0.9463 - val_loss: 0.4728 - val_accuracy: 0.8457\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.1615 - accuracy: 0.9346 - val_loss: 0.5297 - val_accuracy: 0.8511\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.2094 - accuracy: 0.9241 - val_loss: 0.4783 - val_accuracy: 0.8511\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1775 - accuracy: 0.9306 - val_loss: 0.5147 - val_accuracy: 0.8511\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1962 - accuracy: 0.9267 - val_loss: 0.5570 - val_accuracy: 0.8404\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1759 - accuracy: 0.9385 - val_loss: 0.5451 - val_accuracy: 0.8457\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1545 - accuracy: 0.9424 - val_loss: 0.5123 - val_accuracy: 0.8511\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1397 - accuracy: 0.9476 - val_loss: 0.5354 - val_accuracy: 0.8404\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1650 - accuracy: 0.9476 - val_loss: 0.5057 - val_accuracy: 0.8511\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1645 - accuracy: 0.9424 - val_loss: 0.5298 - val_accuracy: 0.8511\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1743 - accuracy: 0.9319 - val_loss: 0.4966 - val_accuracy: 0.8564\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1626 - accuracy: 0.9346 - val_loss: 0.5089 - val_accuracy: 0.8564\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1409 - accuracy: 0.9542 - val_loss: 0.5249 - val_accuracy: 0.8564\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1642 - accuracy: 0.9385 - val_loss: 0.5186 - val_accuracy: 0.8511\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1338 - accuracy: 0.9542 - val_loss: 0.5211 - val_accuracy: 0.8298\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1462 - accuracy: 0.9437 - val_loss: 0.5163 - val_accuracy: 0.8564\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1365 - accuracy: 0.9503 - val_loss: 0.5241 - val_accuracy: 0.8511\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1549 - accuracy: 0.9490 - val_loss: 0.5546 - val_accuracy: 0.8457\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1284 - accuracy: 0.9542 - val_loss: 0.5300 - val_accuracy: 0.8564\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1581 - accuracy: 0.9411 - val_loss: 0.5254 - val_accuracy: 0.8511\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1434 - accuracy: 0.9463 - val_loss: 0.5365 - val_accuracy: 0.8511\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1516 - accuracy: 0.9463 - val_loss: 0.5028 - val_accuracy: 0.8245\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1581 - accuracy: 0.9476 - val_loss: 0.5371 - val_accuracy: 0.8564\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1384 - accuracy: 0.9516 - val_loss: 0.5528 - val_accuracy: 0.8457\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1416 - accuracy: 0.9437 - val_loss: 0.5702 - val_accuracy: 0.8511\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1430 - accuracy: 0.9490 - val_loss: 0.5312 - val_accuracy: 0.8564\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1553 - accuracy: 0.9359 - val_loss: 0.5171 - val_accuracy: 0.8564\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1252 - accuracy: 0.9581 - val_loss: 0.5376 - val_accuracy: 0.8457\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.1373 - accuracy: 0.9516 - val_loss: 0.5313 - val_accuracy: 0.8511\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1411 - accuracy: 0.9529 - val_loss: 0.5232 - val_accuracy: 0.8404\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1293 - accuracy: 0.9620 - val_loss: 0.5393 - val_accuracy: 0.8564\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1179 - accuracy: 0.9620 - val_loss: 0.5522 - val_accuracy: 0.8511\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1214 - accuracy: 0.9555 - val_loss: 0.5617 - val_accuracy: 0.8191\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1159 - accuracy: 0.9516 - val_loss: 0.5559 - val_accuracy: 0.8511\n",
      "Test accuracy of the CNN model for subject 6: 0.7850000262260437\n",
      "Shape of X after trimming: (186, 22, 500)\n",
      "Shape of X after maxpooling: (186, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (372, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (744, 22, 250)\n",
      "Shape of X after trimming: (46, 22, 500)\n",
      "Shape of X after maxpooling: (46, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (92, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (184, 22, 250)\n",
      "Shape of X after trimming: (50, 22, 500)\n",
      "Shape of X after maxpooling: (50, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (100, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (200, 22, 250)\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 20ms/step - loss: 2.0375 - accuracy: 0.4987 - val_loss: 2.7693 - val_accuracy: 0.3424\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 1.6830 - accuracy: 0.5538 - val_loss: 2.6858 - val_accuracy: 0.3696\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 1.4331 - accuracy: 0.5833 - val_loss: 2.4702 - val_accuracy: 0.3696\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 1.2379 - accuracy: 0.6116 - val_loss: 2.0522 - val_accuracy: 0.4076\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 1.1508 - accuracy: 0.6169 - val_loss: 1.8789 - val_accuracy: 0.4348\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 1.0174 - accuracy: 0.6492 - val_loss: 1.7448 - val_accuracy: 0.4565\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.9462 - accuracy: 0.6720 - val_loss: 1.6886 - val_accuracy: 0.4565\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.8906 - accuracy: 0.7030 - val_loss: 1.4494 - val_accuracy: 0.4783\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.8342 - accuracy: 0.7056 - val_loss: 1.3132 - val_accuracy: 0.5054\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.7332 - accuracy: 0.7177 - val_loss: 1.2185 - val_accuracy: 0.5326\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.6817 - accuracy: 0.7433 - val_loss: 1.1741 - val_accuracy: 0.5489\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.6262 - accuracy: 0.7581 - val_loss: 1.1601 - val_accuracy: 0.5543\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5925 - accuracy: 0.7702 - val_loss: 1.0169 - val_accuracy: 0.5598\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5488 - accuracy: 0.7836 - val_loss: 0.9722 - val_accuracy: 0.5435\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5869 - accuracy: 0.7742 - val_loss: 1.1326 - val_accuracy: 0.5435\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.5827 - accuracy: 0.8011 - val_loss: 1.0105 - val_accuracy: 0.5543\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4928 - accuracy: 0.8038 - val_loss: 0.9153 - val_accuracy: 0.5870\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4523 - accuracy: 0.8347 - val_loss: 0.9450 - val_accuracy: 0.6141\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4719 - accuracy: 0.8172 - val_loss: 0.8898 - val_accuracy: 0.6359\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4295 - accuracy: 0.8508 - val_loss: 0.8464 - val_accuracy: 0.6467\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4455 - accuracy: 0.8280 - val_loss: 0.7679 - val_accuracy: 0.7228\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.3757 - accuracy: 0.8522 - val_loss: 0.7696 - val_accuracy: 0.7391\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.4224 - accuracy: 0.8347 - val_loss: 0.7754 - val_accuracy: 0.7120\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.3506 - accuracy: 0.8642 - val_loss: 0.7866 - val_accuracy: 0.7174\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3674 - accuracy: 0.8642 - val_loss: 0.7915 - val_accuracy: 0.7120\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3622 - accuracy: 0.8454 - val_loss: 0.7525 - val_accuracy: 0.7120\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.3321 - accuracy: 0.8710 - val_loss: 0.7273 - val_accuracy: 0.7337\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.3816 - accuracy: 0.8656 - val_loss: 0.7213 - val_accuracy: 0.7391\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3373 - accuracy: 0.8710 - val_loss: 0.7703 - val_accuracy: 0.7065\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3335 - accuracy: 0.8831 - val_loss: 0.7545 - val_accuracy: 0.7391\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3119 - accuracy: 0.8804 - val_loss: 0.7509 - val_accuracy: 0.7337\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3002 - accuracy: 0.8898 - val_loss: 0.7284 - val_accuracy: 0.7283\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2885 - accuracy: 0.8858 - val_loss: 0.7236 - val_accuracy: 0.7174\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2804 - accuracy: 0.8884 - val_loss: 0.7479 - val_accuracy: 0.7120\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2677 - accuracy: 0.9059 - val_loss: 0.7925 - val_accuracy: 0.6848\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2714 - accuracy: 0.8992 - val_loss: 0.7915 - val_accuracy: 0.7065\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3242 - accuracy: 0.8804 - val_loss: 0.7983 - val_accuracy: 0.6902\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2664 - accuracy: 0.8884 - val_loss: 0.7900 - val_accuracy: 0.6793\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2987 - accuracy: 0.8884 - val_loss: 0.7411 - val_accuracy: 0.7283\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2605 - accuracy: 0.9059 - val_loss: 0.7711 - val_accuracy: 0.7065\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2535 - accuracy: 0.9099 - val_loss: 0.7838 - val_accuracy: 0.6848\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2506 - accuracy: 0.9126 - val_loss: 0.7422 - val_accuracy: 0.6957\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2747 - accuracy: 0.8952 - val_loss: 0.7736 - val_accuracy: 0.7065\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2359 - accuracy: 0.8978 - val_loss: 0.7506 - val_accuracy: 0.7174\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2361 - accuracy: 0.9113 - val_loss: 0.7883 - val_accuracy: 0.6739\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2523 - accuracy: 0.9153 - val_loss: 0.7899 - val_accuracy: 0.6902\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2445 - accuracy: 0.9113 - val_loss: 0.7251 - val_accuracy: 0.7228\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2242 - accuracy: 0.9167 - val_loss: 0.7372 - val_accuracy: 0.7228\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2623 - accuracy: 0.9046 - val_loss: 0.7307 - val_accuracy: 0.7011\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.2111 - accuracy: 0.9328 - val_loss: 0.7308 - val_accuracy: 0.6793\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.2389 - accuracy: 0.9073 - val_loss: 0.7474 - val_accuracy: 0.6793\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2009 - accuracy: 0.9301 - val_loss: 0.7421 - val_accuracy: 0.6957\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2341 - accuracy: 0.9099 - val_loss: 0.7488 - val_accuracy: 0.6793\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2063 - accuracy: 0.9220 - val_loss: 0.7564 - val_accuracy: 0.6848\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2051 - accuracy: 0.9274 - val_loss: 0.7153 - val_accuracy: 0.7065\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.2043 - accuracy: 0.9220 - val_loss: 0.6956 - val_accuracy: 0.7228\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.1803 - accuracy: 0.9355 - val_loss: 0.6698 - val_accuracy: 0.7283\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1760 - accuracy: 0.9476 - val_loss: 0.6680 - val_accuracy: 0.7337\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1733 - accuracy: 0.9355 - val_loss: 0.6967 - val_accuracy: 0.7174\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1841 - accuracy: 0.9382 - val_loss: 0.7297 - val_accuracy: 0.6848\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1764 - accuracy: 0.9462 - val_loss: 0.6924 - val_accuracy: 0.7174\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1789 - accuracy: 0.9341 - val_loss: 0.7152 - val_accuracy: 0.6739\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1982 - accuracy: 0.9301 - val_loss: 0.6922 - val_accuracy: 0.7446\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1611 - accuracy: 0.9368 - val_loss: 0.7264 - val_accuracy: 0.7174\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1623 - accuracy: 0.9409 - val_loss: 0.7351 - val_accuracy: 0.6902\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1961 - accuracy: 0.9274 - val_loss: 0.6955 - val_accuracy: 0.7120\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1743 - accuracy: 0.9368 - val_loss: 0.7335 - val_accuracy: 0.6902\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1881 - accuracy: 0.9355 - val_loss: 0.7092 - val_accuracy: 0.7065\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1636 - accuracy: 0.9422 - val_loss: 0.6784 - val_accuracy: 0.7228\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1615 - accuracy: 0.9476 - val_loss: 0.6910 - val_accuracy: 0.7446\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1439 - accuracy: 0.9476 - val_loss: 0.6807 - val_accuracy: 0.7446\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1502 - accuracy: 0.9435 - val_loss: 0.6863 - val_accuracy: 0.7283\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1606 - accuracy: 0.9422 - val_loss: 0.7089 - val_accuracy: 0.7120\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1740 - accuracy: 0.9449 - val_loss: 0.6982 - val_accuracy: 0.7011\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1535 - accuracy: 0.9503 - val_loss: 0.6848 - val_accuracy: 0.7120\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1616 - accuracy: 0.9422 - val_loss: 0.6480 - val_accuracy: 0.7446\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1313 - accuracy: 0.9583 - val_loss: 0.6621 - val_accuracy: 0.7391\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1679 - accuracy: 0.9503 - val_loss: 0.6860 - val_accuracy: 0.7283\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.1341 - accuracy: 0.9570 - val_loss: 0.6775 - val_accuracy: 0.7337\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1552 - accuracy: 0.9382 - val_loss: 0.6953 - val_accuracy: 0.6957\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1382 - accuracy: 0.9516 - val_loss: 0.6953 - val_accuracy: 0.7228\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1363 - accuracy: 0.9516 - val_loss: 0.6779 - val_accuracy: 0.7337\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1319 - accuracy: 0.9476 - val_loss: 0.6712 - val_accuracy: 0.7228\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1571 - accuracy: 0.9422 - val_loss: 0.7004 - val_accuracy: 0.7283\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1380 - accuracy: 0.9556 - val_loss: 0.7050 - val_accuracy: 0.7283\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1430 - accuracy: 0.9503 - val_loss: 0.6875 - val_accuracy: 0.7446\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.1523 - accuracy: 0.9543 - val_loss: 0.6874 - val_accuracy: 0.7500\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1187 - accuracy: 0.9597 - val_loss: 0.7025 - val_accuracy: 0.7120\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1602 - accuracy: 0.9449 - val_loss: 0.6961 - val_accuracy: 0.7174\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1368 - accuracy: 0.9489 - val_loss: 0.6933 - val_accuracy: 0.7446\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1315 - accuracy: 0.9516 - val_loss: 0.7423 - val_accuracy: 0.7120\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.1433 - accuracy: 0.9556 - val_loss: 0.7172 - val_accuracy: 0.7174\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1493 - accuracy: 0.9382 - val_loss: 0.6837 - val_accuracy: 0.7554\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1435 - accuracy: 0.9516 - val_loss: 0.6897 - val_accuracy: 0.7228\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1457 - accuracy: 0.9489 - val_loss: 0.7342 - val_accuracy: 0.7011\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1380 - accuracy: 0.9489 - val_loss: 0.7172 - val_accuracy: 0.7228\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0977 - accuracy: 0.9677 - val_loss: 0.6962 - val_accuracy: 0.7228\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1233 - accuracy: 0.9516 - val_loss: 0.6722 - val_accuracy: 0.7228\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1123 - accuracy: 0.9624 - val_loss: 0.6745 - val_accuracy: 0.7283\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1228 - accuracy: 0.9543 - val_loss: 0.7017 - val_accuracy: 0.7337\n",
      "Test accuracy of the CNN model for subject 7: 0.675000011920929\n",
      "Shape of X after trimming: (185, 22, 500)\n",
      "Shape of X after maxpooling: (185, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (370, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (740, 22, 250)\n",
      "Shape of X after trimming: (46, 22, 500)\n",
      "Shape of X after maxpooling: (46, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (92, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (184, 22, 250)\n",
      "Shape of X after trimming: (47, 22, 500)\n",
      "Shape of X after maxpooling: (47, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (94, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (188, 22, 250)\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 18ms/step - loss: 1.8033 - accuracy: 0.5527 - val_loss: 2.1855 - val_accuracy: 0.5054\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 1.3897 - accuracy: 0.5905 - val_loss: 1.6771 - val_accuracy: 0.5870\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 1.2374 - accuracy: 0.6338 - val_loss: 1.4049 - val_accuracy: 0.6033\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.9645 - accuracy: 0.7027 - val_loss: 1.1649 - val_accuracy: 0.6413\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.7803 - accuracy: 0.7257 - val_loss: 1.0883 - val_accuracy: 0.6630\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.8071 - accuracy: 0.7351 - val_loss: 1.1054 - val_accuracy: 0.6739\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.7640 - accuracy: 0.7446 - val_loss: 0.9511 - val_accuracy: 0.6957\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.6161 - accuracy: 0.7824 - val_loss: 0.7880 - val_accuracy: 0.7228\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.5834 - accuracy: 0.7905 - val_loss: 0.7412 - val_accuracy: 0.7228\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.5457 - accuracy: 0.7959 - val_loss: 0.6477 - val_accuracy: 0.7609\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.5876 - accuracy: 0.7811 - val_loss: 0.6206 - val_accuracy: 0.7880\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4543 - accuracy: 0.8297 - val_loss: 0.5750 - val_accuracy: 0.7609\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4175 - accuracy: 0.8500 - val_loss: 0.5596 - val_accuracy: 0.7935\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4092 - accuracy: 0.8486 - val_loss: 0.5442 - val_accuracy: 0.8207\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4396 - accuracy: 0.8311 - val_loss: 0.5313 - val_accuracy: 0.8370\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4011 - accuracy: 0.8405 - val_loss: 0.5474 - val_accuracy: 0.7989\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.3889 - accuracy: 0.8486 - val_loss: 0.5769 - val_accuracy: 0.8207\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3202 - accuracy: 0.8784 - val_loss: 0.5173 - val_accuracy: 0.8859\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3232 - accuracy: 0.8824 - val_loss: 0.4276 - val_accuracy: 0.8913\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2896 - accuracy: 0.8892 - val_loss: 0.4955 - val_accuracy: 0.8587\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2599 - accuracy: 0.8919 - val_loss: 0.5004 - val_accuracy: 0.8587\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2685 - accuracy: 0.8905 - val_loss: 0.4681 - val_accuracy: 0.8696\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2921 - accuracy: 0.8932 - val_loss: 0.4526 - val_accuracy: 0.8750\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2666 - accuracy: 0.8973 - val_loss: 0.4702 - val_accuracy: 0.8696\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2954 - accuracy: 0.8986 - val_loss: 0.4306 - val_accuracy: 0.8913\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2768 - accuracy: 0.8919 - val_loss: 0.4420 - val_accuracy: 0.8750\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2617 - accuracy: 0.9108 - val_loss: 0.4452 - val_accuracy: 0.8696\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.2468 - accuracy: 0.8973 - val_loss: 0.4397 - val_accuracy: 0.8696\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2412 - accuracy: 0.9054 - val_loss: 0.4571 - val_accuracy: 0.8696\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2594 - accuracy: 0.9027 - val_loss: 0.4905 - val_accuracy: 0.8750\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2050 - accuracy: 0.9230 - val_loss: 0.4261 - val_accuracy: 0.8913\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.1963 - accuracy: 0.9243 - val_loss: 0.4195 - val_accuracy: 0.8913\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.1945 - accuracy: 0.9216 - val_loss: 0.3854 - val_accuracy: 0.8913\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2174 - accuracy: 0.9311 - val_loss: 0.3720 - val_accuracy: 0.8913\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2106 - accuracy: 0.9203 - val_loss: 0.3980 - val_accuracy: 0.8859\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1935 - accuracy: 0.9216 - val_loss: 0.4159 - val_accuracy: 0.8859\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1942 - accuracy: 0.9257 - val_loss: 0.4125 - val_accuracy: 0.8696\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1878 - accuracy: 0.9297 - val_loss: 0.3699 - val_accuracy: 0.8913\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1735 - accuracy: 0.9351 - val_loss: 0.3840 - val_accuracy: 0.8913\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1732 - accuracy: 0.9378 - val_loss: 0.4049 - val_accuracy: 0.8913\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1354 - accuracy: 0.9473 - val_loss: 0.3847 - val_accuracy: 0.8913\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1784 - accuracy: 0.9284 - val_loss: 0.4291 - val_accuracy: 0.9076\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 0.1768 - accuracy: 0.9378 - val_loss: 0.3863 - val_accuracy: 0.9076\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.1269 - accuracy: 0.9541 - val_loss: 0.3912 - val_accuracy: 0.8913\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 0.1590 - accuracy: 0.9419 - val_loss: 0.3983 - val_accuracy: 0.8859\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1511 - accuracy: 0.9378 - val_loss: 0.4102 - val_accuracy: 0.9022\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1716 - accuracy: 0.9419 - val_loss: 0.3980 - val_accuracy: 0.8859\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1389 - accuracy: 0.9500 - val_loss: 0.4067 - val_accuracy: 0.8913\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1349 - accuracy: 0.9568 - val_loss: 0.4126 - val_accuracy: 0.8913\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1523 - accuracy: 0.9392 - val_loss: 0.3819 - val_accuracy: 0.9022\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1542 - accuracy: 0.9473 - val_loss: 0.4097 - val_accuracy: 0.8967\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1562 - accuracy: 0.9514 - val_loss: 0.4564 - val_accuracy: 0.8859\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1539 - accuracy: 0.9419 - val_loss: 0.4147 - val_accuracy: 0.8804\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1629 - accuracy: 0.9392 - val_loss: 0.3689 - val_accuracy: 0.8913\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1660 - accuracy: 0.9365 - val_loss: 0.3564 - val_accuracy: 0.8967\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1273 - accuracy: 0.9527 - val_loss: 0.3289 - val_accuracy: 0.8913\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1409 - accuracy: 0.9486 - val_loss: 0.3771 - val_accuracy: 0.9022\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1183 - accuracy: 0.9635 - val_loss: 0.3273 - val_accuracy: 0.8967\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.1194 - accuracy: 0.9635 - val_loss: 0.3379 - val_accuracy: 0.9130\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1200 - accuracy: 0.9568 - val_loss: 0.3333 - val_accuracy: 0.9130\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1347 - accuracy: 0.9459 - val_loss: 0.3362 - val_accuracy: 0.8967\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1304 - accuracy: 0.9527 - val_loss: 0.3835 - val_accuracy: 0.9130\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.1331 - accuracy: 0.9568 - val_loss: 0.3406 - val_accuracy: 0.8913\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.1017 - accuracy: 0.9649 - val_loss: 0.3590 - val_accuracy: 0.8967\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1090 - accuracy: 0.9622 - val_loss: 0.3844 - val_accuracy: 0.8859\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1363 - accuracy: 0.9486 - val_loss: 0.3489 - val_accuracy: 0.8913\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1376 - accuracy: 0.9473 - val_loss: 0.3752 - val_accuracy: 0.8967\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0955 - accuracy: 0.9676 - val_loss: 0.3694 - val_accuracy: 0.9130\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0847 - accuracy: 0.9703 - val_loss: 0.3577 - val_accuracy: 0.8913\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1445 - accuracy: 0.9541 - val_loss: 0.3796 - val_accuracy: 0.9076\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1179 - accuracy: 0.9514 - val_loss: 0.3291 - val_accuracy: 0.8913\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1124 - accuracy: 0.9595 - val_loss: 0.3417 - val_accuracy: 0.8967\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0974 - accuracy: 0.9689 - val_loss: 0.3217 - val_accuracy: 0.9076\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1111 - accuracy: 0.9541 - val_loss: 0.3081 - val_accuracy: 0.9130\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0919 - accuracy: 0.9784 - val_loss: 0.3162 - val_accuracy: 0.8913\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1247 - accuracy: 0.9527 - val_loss: 0.3017 - val_accuracy: 0.8967\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1064 - accuracy: 0.9635 - val_loss: 0.3260 - val_accuracy: 0.9130\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1000 - accuracy: 0.9703 - val_loss: 0.3510 - val_accuracy: 0.9022\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1082 - accuracy: 0.9608 - val_loss: 0.2966 - val_accuracy: 0.9076\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1081 - accuracy: 0.9595 - val_loss: 0.3375 - val_accuracy: 0.9022\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1108 - accuracy: 0.9608 - val_loss: 0.3346 - val_accuracy: 0.8913\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0917 - accuracy: 0.9622 - val_loss: 0.3498 - val_accuracy: 0.8967\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0718 - accuracy: 0.9811 - val_loss: 0.3316 - val_accuracy: 0.8696\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1022 - accuracy: 0.9622 - val_loss: 0.3034 - val_accuracy: 0.8859\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0953 - accuracy: 0.9676 - val_loss: 0.3086 - val_accuracy: 0.9022\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 0.0819 - accuracy: 0.9676 - val_loss: 0.3450 - val_accuracy: 0.8967\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1023 - accuracy: 0.9635 - val_loss: 0.3292 - val_accuracy: 0.9022\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1017 - accuracy: 0.9676 - val_loss: 0.3442 - val_accuracy: 0.8913\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0905 - accuracy: 0.9635 - val_loss: 0.2993 - val_accuracy: 0.9239\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0933 - accuracy: 0.9716 - val_loss: 0.3540 - val_accuracy: 0.8967\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0930 - accuracy: 0.9622 - val_loss: 0.3303 - val_accuracy: 0.8967\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0964 - accuracy: 0.9689 - val_loss: 0.3339 - val_accuracy: 0.8913\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0814 - accuracy: 0.9716 - val_loss: 0.3004 - val_accuracy: 0.9022\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0755 - accuracy: 0.9730 - val_loss: 0.3321 - val_accuracy: 0.8696\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0899 - accuracy: 0.9649 - val_loss: 0.3479 - val_accuracy: 0.8750\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0712 - accuracy: 0.9703 - val_loss: 0.4188 - val_accuracy: 0.8913\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0853 - accuracy: 0.9649 - val_loss: 0.3375 - val_accuracy: 0.8967\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0863 - accuracy: 0.9689 - val_loss: 0.2972 - val_accuracy: 0.9022\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0961 - accuracy: 0.9743 - val_loss: 0.3488 - val_accuracy: 0.8859\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0714 - accuracy: 0.9689 - val_loss: 0.3131 - val_accuracy: 0.9022\n",
      "Test accuracy of the CNN model for subject 8: 0.8457446694374084\n"
     ]
    }
   ],
   "source": [
    "subjects = 9\n",
    "for subject in range(subjects):\n",
    "    tf.keras.backend.clear_session()\n",
    "    learning_rate = 1e-3\n",
    "    epochs = 100\n",
    "    cnn_subject_model_optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test = preprocess_subjects(subject=subject)\n",
    "    cnn_subject_model.compile(loss='categorical_crossentropy',\n",
    "                    optimizer=cnn_subject_model_optimizer,\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "    cnn_subject_model_results = cnn_subject_model.fit(x_train,\n",
    "                y_train,\n",
    "                batch_size=64,\n",
    "                epochs=epochs,\n",
    "                validation_data=(x_valid, y_valid), verbose=True)\n",
    "    \n",
    "    cnn_subject_model_score = cnn_subject_model.evaluate(x_test, y_test, verbose=False)\n",
    "    print(f'Test accuracy of the CNN model for subject {subject}:',cnn_subject_model_score[1])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Evaluate the classification accuracy as a function of time (e.g., does it increase as you have data over longer periods of time? how much time is required to get a reasonable classification accuracy?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper Exploration and Analysis into other architectures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee147",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
