{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf                  \n",
    "import tensorflow.nn as nn               \n",
    "from tensorflow import keras             \n",
    "import tensorflow.keras.layers as layers \n",
    "import numpy as np                       \n",
    "from sklearn.model_selection import train_test_split   \n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, Dense, Dropout, ELU, Flatten, MaxPool2D\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.initializers import lecun_uniform\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = np.load(\"X_test.npy\")\n",
    "y_test = np.load(\"y_test.npy\")\n",
    "person_train_valid = np.load(\"person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"y_train_valid.npy\")\n",
    "person_test = np.load(\"person_test.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Valid data shape: (2115, 22, 1000)\n",
      "Test data shape: (443, 22, 1000)\n",
      "Training/Valid target shape: (2115,)\n",
      "Test target shape: (443,)\n",
      "Person train/valid shape: (2115, 1)\n",
      "Person test shape: (443, 1)\n"
     ]
    }
   ],
   "source": [
    "print ('Training/Valid data shape: {}'.format(X_train_valid.shape))\n",
    "print ('Test data shape: {}'.format(X_test.shape))\n",
    "print ('Training/Valid target shape: {}'.format(y_train_valid.shape))\n",
    "print ('Test target shape: {}'.format(y_test.shape))\n",
    "print ('Person train/valid shape: {}'.format(person_train_valid.shape))\n",
    "print ('Person test shape: {}'.format(person_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data: (2115, 1000, 22)\n",
      "testing data: (443, 1000, 22)\n"
     ]
    }
   ],
   "source": [
    "# rearange input dimensions\n",
    "# model expects (batch, seq, features) but currently we have (batch, features, seq)\n",
    "\n",
    "X_train_valid = np.transpose(X_train_valid, (0, 2, 1))\n",
    "X_test = np.transpose(X_test, (0, 2, 1))\n",
    "\n",
    "print(\"training data:\", X_train_valid.shape)\n",
    "print(\"testing data:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-assign labels to be between 0 and 3\n",
    "y_train_valid[y_train_valid==769] = 0  # cue onset left\n",
    "y_train_valid[y_train_valid==770] = 1  # cue onset right\n",
    "y_train_valid[y_train_valid==771] = 2  # cue onset foot\n",
    "y_train_valid[y_train_valid==772] = 3  # cue onset tongue\n",
    "\n",
    "y_test[y_test==769] = 0  # cue onset left\n",
    "y_test[y_test==770] = 1  # cue onset right\n",
    "y_test[y_test==771] = 2  # cue onset foot\n",
    "y_test[y_test==772] = 3  # cue onset tongue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the training and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size=0.2, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training images: (1692, 1000, 22, 1)\n",
      "training labels: (1692,)\n",
      "validation images: (423, 1000, 22, 1)\n",
      "validation labels: (423,)\n",
      "test images: (443, 1000, 22, 1)\n",
      "test labels: (443,)\n"
     ]
    }
   ],
   "source": [
    "# add extra dimension for grayscale images\n",
    "X_train_grayscale = np.expand_dims(X_train, axis=-1)\n",
    "X_valid_grayscale = np.expand_dims(X_valid, axis=-1)\n",
    "X_test_grayscale = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "print(\"training images:\", X_train_grayscale.shape)\n",
    "print(\"training labels:\", y_train.shape)\n",
    "print(\"validation images:\", X_valid_grayscale.shape)\n",
    "print(\"validation labels:\", y_valid.shape)\n",
    "print(\"test images:\", X_test_grayscale.shape)\n",
    "print(\"test labels:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 22:42:13.374405: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# construct the dataset from the NumPy arrays\n",
    "X_train_grayscale = tf.data.Dataset.from_tensor_slices((X_train_grayscale, y_train))\n",
    "X_valid_grayscale = tf.data.Dataset.from_tensor_slices((X_valid_grayscale, y_valid))\n",
    "X_test_grayscale = tf.data.Dataset.from_tensor_slices((X_test_grayscale, y_test))\n",
    "\n",
    "# batch and shuffle\n",
    "X_train_grayscale = X_train_grayscale.shuffle(1000).batch(32)\n",
    "X_valid_grayscale = X_valid_grayscale.shuffle(300).batch(32)\n",
    "X_test_grayscale = X_test_grayscale.shuffle(300).batch(32)\n",
    "\n",
    "\n",
    "# construct the dataset from the NumPy arrays\n",
    "X_train = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "X_valid = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "X_test = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "# batch and shuffle\n",
    "X_train = X_train.shuffle(1024).batch(32)\n",
    "X_valid = X_valid.shuffle(1024).batch(32)\n",
    "X_test = X_test.shuffle(1024).batch(32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Optimize the classification accuracy for subject 1. Does it help to train across all subjects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Optimize the classification accuracy across all subjects. How does the classifier do? Do you notice any interesting trends?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Evaluate the classification accuracy as a function of time (e.g., does it increase as you have data over longer periods of time? how much time is required to get a reasonable classification accuracy?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper Exploration and Analysis into other architectures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee147",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
